[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog is dedicated to building the community around the data.table R package. Its startup is funded by NSF-POSE Grant called “Expanding the data.table ecosystem for efficient big data manipulation in R”. Here you will find announcements related to the grant, as well as tips, updates, and tutorials for data.table and its sister packages.\nBlog managed by Kelly Bodwin, Tyson Barrett, and Toby Hocking. Email us at r.data.table@gmail.com.\nFind us in the R-Bloggers feed!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeal of Approval: dtplyr\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nSeal of Approval: nc\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\nToby Dylan Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nSeal of Approval: tidyfast\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\nTyson S. Barrett\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncement: The ‘Seal of Approval’\n\n\n\n\n\n\n\n\n\n\n\nJul 31, 2024\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nTwo Roads Diverged\n\n\n\n\n\n\n\n\n\n\n\nJun 4, 2024\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nTesting infrastructure for data.table\n\n\n\n\n\n\n\n\n\n\n\nMar 10, 2024\n\n\nToby Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nCommunity interviews about data.table\n\n\n\n\n\n\n\n\n\n\n\nMar 6, 2024\n\n\nAnirban Chetia\n\n\n\n\n\n\n\n\n\n\n\n\nResults of the 2023 survey\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2024\n\n\nAljaž Sluga\n\n\n\n\n\n\n\n\n\n\n\n\nColumn assignment and reference semantics in data.table\n\n\n\n\n\n\n\n\n\n\n\nFeb 18, 2024\n\n\nToby Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nThe Benefits of data.table Syntax\n\n\n\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\nTyson Barrett\n\n\n\n\n\n\n\n\n\n\n\n\nNew governance, release with new features\n\n\n\n\n\n\n\n\n\n\n\nJan 30, 2024\n\n\nToby Dylan Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nPiping data.tables\n\n\n\n\n\n\n\n\n\n\n\nJan 28, 2024\n\n\nElio Campitelli\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncement: Jan Gorecki, data.table Ambassador\n\n\n\n\n\n\n\n\n\n\n\nJan 14, 2024\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nSummary of LatinR conference\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2023\n\n\nToby Dylan Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncement: The data.table Ambassadors Travel Grant\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncement: data.table translation projects\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to the data.table ecosystem project!\n\n\nAn NSF-POSE funded venture.\n\n\n\n\n\n\n\n\nOct 15, 2023\n\n\nToby Hocking\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-01-28-piping_data_tables-elio_campitelli/index.html",
    "href": "posts/2024-01-28-piping_data_tables-elio_campitelli/index.html",
    "title": "Piping data.tables",
    "section": "",
    "text": "Like a devoted plumber, modern R loves pipes. The magrittr pipe has a long history and it’s fair share of detractors, but with the implementation of the native pipe operator released in May 2021 it’s clear that chaining operations is now part of R vernacular.\nSo it’s no surprise that people often wonder how can you use pipes with data.table, as one participant of the recent data.table tutorial during LatinR 2023. The surprising answer is that data.table has supported pipelines since its inception in 2006. Furthermore, you can easily use either the magrittr or native pipes."
  },
  {
    "objectID": "posts/2024-01-28-piping_data_tables-elio_campitelli/index.html#the-data.table-pipe",
    "href": "posts/2024-01-28-piping_data_tables-elio_campitelli/index.html#the-data.table-pipe",
    "title": "Piping data.tables",
    "section": "The data.table “pipe”",
    "text": "The data.table “pipe”\nInstead of passing data to functions, data.table syntax is all about operating inside the [ operator1 .\n \n\nDT[rows, columns, by]\n\n\nWhere DT is a data.table object, the rows argument is used for filtering and joining operations, the columns argument can summarise and mutate, and the by argument defines the groups to which to apply these operations.\nSo, to get only Chinstrap penguins from the penguins dataset, instead of using base::subset() or dplyr::filter() you would do\n\npenguins_chinstrap &lt;- penguins[species == \"Chinstrap\"]\n\nOr, to get the mean mean flipper length of these penguins for each island and sex, you could summarise the data like this:\n\npenguins_chinstrap[, .(mean_flipper_length = mean(flipper_length_mm)), by = .(sex, island)]\n\nBut because the output of the first operation is a data.table, you can add another [ operator after the first to chain both operations:\n\npenguins[species == \"Chinstrap\"][, .(mean_flipper_length = mean(flipper_length_mm)), by = .(sex, island)]\n\nI usually call this the ][ pipe.\nYou might have noticed that for just two operations, this line of code is already too long, so for even moderately long chains it’s usually advisable to put each operation in its own line. There’s some controversy on how to break the ][ pipe into lines and indent it. One option is to add a new line after the second [, which has the advantage of actually writing the ][ pipe explicitly.\n\npenguins[species == \"Chinstrap\" ][\n  , .(mean_flipper_length = mean(flipper_length_mm)), by = .(sex, island)]\n\nA second options is to add the new line before the end of the first operation like so:\n\npenguins[species == \"Chinstrap\" \n       ][ , .(mean_flipper_length = mean(flipper_length_mm)), by = .(sex, island)]\n\nPersonally, I don’t like this syntax very much. No matter how you slice it, you always get what feels to me as incomplete lines. Also, RStudio doesn’t correctly indent the second syntax automatically.\nAlternatively, the ][ pipe can go in its own line like so:\n\npenguins[species == \"Chinstrap\" \n][ \n  , .(mean_flipper_length = mean(flipper_length_mm)), by = .(sex, island)\n]\n\nThis is indented correctly by RStudio and has the advantage of making easy to comment out each individual step:\n\npenguins[species == \"Chinstrap\" \n][ \n  # , .(mean_flipper_length = mean(flipper_length_mm)), by = .(sex, island)\n]"
  },
  {
    "objectID": "posts/2024-01-28-piping_data_tables-elio_campitelli/index.html#data.table-and-magrittr",
    "href": "posts/2024-01-28-piping_data_tables-elio_campitelli/index.html#data.table-and-magrittr",
    "title": "Piping data.tables",
    "section": "data.table and magrittr",
    "text": "data.table and magrittr\nUntil the introduction of the native pipe, I used to write long data.table pipelines using magrittr. To do this, I took advantage of the . placeholder which, within a magrittr pipe, refers to the result of the previous step.\n\nlibrary(magrittr)\n\npenguins[species == \"Chinstrap\"] %&gt;%\n  .[ , .(mean_flipper_length = mean(flipper_length_mm)), by = .(sex, island)]\n\nI really like this syntax as it’s very clean. Each line of code is a complete operation without dangling parts and it’s easy to comment out single steps.\nThe only downside is that the dot here has two meanings: as the placeholder for the previous result in .[, and as an alias for list in .(mean_flipper_length = mean(flipper_length_mm)). It’s not a huge issue, though, since I tend to read .[ as a single entity, but it can trip up some people."
  },
  {
    "objectID": "posts/2024-01-28-piping_data_tables-elio_campitelli/index.html#using-the-native-pipe",
    "href": "posts/2024-01-28-piping_data_tables-elio_campitelli/index.html#using-the-native-pipe",
    "title": "Piping data.tables",
    "section": "Using the native pipe",
    "text": "Using the native pipe\nThe native pipe at first didn’t have a placeholder and it didn’t chaining to [, so this so the above syntax wasn’t directly applicable. But you could cheat by creating an alias for [ and use that alias as a regular function. So this works:\n\nDT &lt;- `[`\n\npenguins[species == \"Chinstrap\"] |&gt; \n  DT( , .(mean_flipper_length = mean(flipper_length_mm)), by = .(sex, island))\n\nThis worked so well that data.table officially added the DT() function (currently only in the development version), so if you’re using the latest development version you don’t even need the first line2.\nThis syntax is fine but I don’t like that I need ro write one more character and the closing character being a ) can get confusing because it adds to the closing ) that you usually have in the by argument.\nFrom R 4.3.0 onwards, the native pipe supports a _ placeholder to the right-hand side fo the pipe. So now the magrittr syntax can be directly translated to\n\npenguins[species == \"Chinstrap\"] |&gt; \n  _[ , .(mean_flipper_length = mean(flipper_length_mm)), by = .(sex, island)]\n\nI like this syntax even more than the original magrittr one because it solves the double meaning problem and operations get hugged by a pair of brackets."
  },
  {
    "objectID": "posts/2024-01-28-piping_data_tables-elio_campitelli/index.html#the-four-pipes-of-data.table",
    "href": "posts/2024-01-28-piping_data_tables-elio_campitelli/index.html#the-four-pipes-of-data.table",
    "title": "Piping data.tables",
    "section": "The four pipes of data.table",
    "text": "The four pipes of data.table\nSo, there you are, 4 different ways you can pipe your data.tables.\nUse the ][ pipe if you want your code to have minimal dependencies and work in older versions of R. Use the %&gt;% pipe if you want your code to work in older versions of R and don’t mind the extra dependency. Use any version of the |&gt; pipe if you want minimal dependencies and don’t mind depending on R &gt;= 4.3.0.\n\npenguins[species == \"Chinstrap\" ][\n  , .(mean_flipper_length = mean(flipper_length_mm)), by = .(sex, island)]\n\npenguins[species == \"Chinstrap\"] %&gt;%\n  .[ , .(mean_flipper_length = mean(flipper_length_mm)), by = .(sex, island)]\n\npenguins[species == \"Chinstrap\"] |&gt; \n  DT( , .(mean_flipper_length = mean(flipper_length_mm)), by = .(sex, island))\n\npenguins[species == \"Chinstrap\"] |&gt; \n  _[ , .(mean_flipper_length = mean(flipper_length_mm)), by = .(sex, island)]\n\nImage by storyset on Freepik"
  },
  {
    "objectID": "posts/2024-01-28-piping_data_tables-elio_campitelli/index.html#footnotes",
    "href": "posts/2024-01-28-piping_data_tables-elio_campitelli/index.html#footnotes",
    "title": "Piping data.tables",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTechnically, the [ operator is itself a function, but the syntax is not function-like.↩︎\nThis function also allows the user to use data.table syntax to data.frames and tibbles retaining the class of the output.↩︎"
  },
  {
    "objectID": "posts/2024-08-01-seal_of_approval-nc/index.html",
    "href": "posts/2024-08-01-seal_of_approval-nc/index.html",
    "title": "Seal of Approval: nc",
    "section": "",
    "text": "nc hex sticker\n\n\n\nMaintainer: Toby Dylan Hocking (toby.hocking@r-project.org)\nUser-friendly functions for extracting a data table (row for each match, column for each group) from non-tabular text data using regular expressions, and for melting columns that match a regular expression. Patterns are defined using a readable syntax that makes it easy to build complex patterns in terms of simpler, re-usable sub-patterns. Named R arguments are translated to column names in the output, thereby providing a standard interface to three regular expression ‘C’ libraries (‘PCRE’, ‘RE2’, ‘ICU’). Output can also include numeric columns via user-specified type conversion functions."
  },
  {
    "objectID": "posts/2024-08-01-seal_of_approval-nc/index.html#nc",
    "href": "posts/2024-08-01-seal_of_approval-nc/index.html#nc",
    "title": "Seal of Approval: nc",
    "section": "",
    "text": "nc hex sticker\n\n\n\nMaintainer: Toby Dylan Hocking (toby.hocking@r-project.org)\nUser-friendly functions for extracting a data table (row for each match, column for each group) from non-tabular text data using regular expressions, and for melting columns that match a regular expression. Patterns are defined using a readable syntax that makes it easy to build complex patterns in terms of simpler, re-usable sub-patterns. Named R arguments are translated to column names in the output, thereby providing a standard interface to three regular expression ‘C’ libraries (‘PCRE’, ‘RE2’, ‘ICU’). Output can also include numeric columns via user-specified type conversion functions."
  },
  {
    "objectID": "posts/2024-08-01-seal_of_approval-nc/index.html#relationship-with-data.table",
    "href": "posts/2024-08-01-seal_of_approval-nc/index.html#relationship-with-data.table",
    "title": "Seal of Approval: nc",
    "section": "Relationship with data.table",
    "text": "Relationship with data.table\nWhereas data.table provides several functions such as patterns() and measure() which support some regex engines (PCRE, TRE), nc interfaces with two other engines (RE2, ICU). nc imports data.table, and always returns regex match results as a data.table."
  },
  {
    "objectID": "posts/2024-08-01-seal_of_approval-nc/index.html#overview",
    "href": "posts/2024-08-01-seal_of_approval-nc/index.html#overview",
    "title": "Seal of Approval: nc",
    "section": "Overview",
    "text": "Overview\nnc is useful for extracting numeric data from text, for example consider the following strings, which indicate genomic positions, in bases on a chromosome:\n\nchr.pos.vec &lt;- c(\n  \"chr10:213,054,000-213,055,000\",\n  \"chrM:111,000\",              # no end.\n  \"chr1:110-111 chr2:220-222\") # two ranges.\n\nThe data above consist of a chromosome name (chr10), followed by a start position, and then optionally a dash and an end position. Using nc, we can extract these different pieces of information into a data table using the code below, which inputs the data to parse (first argument), along with a regular expression (subsequent arguments).\n\nnc::capture_first_vec(\n  chr.pos.vec,\n  chrom=\"chr.*?\",\n  \":\",\n  start=\"[0-9,]+\")\n\n    chrom       start\n   &lt;char&gt;      &lt;char&gt;\n1:  chr10 213,054,000\n2:   chrM     111,000\n3:   chr1         110\n\n\nThe code above uses chrom and start as argument names, which are therefore used for column names in the output data table (one row per input subject string, one column per named argument / capture group). However the code above only parses the start position (and not the optional end position). Below, we create a more complex regex to parse both the start and end, by first defining a common pattern to parse an integer,\n\nkeep.digits &lt;- function(x) as.integer(gsub(\"[^0-9]\", \"\", x))\n\nint.pattern &lt;- list(\"[0-9,]+\", keep.digits)\n\nIn the code above, we use a list to group the regex \"[0-9],]+\" with the function keep.digits which will be used for parsing the text that is extracted by that regex. We use that pattern twice in the code below,\n\nrange.pattern &lt;- list(\n  chrom=\"chr.*?\",\n  \":\",\n  start=int.pattern,\n  list( # un-named list becomes non-capturing group.\n    \"-\",\n    end=int.pattern\n  ), \"?\") # chromEnd is optional.\nnc::capture_first_vec(chr.pos.vec, range.pattern)\n\n    chrom     start       end\n   &lt;char&gt;     &lt;int&gt;     &lt;int&gt;\n1:  chr10 213054000 213055000\n2:   chrM    111000        NA\n3:   chr1       110       111\n\n\nThe result above is a data table containing the first match in each subject (three rows total). Note the second row has end=NA because that optional group did not match.\nBut the last subject has two potential matches (only the first is reported above). What if we wanted to get all matches in each subject? We can use another function, as in the code below.\n\nnc::capture_all_str(chr.pos.vec, range.pattern)\n\n    chrom     start       end\n   &lt;char&gt;     &lt;int&gt;     &lt;int&gt;\n1:  chr10 213054000 213055000\n2:   chrM    111000        NA\n3:   chr1       110       111\n4:   chr2       220       222\n\n\nThe output above includes all matches in each subject (four rows total), but does not include any information about which subject each row came from, because it treats the subject as a single string to parse. To get that info, we can use capture_all_str() for each row, using by=.I as in the code below.\n\nlibrary(data.table)\ndata.table(chr.pos.vec)[, nc::capture_all_str(\n  chr.pos.vec, range.pattern), by=.I]\n\n       I  chrom     start       end\n   &lt;int&gt; &lt;char&gt;     &lt;int&gt;     &lt;int&gt;\n1:     1  chr10 213054000 213055000\n2:     2   chrM    111000        NA\n3:     3   chr1       110       111\n4:     3   chr2       220       222\n\n\nThe output above includes the additional I column which is the index of the subject that each match came from (two rows with I=3 because there are two matches in the third subject).\nFinally, data.table::melt() is used to power the long-to-wide data reshaping functionality in nc. In data.table we could use measure() to specify a set of variables to reshape, as in the code below.\n\n(iris.wide &lt;- data.table(iris)[1])\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;  &lt;fctr&gt;\n1:          5.1         3.5          1.4         0.2  setosa\n\nmelt(iris.wide, measure.vars=measure(value.name, dim, pattern=\"(.*)[.](.*)\"))\n\n   Species    dim Sepal Petal\n    &lt;fctr&gt; &lt;char&gt; &lt;num&gt; &lt;num&gt;\n1:  setosa Length   5.1   1.4\n2:  setosa  Width   3.5   0.2\n\n\nThe result above has reshaped the four numeric input columns into two numeric output columns (value.name is the sentinel/keyword indicating that we want to make a new column for each unique value captured in that group). The equivalent nc code would be as below, with the regex defined using a named argument for each capture group (instead of one long pattern string with parentheses for each capture group).\n\nnc::capture_melt_multiple(\n  iris.wide,\n  column=\".*\",\n  \"[.]\",\n  dim=\".*\")\n\n   Species    dim Petal Sepal\n    &lt;fctr&gt; &lt;char&gt; &lt;num&gt; &lt;num&gt;\n1:  setosa Length   1.4   5.1\n2:  setosa  Width   0.2   3.5\n\n\nThe nc code above produces the same result, and in fact uses data.table::melt() internally.\nFor more info about the nc package, please read the vignettes on its CRAN page."
  },
  {
    "objectID": "posts/2024-02-05-tips_tricks_1-tyson_barrett/index.html",
    "href": "posts/2024-02-05-tips_tricks_1-tyson_barrett/index.html",
    "title": "The Benefits of data.table Syntax",
    "section": "",
    "text": "Among the many reasons to use data.table in your code (which includes the more common answers of speed, memory efficiency, etc.) is the syntax. The syntax is\nIn this post, I’d like to show how these features are beneficial and useful in working with data regardless of the size of the data. To do this, I’ll use two packages:\nlibrary(data.table)\nlibrary(palmerpenguins)\nand we’ll create a data.table of the penguins data set (and a data.frame version for other examples):\ndt &lt;- as.data.table(penguins)\ndf &lt;- as.data.frame(penguins)\nThis post assumes some familiarity with data.table syntax but even if you are new to it, there is likely a lot of information that is quite useful for you."
  },
  {
    "objectID": "posts/2024-02-05-tips_tricks_1-tyson_barrett/index.html#concise",
    "href": "posts/2024-02-05-tips_tricks_1-tyson_barrett/index.html#concise",
    "title": "The Benefits of data.table Syntax",
    "section": "Concise",
    "text": "Concise\nThe syntax ultimately is built around the concise dt[i, j, by] framework (built on the core functionality of data frames, see the R-centric section below). This syntax allows you to:\n\nSubset (“filter”) your data using the i argument.\n\n\n# Subset to only Adelie species\ndt[species == \"Adelie\"]\n\n     species    island bill_length_mm bill_depth_mm flipper_length_mm\n      &lt;fctr&gt;    &lt;fctr&gt;          &lt;num&gt;         &lt;num&gt;             &lt;int&gt;\n  1:  Adelie Torgersen           39.1          18.7               181\n  2:  Adelie Torgersen           39.5          17.4               186\n  3:  Adelie Torgersen           40.3          18.0               195\n  4:  Adelie Torgersen             NA            NA                NA\n  5:  Adelie Torgersen           36.7          19.3               193\n ---                                                                 \n148:  Adelie     Dream           36.6          18.4               184\n149:  Adelie     Dream           36.0          17.8               195\n150:  Adelie     Dream           37.8          18.1               193\n151:  Adelie     Dream           36.0          17.1               187\n152:  Adelie     Dream           41.5          18.5               201\n     body_mass_g    sex  year\n           &lt;int&gt; &lt;fctr&gt; &lt;int&gt;\n  1:        3750   male  2007\n  2:        3800 female  2007\n  3:        3250 female  2007\n  4:          NA   &lt;NA&gt;  2007\n  5:        3450 female  2007\n ---                         \n148:        3475 female  2009\n149:        3450 female  2009\n150:        3750   male  2009\n151:        3700 female  2009\n152:        4000   male  2009\n\n\nOther ways to do this include the more redundant base R approach\n\ndf[df$species == \"Adele\"]\n\ndata frame with 0 columns and 344 rows\n\n\nand the more verbose approach in the tidyverse.\n\nlibrary(tidyverse)\ndf %&gt;% \n  filter(species == \"Adele\")\n\n\nMutate or transform your variables using the j argument. Note that the use of := mutates in place so no need for other assignment (e.g., &lt;-).\n\n\n# change body_mass_g to pounds\ndt[, body_mass_lbs := body_mass_g*0.00220462]\n\n\n\n       species body_mass_lbs\n        &lt;fctr&gt;         &lt;num&gt;\n  1:    Adelie      8.267325\n  2:    Adelie      8.377556\n  3:    Adelie      7.165015\n  4:    Adelie            NA\n  5:    Adelie      7.605939\n ---                        \n340: Chinstrap      8.818480\n341: Chinstrap      7.495708\n342: Chinstrap      8.322441\n343: Chinstrap      9.038942\n344: Chinstrap      8.322441\n\n\nWe could also do this in base R a number of ways, all of which are more redundant:\n\ndf$body_mass_lbs &lt;- df$body_mass_g*0.00220462\ndf[, \"body_mass_lbs\"] &lt;- df[, \"body_mass_g\"]*0.00220462\ndf[[\"body_mass_lbs\"]] &lt;- df[[\"body_mass_g\"]]*0.00220462\n\n\nDo all sorts of data work on groups using the by argument.\n\n\n# create a new variable that is the average of the body mass by species\ndt[, avg_mass_lbs := mean(body_mass_lbs, na.rm=TRUE), by = sex]\n\n\n\n       species    sex avg_mass_lbs\n        &lt;fctr&gt; &lt;fctr&gt;        &lt;num&gt;\n  1:    Adelie   male    10.021507\n  2:    Adelie female     8.514844\n  3:    Adelie female     8.514844\n  4:    Adelie   &lt;NA&gt;     8.830728\n  5:    Adelie female     8.514844\n ---                              \n340: Chinstrap   male    10.021507\n341: Chinstrap female     8.514844\n342: Chinstrap   male    10.021507\n343: Chinstrap   male    10.021507\n344: Chinstrap female     8.514844\n\n\nThis is more difficult, but possible, in base R to get a summary and add it to the existing data.frame:\n\ntapply(df$body_mass_lbs, df$sex, mean, na.rm=TRUE) # doesn't keep all rows\n\n# does keep all rows but complicated code\ndf &lt;- \n  by(df, INDICES = df$sex,                           \n     FUN = function(x){\n       x$avg_mass_lbs &lt;- mean(x$body_mass_lbs)\n       return(x)\n  })\ndf &lt;- do.call(\"rbind\", df)\n\nand can definitely be done in the tidyverse.\n\ndf &lt;- df %&gt;% \n  group_by(sex) %&gt;% \n  mutate(avg_mass_lbs = mean(body_mass_lbs, na.rm=TRUE)) %&gt;% \n  ungroup()\n\nIn each example, you can see a lot of work can be done in a single line of code with minimal redundancy. Although in each situation base R and tidyverse equivalents exist (often with a lot of powerful flexibility in the tidyverse approaches), the concise nature of data.table syntax can make writing and reading the code quicker."
  },
  {
    "objectID": "posts/2024-02-05-tips_tricks_1-tyson_barrett/index.html#predictable",
    "href": "posts/2024-02-05-tips_tricks_1-tyson_barrett/index.html#predictable",
    "title": "The Benefits of data.table Syntax",
    "section": "Predictable",
    "text": "Predictable\nThe syntax is naturally predictable without being verbose. For instance, whenever you use :=, it’s going to keep the same shape as the current data (“mutate”) while the use of .(var = fun(x)) will summarize to the fewest number of rows appropriate (1 row for non-grouped expressions and x rows for x number of unique groups).\nTo get an idea of how this predictability manifests in the code, we’ll use an example. Here, we can grab the average bill length by sex. We could do this two ways. The first is mutating in place where the data do not change size or shape. Note, the .() function is shorthand for list().\n\ndt[, avg_bill_length := mean(bill_length_mm, na.rm=TRUE), by = sex]\n\nThis gives us a new variable in the original data.\n\n\n       species    sex avg_bill_length\n        &lt;fctr&gt; &lt;fctr&gt;           &lt;num&gt;\n  1:    Adelie   male        45.85476\n  2:    Adelie female        42.09697\n  3:    Adelie female        42.09697\n  4:    Adelie   &lt;NA&gt;        41.30000\n  5:    Adelie female        42.09697\n ---                                 \n340: Chinstrap   male        45.85476\n341: Chinstrap female        42.09697\n342: Chinstrap   male        45.85476\n343: Chinstrap   male        45.85476\n344: Chinstrap female        42.09697\n\n\nHowever, sometimes we just want the data summarized. We can use the syntax below for that (notice no :=).\n\ndt[, .(avg_bill_length = mean(bill_length_mm, na.rm=TRUE)), by = sex]\n\n      sex avg_bill_length\n   &lt;fctr&gt;           &lt;num&gt;\n1:   male        45.85476\n2: female        42.09697\n3:   &lt;NA&gt;        41.30000\n\n\nWe can always assign this so we can access it later.\n\navg_bill &lt;- dt[, .(avg_bill_length = mean(bill_length_mm, na.rm=TRUE)), by = sex]\n\nOne way data.table makes the code predictable is that the data operations happen all within the square brackets without lingering attributes that may produce surprising results. That is, whatever I put in the brackets will be run together and then done. For example, I may have several grouping variables that I use to modify some variables, and only do it for a subset of the data.\n\ndt[species == \"Adelie\", max_bill := max(bill_length_mm, na.rm=TRUE), by = .(species, sex)]\n\nThe new variable max_bill is made for the data but is only applicable to the Adelie species and is done by both species as sex. Once this operation is done, the grouping variables are just normal variables again and we still have access to the full data.\n\n\n       species    sex max_bill\n        &lt;fctr&gt; &lt;fctr&gt;    &lt;num&gt;\n  1:    Adelie   male     46.0\n  2:    Adelie female     42.2\n  3:    Adelie female     42.2\n  4:    Adelie   &lt;NA&gt;     42.0\n  5:    Adelie female     42.2\n ---                          \n340: Chinstrap   male       NA\n341: Chinstrap female       NA\n342: Chinstrap   male       NA\n343: Chinstrap   male       NA\n344: Chinstrap female       NA"
  },
  {
    "objectID": "posts/2024-02-05-tips_tricks_1-tyson_barrett/index.html#r-centric",
    "href": "posts/2024-02-05-tips_tricks_1-tyson_barrett/index.html#r-centric",
    "title": "The Benefits of data.table Syntax",
    "section": "R-centric",
    "text": "R-centric\nAll of the main functionality in data.table is structured around vectors, lists, and (a modified form) of data frames. These core structures in R can be seeing throughout the syntax and design of the package. Even the dt[i, j, by] syntax is designed to mirror (and simplify) data frames. For new users, this can be particularly useful: no additional data structures are needed to work with the data and do both simple and complicated data operations."
  },
  {
    "objectID": "posts/2023-11-19-LatinR_summary-toby_hocking/index.html",
    "href": "posts/2023-11-19-LatinR_summary-toby_hocking/index.html",
    "title": "Summary of LatinR conference",
    "section": "",
    "text": "Last month, I (Toby) went to the LatinR conference in Montevideo, Uruguay. I had two goals: to teach about data.table in a tutorial, and to find people to work on translations."
  },
  {
    "objectID": "posts/2023-11-19-LatinR_summary-toby_hocking/index.html#tutorial-about-data.table",
    "href": "posts/2023-11-19-LatinR_summary-toby_hocking/index.html#tutorial-about-data.table",
    "title": "Summary of LatinR conference",
    "section": "Tutorial about data.table",
    "text": "Tutorial about data.table\nI presented a tutorial on the first day of the LatinR meeting, to an audience of about 50 students.\n\n\n\nPhoto from the LatinR data.table tutorial\n\n\nThe google slides that I used are online, and I also created a GitHub repo with the source files that I used for creating the figures in the slides. During the talk, Elio Campitelli and Paola Corrales were there to help people in the audience with individual/technical questions (for example, installation of data.table from github master was difficult for some people using windows). I did not have enough time to do all of the exercises, but I did spend about 10 minutes at the end of my talk, to invite people to participate in the translation projects and travel awards.\nOne of the students in the audience was Mara Destefanis, who said she was very interested to participate in the translation project. In fact, since the conference, she has sent me several emails, updating me about a Spanish translation of my tutorial slides."
  },
  {
    "objectID": "posts/2023-11-19-LatinR_summary-toby_hocking/index.html#translation-workshop",
    "href": "posts/2023-11-19-LatinR_summary-toby_hocking/index.html#translation-workshop",
    "title": "Summary of LatinR conference",
    "section": "Translation workshop",
    "text": "Translation workshop\nDuring the conference, there was a translation workshop.\n\n\n\nPhoto from the LatinR translation workshop\n\n\nI learned that there has been a lot of progress recently, about translations in R.\n\nDuring the R project sprint in summer 2023, there was an effort to crowdsource translations of messages, using a new weblate server for R.\nElio Campitelli proposed a R Consortium project about translating Rd pages. Currently, there is only one Rd page for help on any given topic, and it is possible in theory to write an Rd page in multiple languages, but in practice most are in English only.\nNestor Montano told me that he recorded an online workshop about data.table, in Spanish! Slides, Youtube.\nRiva Quiroga told me about the R Para Ciencia de Datos project, which is a Spanish translation of the popular R for Data Science online textbook (an important reference about tidyverse). Part of that project involved creating new data sets in the datos package, with column names and man pages translated to Spanish. For example vuelos.rd is the man page for the translated version of flights data. Riva told me about her experience leading this translation project. For our NSF data.table project, she suggested several important criteria to consider, that I had not written in my original call for translation projects. After discussing with her, I wrote a PR to revise and improve the call for translation projects.\n\nOverall I felt that going to LatinR was a very worthwhile experience, because of the successful outreach in my tutorial, and the great networking at the translation workshop."
  },
  {
    "objectID": "posts/2023-11-19-LatinR_summary-toby_hocking/index.html#communcations-since-then",
    "href": "posts/2023-11-19-LatinR_summary-toby_hocking/index.html#communcations-since-then",
    "title": "Summary of LatinR conference",
    "section": "Communcations since then",
    "text": "Communcations since then\nSince then, I have been in email contact with several people who have expressed interest in the data.table translation project.\n\nFor Spanish, Riva Quiroga (Chile) told me that she would like to lead, with several other people who expressed interest to participate: Nati Labadie (Argentina), Andrea Gomez Vargas (Colombia), Emanuel Ciardullo (Argentina), Nestor Montano (Ecuador), Mara Destefanis (Uruguay), …\nFor French, Philippe Grojean told me that he would be interested to lead, and submit an application in the next few months.\nFor Portuguese, Leonardo Ferreira Fontenelle told me that he would be interested to lead, and submit an application soon.\n\nI look forward to seeing these projects develop in the coming months!"
  },
  {
    "objectID": "posts/2024-01-14-ambassador_gorecki-community_team/index.html",
    "href": "posts/2024-01-14-ambassador_gorecki-community_team/index.html",
    "title": "Announcement: Jan Gorecki, data.table Ambassador",
    "section": "",
    "text": "A round of applause!\n\n\nJan is a natural choice for an Ambassador, due to his many years of fantastic contribution to the data.table package. You can find his great work in open-source development at github.com/jangorecki. Find him on Mastodon at fosstodon.org/@jangorecki to say congratulations, or chat about all things data.table."
  },
  {
    "objectID": "posts/2024-01-14-ambassador_gorecki-community_team/index.html#please-join-me-in-congratulting-our-first-ever-data.table-ambassador-jan-gorecki",
    "href": "posts/2024-01-14-ambassador_gorecki-community_team/index.html#please-join-me-in-congratulting-our-first-ever-data.table-ambassador-jan-gorecki",
    "title": "Announcement: Jan Gorecki, data.table Ambassador",
    "section": "",
    "text": "A round of applause!\n\n\nJan is a natural choice for an Ambassador, due to his many years of fantastic contribution to the data.table package. You can find his great work in open-source development at github.com/jangorecki. Find him on Mastodon at fosstodon.org/@jangorecki to say congratulations, or chat about all things data.table."
  },
  {
    "objectID": "posts/2024-01-14-ambassador_gorecki-community_team/index.html#talks",
    "href": "posts/2024-01-14-ambassador_gorecki-community_team/index.html#talks",
    "title": "Announcement: Jan Gorecki, data.table Ambassador",
    "section": "Talks",
    "text": "Talks\nAs part of his Ambassador role, Jan will be giving three exciting talks this year:\n\nRolling statistics and moving windows in Edinburgh on January 26th, with the Edinburgh R Users group.\n\nRolling statistics are an interesting topic for optimizations, therefore in my talk I will use R language to present naive implementation, and the optimized implementation, on a simple case of rolling mean. Then I will move to data.table implementations of rolling statistics explaining possible optimizations in other functions, which are not that straightforward anymore, like min/max and, actually very complex, median. Finally benchmarks will be presented comparing data.table implementations to base R, pandas, polars, slider/dplyr, duckdb and spark.\n\n\n\nHigh-productivity data frame operations with data.table on February 8th in Sevilla, Spain with the Sevilla R Users group\n\n\n\nTalk with SevillaR\n\n\nRegister to watch online here\n\n\nThe Spanish R Conference in Sevilla, Spain. (Date and topic TBD)\nWe look forward to Jan’s excellent talks, and a forthcoming blog post right here on The Raft to share his experiences - and of course, his continued wonderful contributions to the data.table community. Thank you, Jan!"
  },
  {
    "objectID": "posts/2024-01-14-ambassador_gorecki-community_team/index.html#become-an-ambassador",
    "href": "posts/2024-01-14-ambassador_gorecki-community_team/index.html#become-an-ambassador",
    "title": "Announcement: Jan Gorecki, data.table Ambassador",
    "section": "Become an Ambassador",
    "text": "Become an Ambassador\nDo you have ideas for talks about data.table? Do you want to be part of this new community movement? Apply now for the data.table Ambassadors Grant to fund conference travel for presentations related to data.table. More details on the Ambassadors program at this blog post, and more details on the grant project itself are here.\nQuestions? Email r.data.table@gmail.com with any and all questions!"
  },
  {
    "objectID": "posts/2024-03-10-testing_plan-toby_hocking/index.html",
    "href": "posts/2024-03-10-testing_plan-toby_hocking/index.html",
    "title": "Testing infrastructure for data.table",
    "section": "",
    "text": "One major element of the NSF POSE grant for data.table is to create more documentation and testing infrastructure, in order to help expand the data.table ecosystem. This blog post explains what we proposed to do to improve the testing infrastructure."
  },
  {
    "objectID": "posts/2024-03-10-testing_plan-toby_hocking/index.html#current-testing-infrastructure",
    "href": "posts/2024-03-10-testing_plan-toby_hocking/index.html#current-testing-infrastructure",
    "title": "Testing infrastructure for data.table",
    "section": "Current testing infrastructure",
    "text": "Current testing infrastructure\nCurrent testing is limited to package checks that run on CI:\n\nGithub actions runs R CMD check on Ubuntu for each PR.\nAppVeyor runs R CMD check on Windows for each PR.\nCodeCov is used to track code coverage for each PR.\nGitLab runs R CMD check on ten different platforms, for each push to master."
  },
  {
    "objectID": "posts/2024-03-10-testing_plan-toby_hocking/index.html#new-asymptotic-performance-testing-framework",
    "href": "posts/2024-03-10-testing_plan-toby_hocking/index.html#new-asymptotic-performance-testing-framework",
    "title": "Testing infrastructure for data.table",
    "section": "New asymptotic performance testing framework",
    "text": "New asymptotic performance testing framework\nCurrent performance testing is informal. For example before release devs run test.data.table(memtest=TRUE) to examine how much memory is used during tests. If too much memory is used, then that could result in a check failure on CRAN. Sometimes git bisect is used to find the commit which caused a performance regression. But there are no systematic performance tests that are regularly done, even though performance is a major feature of data.table.\nThe lack of a performance testing framework to run on CI often results in performance regressions. Because an emphasis in data.table is on handling large data sets, fast and memory-efficient code is essential, and that is a primary reason why people use data.table for their data analyses. Consequently, when new changes introduce performance regressions (for example, increased computation time), users regularly file issues related to performance. As of October 2022, a search for the keyword performance yields 161 closed and 97 open issues and pull requests. Currently, there is no systematic performance testing framework in place for data.table developers, which unfortunately makes it easy to inadvertently introduce changes that adversely affect performance, and this is a barrier to accepting code contributions.\nWe propose a GitHub Action for comparing the asymptotic performance of a pull request with its parent branch. We, therefore, propose to develop a new infrastructure for systematic empirical asymptotic performance testing, so that data.table developers will be able to easily detect and prevent performance regressions. This will facilitate expanding the data.table ecosystem by making both existing developers and new contributors more confident that their code does not result in performance regressions. We plan to build a solution that uses R package atime, which we created in order to facilitate comparing the empirical asymptotic performance of different R package versions. The main idea is that the user defines some R code that depends on an input data size N, and then atime keeps increasing the data size N until it reaches some time limit, for example, 1 second. Time and memory usage is measured for each data size and R package version, so it is easy to see if there are any significant performance differences. For example, we used atime with git bisect to find the commit which was responsible for the slowdown in a recent issue.\nWe have previous experience building GitHub actions for continuous integration testing of R packages, via Rperform and RcppDeepState, so we plan to adapt these existing GitHub actions for empirical asymptotic performance testing of data.table. The two people responsible for implementing this part of the project are Doris Amoakohene and Anirban Chetia, who will build (1) an asymptotic performance test suite that formalizes a set of computations for which efficiency is important, and (2) a GitHub action that runs each test in the suite systematically for each development branch, and creates/updates a comment in the corresponding pull request. If there are any significant differences in empirical asymptotic performance measurements, then the comment will contain a figure showing three empirical asymptotic performance curves:\n\nmost recent commit on the development branch,\nthe most recent commit on the main branch, and\nthe best common ancestor commit (also known as merge base).\n\nThe overall result will make it easy for data.table developers to do systematic asymptotic performance testing of each pull request, thereby reducing the chance of performance regressions, and increasing the security/confidence of accepting new code contributions, which will encourage the data.table contributor ecosystem to expand."
  },
  {
    "objectID": "posts/2024-03-10-testing_plan-toby_hocking/index.html#new-db-benchmark",
    "href": "posts/2024-03-10-testing_plan-toby_hocking/index.html#new-db-benchmark",
    "title": "Testing infrastructure for data.table",
    "section": "New db-benchmark",
    "text": "New db-benchmark\nBecause a major feature of data.table is its efficiency (small time and memory requirements), users and developers are interested to know how its performance compares with similar software tools (in R and in other languages). Until 2021, data.table contributor Jan Gorecki maintained the db-benchmark, that compared computation times of various data manipulation libraries on various different tasks. The most recently computed benchmark result from 2021 shows that data.table is among the fastest software. Other similar benchmarks have been created by developers of other libraries such as polars and duckdb.\nOne of the goals of this project is to get these benchmarks running on a regular basis (every week) on a variety of computing platforms. On one hand, we would like to run benchmarks on Amazon EC2, because that is a public computing resource that anyone can use to verify/reproduce the results. On the other hand, doing all those benchmarks on Amazon EC2 would be prohibitively expensive for most people (the last run was 163 hours, which would cost over $500 per run on a c6i.16xlarge virtual machine with 64 CPUs and 128GB of memory. We, therefore, propose a compromise, where we run the complete set of benchmarks on the NAU Monsoon cluster (which may take over 100 hours, and is free to use for this project, but not reproducible for other groups), and publish them on a web page every week. We will also develop a new db-benchmark-small that can run on Amazon EC2 at a much lower cost (for easy/cheap reproducibility). Our goal will be to have a single Amazon EC2 benchmark run cost $10–$100, which means 3–30 hours of computation time on c6i.16xlarge virtual machines. The result of this project activity will be a new infrastructure for regularly comparing the performance of data.table with similar software libraries, which will be useful for identifying areas where data.table has advantages or could be improved."
  },
  {
    "objectID": "posts/2024-03-10-testing_plan-toby_hocking/index.html#continuous-reverse-dependency-checking-on-nau-monsoon-cluster",
    "href": "posts/2024-03-10-testing_plan-toby_hocking/index.html#continuous-reverse-dependency-checking-on-nau-monsoon-cluster",
    "title": "Testing infrastructure for data.table",
    "section": "Continuous reverse dependency checking on NAU Monsoon cluster",
    "text": "Continuous reverse dependency checking on NAU Monsoon cluster\nReverse dependencies are other R packages that require functionality from data.table (over a thousand R packages). A new version of data.table released to CRAN must be compatible with the example and test code in these reverse dependencies. Therefore, before submitting an update to CRAN, each reverse dependency must be checked to ensure that there are no new errors. This involves significant computation time, to run the example/test code in thousands of R packages, and also significant developer time, to investigate any regressions. During this project, we, therefore, propose to create a new infrastructure for continuous reverse dependency checking. In detail, we plan to run regular nightly checks of the data.table main branch on the NAU Monsoon cluster, which is freely available for use by the PI for this project. The results of the checks will be compared with the current check results from the previous release version of data.table on CRAN, and any regressions will be highlighted on a web page for easy identification by data.table developers. Overall the result will be a new shared infrastructure for continuous reverse dependency checking, which will make it much easier for the data.table project to provide more frequent releases.\nActually, this part of the project is almost complete. We have implemented a system on NAU Monsoon which begins a new check every morning just after midnight, and publishes the results to a web page, usually before noon on the same day. There are currently 1400+ revdeps, which would take about two weeks to check, if we run each revdep in sequence on a single CPU. Luckily, we get the results on Monsoon in just a few hours, which is approximately a 30x speedup. A recent result is shown below,\n\n\n\nsignificant differences table\n\n\nThe main result is the table above, which has a row for each significant difference found, when comparing a revdep check using data.table CRAN release, to current data.table master. The Rvers column indicates the version of base R which was used (devel or release), and typical revdep issues show up using both versions of base R. The table is sorted by the first column, which is the SHA1 hash of the first commit which was found to have the issue, according to git bisect. The links lead to the corresponding commit on github (first.bad.commit), the full revdep check log file (Package), and current CRAN check result (CRAN).\nThe revdep check system has been working for over a year now, and has been very helpful in preparing the upcoming release of data.table 1.15.0 (which had several dozen revdep issues that needed to be fixed). There are currently some package installation issues which may cause some false negatives (real revdep issues which are not reported), but at least these installation issues are displayed on the result web page, and we are currently working to resolve them. The source code for the revdep check system is available in the tdhock/data.table-revdeps repository on GitHub."
  },
  {
    "objectID": "posts/2024-03-10-testing_plan-toby_hocking/index.html#conclusion",
    "href": "posts/2024-03-10-testing_plan-toby_hocking/index.html#conclusion",
    "title": "Testing infrastructure for data.table",
    "section": "Conclusion",
    "text": "Conclusion\nWe have discussed the plan for augmenting the testing infrastructure available for data.table (performance testing, benchmarking, and revdep checking). Hopefully the new testing infrastructure will allow contributors to be more confident about merging PRs with bug fixes and new features."
  },
  {
    "objectID": "posts/2024-05-20-kelly_bodwin/index.html",
    "href": "posts/2024-05-20-kelly_bodwin/index.html",
    "title": "Two Roads Diverged",
    "section": "",
    "text": "A little-known historical tidbit is that Robert Frost’s The Road Less Traveled - so often cited as a celebration of individuality and difficult choices - was in fact meant as a joke to tease an indecisive friend. Frost’s intent was to be ironic; to make fun of someone who is overly dramatic looking back on their choices. Come on buddy, he says, just pick a path - they are both beautiful and will get you somewhere interesting.\nI bring this up because, like in the poem, I believe we in the R community often overdramatize moments of divergence between different R dialects, packages, and syntaxes.\nAnd I, like poor Robert Frost, also feel there may be some misunderstanding around the my intent here.\nSo to make sure my opinions are loud and clear, we’ll get some help from this cartoon lady to shout out the things I most want y’all to hear from me:\nThis blog post is my attempt to dispel the myths surrounding the relationship between data.table and the tidyverse, and to explain why I believe deeply in both."
  },
  {
    "objectID": "posts/2024-05-20-kelly_bodwin/index.html#two-roads-diverged-in-a-yellow-wood-and-sorry-i-could-not-travel-both",
    "href": "posts/2024-05-20-kelly_bodwin/index.html#two-roads-diverged-in-a-yellow-wood-and-sorry-i-could-not-travel-both",
    "title": "Two Roads Diverged",
    "section": "“Two roads diverged in a yellow wood, and sorry I could not travel both…”",
    "text": "“Two roads diverged in a yellow wood, and sorry I could not travel both…”\nA bit of history to kick us off. (I know, I know, I’ll keep it short.)\nThe first official 1.0 version of R was released February 29, 2000, and if I had to guess, I’d bet the first add-on package was created the next day. The beating heart of R is the base language, that the R Core Team has lovingly and diligently maintained for over 4 decades - but its soul, if you will, is the incredible collection of packages that expand and adapt this core.\ndata.table was released in 2008 by Matt Dowle. (See this video for a very cool recap of the inspiration and process from Matt himself!) Since then, it has grown enormously in scope, contributors, user base, and dependencies.\nIn 2014, dplyr was released by Hadley Wickham, the birth of what we now know as the tidyverse.\nThis meant that users now had several options to pick from if they wanted to, say, calculate means by group:\n\n## Base R\naggregate(bill_length_mm ~ species, data = penguins, mean)\n\n\n## data.table\n\npenguins_dt &lt;- data.table(penguins)\npenguins_dt[, .(mean_bill=mean(bill_length_mm)), by=species]\n\n\n## dplyr\n\npenguins %&gt;%\n  group_by(species) %&gt;%\n  summarize(mean_bill = mean(bill_length_mm))\n\n\n\n\n\n\ndata.table is not technically Base R!\n\n\n\nIt is true that the data.table syntax most closely mimics that of Base R data frames, and deliberately so. However, data.table is an open-source package like any other. Nobody - and I mean nobody - uses only Base R in their work. What a silly culture that would be, if we have all the beautiful multiverse of an open-source language, and we limit ourselves only to the core functionality!\n\n\n\nI don’t even remember what this ad was for."
  },
  {
    "objectID": "posts/2024-05-20-kelly_bodwin/index.html#looked-down-one-as-far-as-i-could-then-took-the-other-just-as-fair",
    "href": "posts/2024-05-20-kelly_bodwin/index.html#looked-down-one-as-far-as-i-could-then-took-the-other-just-as-fair",
    "title": "Two Roads Diverged",
    "section": "“… looked down one as far as I could … then took the other, just as fair…”",
    "text": "“… looked down one as far as I could … then took the other, just as fair…”\nSo: We have multiple dialects. How to choose which one to use?\nThere is no single right answer to that question; it’s all a matter of individual preference and use case. What do you, as the programmer, value most? Brevity of code? Readability of code? Speed? Familiarity? Consistency with collaborators? Availability of learning resources?\nI could go on - there are infinitely many reasons, from the personal to the professional to the practical, to choose one path or the other. Sometimes, the answer is as simple as, “This is the way that I know how to do it.”\nI can’t tell you how to pick what works for you.\n\n\n\n\n\n“Dialect” or syntax choices in R are contextual and case-by-case, not lifetime commitments!\n\n\n\nThe idea of “loyalty” to a package is nonsense. A package is a tool. You might have admiration, respect, or even loyalty to a package developer; you might even therefore trust that its worth your time and energy to follow their recommendations.\nBut if you start feeling bad when you sprinkle a little Base R into your tidy workflow… if you are ashamed for piping a data.table object into ggplot… well that’s getting us nowhere, is it? We are blessed with an overabundance of useful tools and we shouldn’t be limiting ourselves!\n\n## Great news!  This is not illegal!\n\npenguins %&gt;%\n  data.table() %&gt;%\n  .[, .(mean_bill=mean(bill_length_mm)), by=species]\n\nWe all use our own favorite collection of packages, in the combinations that work for us. It might be fun to discuss and learn about new options or new preferences, but no more purity culture, please!\n\n\n\n\n\nIt’s okay to use different syntaxes and package styles all in one workflow!"
  },
  {
    "objectID": "posts/2024-05-20-kelly_bodwin/index.html#i-shall-be-telling-this-with-a-sigh-somewhere-ages-and-ages-hence",
    "href": "posts/2024-05-20-kelly_bodwin/index.html#i-shall-be-telling-this-with-a-sigh-somewhere-ages-and-ages-hence",
    "title": "Two Roads Diverged",
    "section": "“…I shall be telling this with a sigh, somewhere ages and ages hence…”",
    "text": "“…I shall be telling this with a sigh, somewhere ages and ages hence…”\nIt has come to the point where I can’t avoid mentioning what mainly motivated this blog post: The Great Twitter War of 2018. (Please read that sentence with every ounce of irony you have in you).\nBriefly for those who weren’t “lucky” enough to be in the tweetstorm: Sometime around 2018, the #rstats Twitter community exploded into a debate about the relative merits of the tidyverse, data.table, and Base R.\n\n\n\nIt was basically a lot of this. (Source: XKCD#386)\n\n\nIt’s sad to me that the community seems to remember this time as a fight, because so much of that conversation was productive and interesting. Educators shared their experiences teaching with different dialects. Developers talked about the speed trade-offs of the various options. New users were excited to be exposed to information about their options.\nBut - as seems to be the norm on the internet - a vocal subset of this conversation took the form of an “us vs. them” debate, and weird lines were drawn between data.table/Base R and the tidyverse.\n\n\n\nRabblerabblerabble\n\n\nIt’s important to note that the primary developers themselves - Hadley Wickham and Matt Dowle - were not the cause of the drama. In fact, this good conversations from this Twitter whirlwind lead to the creation of on of my favorite packages, dtplyr!\nSo why am I partially digging up a buried hatchet?\nBecause sadly, even today, I sometimes run into vitriol when I post on social media about data.table or the tidyverse, and I know I’m not alone in this.\nEven today, I have my college students asking me about the rift in the R world, and if they have to “choose a side” to learn R.\nAnd most relevant to this blog - I have gotten a lot of questions about why I am involved in a data.table project, since I’m “supposed” to be Team Tidyverse.\nTherefore, to be ultra clear:\n\n\n\n\n\nThis grant is NOT about helping data.table “beat” dplyr.\n\n\n\nThis could not be further from the truth! I’m a tidyverse girlie - from my dplyr earrings to my hex fabric shirts - and I am also a data.table girlie. I, personally, would not be working on this project if I thought anyone involved viewed it as anti-tidyverse in any way.\nWhat we want is the same thing any open-source fan wants:\n\nWe want users to be aware of the many fantastic tools, including data.table, that exist in the R world.\nWe want developers to be inspired to build new and exciting packages, that stand on the shoulders of giants like data.table, the tidyverse, and so many others.\nWe want beloved packages like data.table to stick around long term, and to grow and evolve with R and the R community."
  },
  {
    "objectID": "posts/2024-05-20-kelly_bodwin/index.html#and-that-has-made-all-the-difference.",
    "href": "posts/2024-05-20-kelly_bodwin/index.html#and-that-has-made-all-the-difference.",
    "title": "Two Roads Diverged",
    "section": "“… and that has made all the difference.”",
    "text": "“… and that has made all the difference.”\nSo, where are we going from here, as a community? Only good places, I think, no matter which path we take in the yellow wood!\nI am so excited about this project and about the NSF-POSE grant - both for the longevity of data.table, and for everything we are learning about open-source ecosystems and how to sustain them.\n\n\n\n\n\nI love the #rstats community!!! Let’s do cool stuff together."
  },
  {
    "objectID": "posts/2024-05-20-kelly_bodwin/index.html#addendum",
    "href": "posts/2024-05-20-kelly_bodwin/index.html#addendum",
    "title": "Two Roads Diverged",
    "section": "Addendum",
    "text": "Addendum\nWant to hear me rant more about the R community, multiple dialects/languages, and this grant project? I’ll be speaking on these topics at UseR!2024, JSM, and Posit::conf - or you can always find me on BlueSky or Fosstodon!"
  },
  {
    "objectID": "posts/2024-01-30-new_governance_new_release-toby_hocking/index.html",
    "href": "posts/2024-01-30-new_governance_new_release-toby_hocking/index.html",
    "title": "New governance, release with new features",
    "section": "",
    "text": "I am proud to report that today, the first major new data.table features in several years have been released to CRAN!\nThis new release, version 1.15.0, is remarkable because it is the first new feature release using the new community governance, which was adopted last month.\nHere is a brief timeline of the recent activities."
  },
  {
    "objectID": "posts/2024-01-30-new_governance_new_release-toby_hocking/index.html#governance-discussion",
    "href": "posts/2024-01-30-new_governance_new_release-toby_hocking/index.html#governance-discussion",
    "title": "New governance, release with new features",
    "section": "Governance discussion",
    "text": "Governance discussion\n\n\n\nAn ongoing discussion\n\n\nIn Aug 2023, I started a discussion in issue#5676 about the process and goal of creating a formal governance document. 17 people commented on that issue, and the consensus was to publish a first draft community governance document in Nov 2023. Discussion in related issues included:\n\nWhat are the guiding principles of the project? issue#5693\nWhat is within scope for features? issue#5722\nWhat roles/permissions should we define, and how can people obtain them? My proposal.\nWhat conventions should we use for version numbers? issue#5715\nWhat code of conduct should we adopt? issue#5708\nWhat communication should we expect between the CRAN maintainer and the rest of the dev team? issue#5714\n\nAt first, it was not clear that commenters on that issue would agree to adopt a community governance structure, out of respect for the original creator, Matt Dowle, who had not yet expressed his approval of the process. However, that changed on 11 Sep 2023, when I posted Matt’s letters of collaboration that he signed in support of my NSF POSE project (after I asked Matt over email, and he agreed that I post them publicly). After that, there was a much stronger support of the proposed process."
  },
  {
    "objectID": "posts/2024-01-30-new_governance_new_release-toby_hocking/index.html#governance-draft-and-adoption",
    "href": "posts/2024-01-30-new_governance_new_release-toby_hocking/index.html#governance-draft-and-adoption",
    "title": "New governance, release with new features",
    "section": "Governance draft and adoption",
    "text": "Governance draft and adoption\n\n\n\nGovernor Sea Lion\n\n\nAfter much discussion in the above linked issues, I published an initial draft of the governance document on 27 Nov 2023 in PR#5772. That PR was extensively reviewed by four of the current/active contributors: Michael Chirico, Jan Gorecki, Tyson Barrett and Ben Schwendinger After several rounds of comments and revisions, Jan merged the PR on 14 Dec 2023, which signaled the official adoption of the new governance of the project. It can be viewed in the GOVERNANCE.md file in the git repo.\nImportantly, the new governance defines five roles for people involved in the project:\n Contributor: Any member of the public at large who participates in issue discussions, code reviews, or pull requests for data.table.\n Project member: Anyone who has contributed a substantial accepted update - whether technical or documentation based - to data.table.\n Reviewer: A project member who volunteers to help review other contributions.\n Committer: Given merge permissions on main GitHub branch; responsible for reviewing and incorporating updates. Currently: myself, Jan, and Michael.\nCRAN maintainer: Responsible for organizing new releases on GitHub and CRAN. Currently Tyson Barrett.\nInterestingly, CRAN maintainer and Committer permissions are largely orthogonal, and in fact Tyson does not currently have the Committer role permissions.\nAny of these roles is possible to obtain, using the process described in the governance document. If you are at all interested, we could definitely use your help! Please get in contact by commenting on issues/PRs on GitHub."
  },
  {
    "objectID": "posts/2024-01-30-new_governance_new_release-toby_hocking/index.html#release-1.15.0-with-new-features",
    "href": "posts/2024-01-30-new_governance_new_release-toby_hocking/index.html#release-1.15.0-with-new-features",
    "title": "New governance, release with new features",
    "section": "Release 1.15.0 with new features",
    "text": "Release 1.15.0 with new features\n\n\n\nThe new measure function\n\n\nAs outlined in the governance document, section CRAN updates, each release should be discussed and approved by consensus in an issue. The issue that we used for this 1.15.0 release is issue#5823. As can be seen in the NEWS.md file, this new release includes 20 NOTES, 55 BUG FIXES, and 41 NEW FEATURES, and 1 BREAKING CHANGE. Among the new features, I am most excited about one that I implemented: the new measure() function, which makes it easier to do complex wide-to-long reshape operations, as below:\n\nlibrary(data.table)\n(iris.dt &lt;- data.table(iris)[1])\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;  &lt;fctr&gt;\n1:          5.1         3.5          1.4         0.2  setosa\n\nmelt(iris.dt, measure.vars=measure(part, dim, sep=\".\"))\n\n   Species   part    dim value\n    &lt;fctr&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt;\n1:  setosa  Sepal Length   5.1\n2:  setosa  Sepal  Width   3.5\n3:  setosa  Petal Length   1.4\n4:  setosa  Petal  Width   0.2\n\nmelt(iris.dt, measure.vars=measure(value.name, dim, sep=\".\"))\n\n   Species    dim Sepal Petal\n    &lt;fctr&gt; &lt;char&gt; &lt;num&gt; &lt;num&gt;\n1:  setosa Length   5.1   1.4\n2:  setosa  Width   3.5   0.2"
  },
  {
    "objectID": "posts/2024-01-30-new_governance_new_release-toby_hocking/index.html#outlook-for-the-future",
    "href": "posts/2024-01-30-new_governance_new_release-toby_hocking/index.html#outlook-for-the-future",
    "title": "New governance, release with new features",
    "section": "Outlook for the future",
    "text": "Outlook for the future\nSince adopting the community governance document, there has been a lot of new activity on GitHub, and I am looking forward to seeing even more in the months to come. For example, now that we have adopted a code of conduct, we are eligible to apply for NumFOCUS funding, see discussion in issue#5676. Finally, if you use data.table, and are interested to contribute toward our next release, we could use your help, so please contact us in an issue/PR."
  },
  {
    "objectID": "posts/2024-02-18-dt_particularities-toby_hocking/index.html",
    "href": "posts/2024-02-18-dt_particularities-toby_hocking/index.html",
    "title": "Column assignment and reference semantics in {data.table}",
    "section": "",
    "text": "The goal of this blog post is to explain some similarities and differences between the base R data.frame object type, and the data.table object type. We will focus on accessing and assigning values, and discuss two major differences:"
  },
  {
    "objectID": "posts/2024-02-18-dt_particularities-toby_hocking/index.html#difference-in-syntax",
    "href": "posts/2024-02-18-dt_particularities-toby_hocking/index.html#difference-in-syntax",
    "title": "Column assignment and reference semantics in {data.table}",
    "section": "Difference in syntax",
    "text": "Difference in syntax\nTo break down the similarities and differences in syntax, consider the data below,\n\n\nSee source code\nlibrary(data.table)\nlibrary(knitr)\n\nsyntax &lt;- function(type, name, columns, code){\n  mcall &lt;- match.call()\n  dt.args &lt;- lapply(as.list(mcall[-1]), paste)\n  do.call(data.table, dt.args)\n}\n\nsyntax.tab &lt;- rbind(\n  syntax(frame, literal, one, \"df$col_name &lt;- value\"),\n  syntax(table, literal, one, \"DT[, col_name := value]\"),\n  syntax(frame, variable, multiple, 'df[, col_names_list] &lt;- values'),\n  syntax(table, variable, multiple, 'DT[, (col_names_list) := values]'))\n\nsyntax.tab |&gt; kable()\n\n\n\n\n\ntype\nname\ncolumns\ncode\n\n\n\n\nframe\nliteral\none\ndf$col_name &lt;- value\n\n\ntable\nliteral\none\nDT[, col_name := value]\n\n\nframe\nvariable\nmultiple\ndf[, col_names_list] &lt;- values\n\n\ntable\nvariable\nmultiple\nDT[, (col_names_list) := values]\n\n\n\n\n\nThe table above defines the different syntax required to do column assignment in data tables (DT) and frames (df).\n\ntype indicates object type: frame or table.\nname indicates whether the column(s) to assign are literally written in the code (col_name), or if the names are stored in a variable (col_names_list).\ncolumns indicates whether only one or multiple (one or more) columns can be assigned using the syntax.\ncode is the exact syntax of the R code used for the assignment.\n\nNote that there are other ways to do column assignment. For example,\n\nDF[[\"col_name\"]] &lt;- value can also be used for single column assignment in a data frame.\nset(DT, j=col_name_list, value=values) is a more efficient version of column assignment for data tables, that is recommended for use in loops, as it avoids the overhead of the [.data.table method.\n\nBelow is a reshaped version of the table above, to facilitate easier comparison between frame and table versions:\n\n\nSee source code\noptions(width=100)\ndata.table::dcast(syntax.tab, name + columns ~ type, value.var=\"code\")  |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\nname\ncolumns\nframe\ntable\n\n\n\n\nliteral\none\ndf$col_name &lt;- value\nDT[, col_name := value]\n\n\nvariable\nmultiple\ndf[, col_names_list] &lt;- values\nDT[, (col_names_list) := values]\n\n\n\n\n\nThe table above shows the equivalent code for assignment of columns using either a data.frame or data.table. In fact, the code in the frame column above can also be used for assignment of a data.table, but it may be less efficient than the data table square brackets, as we will discuss in the next section.\nOne reason why data.table uses a custom assignment syntax is for consistency: the same syntax can be used, with square brackets and :=, for one or multiple column assignment. (Note the use parentheses around col_names_list in the second row of the table column above, to indicate that the left side of := is a variable storing column names or numbers, instead of a direct unquoted column name.)\nAnother reason why data.table uses a custom assignment syntax is for efficiency, as we see in the next section."
  },
  {
    "objectID": "posts/2024-02-18-dt_particularities-toby_hocking/index.html#base-copy-on-write-versus-data.table-reference-semantics",
    "href": "posts/2024-02-18-dt_particularities-toby_hocking/index.html#base-copy-on-write-versus-data.table-reference-semantics",
    "title": "Column assignment and reference semantics in {data.table}",
    "section": "Base “copy on write” versus data.table reference semantics",
    "text": "Base “copy on write” versus data.table reference semantics\nR has “copy on write” semantics, meaning that in base R if a variable is modified inside a function, a copy is made of the whole variable. For example, consider the code below\n\ndt_outside &lt;- data.table(x=1:3)\n\nbase_assign &lt;- function(dt_inside, variable, value){\n  dt_inside[1,variable] &lt;- value # makes a copy of input variable!\n}\n\nbase_assign(dt_outside, \"x\", 0)\n\ndt_outside\n\n       x\n   &lt;int&gt;\n1:     1\n2:     2\n3:     3\n\n\nIn the code above, we pass dt_outside to the base_assign function, which makes a copy called dt_inside before it is modified, so that the data in dt_outside is unchanged after the function is done. Compare with the code below,\n\ndt_assign &lt;- function(dt_inside, variable, value){\n  dt_inside[1, (variable) := value] # directly modifies input variable\n}\n\ndt_assign(dt_outside, \"x\", 0)\n\ndt_outside\n\n       x\n   &lt;int&gt;\n1:     0\n2:     2\n3:     3\n\n\nThe output above shows that by using the square brackets and := assignment, we can modify data.table objects in functions without copying them. Here, the variables dt_inside and dt_outside point to the same underlying data.\n\nEfficiency of reference semantics\nReference semantics mean that data.table assignment is potentially much more efficient than base R, in terms of time and memory usage. To demonstrate, we use the following benchmark. Assume we have a table with \\(N\\) rows, but we just want to modify one row. This should be a constant time/space operation (independent of \\(N\\)), but because of the base R copy on write semantics, it will be a linear time/space operation, \\(O(N)\\).\n\n\nSee source code\natime_result &lt;- atime::atime(\n  N = 10^seq(1, 7, by = 0.5),\n  setup = {\n    dt &lt;- data.table(x = 1:N)\n  },\n  dt_assign = dt_assign(dt, \"x\", 0),\n  base_assign = base_assign(dt, \"x\", 0))\n\nplot(atime_result)\n\n\n\n\n\n\n\n\n\nWe can see from the plot above that for base_assign, both time and space increase with \\(N\\), because the entire table is copied; whereas dt_assign is constant time/space, because only one row is modified with no copy necessary.\n\n\n\n\n\n\nNote\n\n\n\nThe code in this section used a data.table object in both function calls to illustrate the constant time/space assignment which is possible, but the visualized result also applies to other data structures.\nAs an exercise, add two more expressions to the atime benchmark: base_assign with a data.frame object and tibble object. You should see linear time/space for both."
  },
  {
    "objectID": "posts/2024-02-18-dt_particularities-toby_hocking/index.html#conclusions",
    "href": "posts/2024-02-18-dt_particularities-toby_hocking/index.html#conclusions",
    "title": "Column assignment and reference semantics in {data.table}",
    "section": "Conclusions",
    "text": "Conclusions\nIn this post we have explored the syntax and semantics for assignment using base R and data.table square brackets with :=, and we have seen how the reference semantics of data.table can be very beneficial for computational efficiency."
  },
  {
    "objectID": "posts/2024-08-01-seal_of_approval-tidyfast/index.html",
    "href": "posts/2024-08-01-seal_of_approval-tidyfast/index.html",
    "title": "Seal of Approval: tidyfast",
    "section": "",
    "text": "tidyfast hex sticker\n\n\n\nAuthor(s): Tyson S. Barrett, Mark Fairbanks, Ivan Leung, Indrajeet Patil\nMaintainer: Tyson S. Barrett (t.barrett88@gmail.com)\nThe goal of tidyfast is to provide fast and efficient alternatives to some tidyr (and a few dplyr) functions using data.table under the hood. Each have the prefix of dt_ to allow for autocomplete in IDEs such as RStudio. These should compliment some of the current functionality in dtplyr (but notably does not use the lazy_dt() framework of dtplyr). This package imports data.table and cpp11 (no other dependencies). These are, in essence, translations from a more tidyverse grammar to data.table. Most functions herein are in places where, in my opinion, the data.table syntax is not obvious or clear. As such, these functions can translate a simple function call into the fast, efficient, and concise syntax of data.table."
  },
  {
    "objectID": "posts/2024-08-01-seal_of_approval-tidyfast/index.html#tidyfast",
    "href": "posts/2024-08-01-seal_of_approval-tidyfast/index.html#tidyfast",
    "title": "Seal of Approval: tidyfast",
    "section": "",
    "text": "tidyfast hex sticker\n\n\n\nAuthor(s): Tyson S. Barrett, Mark Fairbanks, Ivan Leung, Indrajeet Patil\nMaintainer: Tyson S. Barrett (t.barrett88@gmail.com)\nThe goal of tidyfast is to provide fast and efficient alternatives to some tidyr (and a few dplyr) functions using data.table under the hood. Each have the prefix of dt_ to allow for autocomplete in IDEs such as RStudio. These should compliment some of the current functionality in dtplyr (but notably does not use the lazy_dt() framework of dtplyr). This package imports data.table and cpp11 (no other dependencies). These are, in essence, translations from a more tidyverse grammar to data.table. Most functions herein are in places where, in my opinion, the data.table syntax is not obvious or clear. As such, these functions can translate a simple function call into the fast, efficient, and concise syntax of data.table."
  },
  {
    "objectID": "posts/2024-08-01-seal_of_approval-tidyfast/index.html#relationship-with-data.table",
    "href": "posts/2024-08-01-seal_of_approval-tidyfast/index.html#relationship-with-data.table",
    "title": "Seal of Approval: tidyfast",
    "section": "Relationship with data.table",
    "text": "Relationship with data.table\ntidyfast was designed to be an extension to and translation of data.table. As such, there are three main ways tidyfast is related to data.table.\n\nThis package is built directly on data.table using direct calls to [.data.table and other functions under the hood.\nIt only relies on two packages, cpp11 and data.table both stable packages that are unlikely to have breaking changes often. This follows the data.table principle of few dependencies.\nIt was designed to also show how others can use data.table within their own package to create functions that flexibly call data.table in complex ways."
  },
  {
    "objectID": "posts/2024-08-01-seal_of_approval-tidyfast/index.html#overview",
    "href": "posts/2024-08-01-seal_of_approval-tidyfast/index.html#overview",
    "title": "Seal of Approval: tidyfast",
    "section": "Overview",
    "text": "Overview\nAs shown on the tidyfast GitHub page, tidyfast has several functions that have the prefix dt_. A few notable functions from the package are shown below.\n\nlibrary(tidyfast)\nlibrary(data.table)\nlibrary(magrittr)\n\n\ndt_fill\nFilling NAs is a useful function but tidyr::fill(), especially when done by many, many groups can become too slow. dt_fill() is useful for this and can be used a few different ways.\n\nx = 1:10\ndt_with_nas &lt;- data.table(\n  x = x,\n  y = shift(x, 2L),\n  z = shift(x, -2L),\n  a = sample(c(rep(NA, 10), x), 10),\n  id = sample(1:3, 10, replace = TRUE)\n)\n\n# Original\ndt_with_nas\n\n        x     y     z     a    id\n    &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1:     1    NA     3    NA     1\n 2:     2    NA     4    NA     3\n 3:     3     1     5    NA     2\n 4:     4     2     6     6     3\n 5:     5     3     7    10     1\n 6:     6     4     8     9     3\n 7:     7     5     9     1     3\n 8:     8     6    10     2     2\n 9:     9     7    NA    NA     2\n10:    10     8    NA     3     1\n\n# All defaults\ndt_fill(dt_with_nas, y, z, a, immutable = FALSE)\n\n        x     y     z     a    id\n    &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1:     1    NA     3    NA     1\n 2:     2    NA     4    NA     3\n 3:     3     1     5    NA     2\n 4:     4     2     6     6     3\n 5:     5     3     7    10     1\n 6:     6     4     8     9     3\n 7:     7     5     9     1     3\n 8:     8     6    10     2     2\n 9:     9     7    10     2     2\n10:    10     8    10     3     1\n\n# by id variable called `grp`\ndt_fill(dt_with_nas, \n        y, z, a, \n        id = list(id))\n\n        x     y     z     a    id\n    &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1:     1    NA     3    NA     1\n 2:     2    NA     4    NA     3\n 3:     3     1     5    NA     2\n 4:     4     2     6     6     3\n 5:     5     3     7    10     1\n 6:     6     4     8     9     3\n 7:     7     5     9     1     3\n 8:     8     6    10     2     2\n 9:     9     7    10     2     2\n10:    10     8    10     3     1\n\n# both down and then up filling by group\ndt_fill(dt_with_nas, \n        y, z, a, \n        id = list(id), \n        .direction = \"downup\")\n\n        x     y     z     a    id\n    &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1:     1     3     3    10     1\n 2:     2     2     4     6     3\n 3:     3     1     5     2     2\n 4:     4     2     6     6     3\n 5:     5     3     7    10     1\n 6:     6     4     8     9     3\n 7:     7     5     9     1     3\n 8:     8     6    10     2     2\n 9:     9     7    10     2     2\n10:    10     8    10     3     1\n\n\n\n\ndt_nest\nNesting data can be useful for a number of reasons, including XX. The dt_nest() function takes a data.table and ID variables and nests the remaining columns into a list column of data.tables as shown below.\n\ndt &lt;- data.table(\n   x = rnorm(1e5),\n   y = runif(1e5),\n   grp = sample(1L:5L, 1e5, replace = TRUE),\n   nested1 = lapply(1:10, sample, 10, replace = TRUE),\n   nested2 = lapply(c(\"thing1\", \"thing2\"), sample, 10, replace = TRUE),\n   id = 1:1e5\n)\n\nnested &lt;- dt_nest(dt, grp)\nnested\n\nKey: &lt;grp&gt;\n     grp                  data\n   &lt;int&gt;                &lt;list&gt;\n1:     1 &lt;data.table[19947x5]&gt;\n2:     2 &lt;data.table[19981x5]&gt;\n3:     3 &lt;data.table[20083x5]&gt;\n4:     4 &lt;data.table[19929x5]&gt;\n5:     5 &lt;data.table[20060x5]&gt;\n\n\n\n\ndt_pivot_longer and dt_pivot_wider\nThe last example for this brief post is pivoting. In my opinion, the pivot syntax is easy to remember and use and as such, is nice to have that syntax with the performance of melt() and dcast(). The syntax, although it doesn’t have the full functionality of tidyr’s pivot functions, can do most things you need to do with reshaping data.\n\nbillboard &lt;- tidyr::billboard \n\nlonger &lt;- billboard %&gt;%\n  dt_pivot_longer(\n     cols = c(-artist, -track, -date.entered),\n     names_to = \"week\",\n     values_to = \"rank\"\n  )\n\nWarning in melt.data.table(data = dt_, id.vars = id_vars, measure.vars = cols,\n: 'measure.vars' [wk1, wk2, wk3, wk4, ...] are not all of the same type. By\norder of hierarchy, the molten data value column will be of type 'double'. All\nmeasure variables not of type 'double' will be coerced too. Check DETAILS in\n?melt.data.table for more on coercion.\n\nlonger\n\n                 artist                   track date.entered   week  rank\n                 &lt;char&gt;                  &lt;char&gt;       &lt;Date&gt; &lt;char&gt; &lt;num&gt;\n    1:            2 Pac Baby Don't Cry (Keep...   2000-02-26    wk1    87\n    2:          2Ge+her The Hardest Part Of ...   2000-09-02    wk1    91\n    3:     3 Doors Down              Kryptonite   2000-04-08    wk1    81\n    4:     3 Doors Down                   Loser   2000-10-21    wk1    76\n    5:         504 Boyz           Wobble Wobble   2000-04-15    wk1    57\n   ---                                                                   \n24088:      Yankee Grey    Another Nine Minutes   2000-04-29   wk76    NA\n24089: Yearwood, Trisha         Real Live Woman   2000-04-01   wk76    NA\n24090:  Ying Yang Twins Whistle While You Tw...   2000-03-18   wk76    NA\n24091:    Zombie Nation           Kernkraft 400   2000-09-02   wk76    NA\n24092:  matchbox twenty                    Bent   2000-04-29   wk76    NA\n\n\nCan also take that long data set and turn it wide again.\n\nwider &lt;- longer %&gt;% \n  dt_pivot_wider(\n    names_from = week,\n    values_from = rank\n  )\nwider[, .(artist, track, wk1, wk2)]\n\n               artist                   track   wk1   wk2\n               &lt;char&gt;                  &lt;char&gt; &lt;num&gt; &lt;num&gt;\n  1:            2 Pac Baby Don't Cry (Keep...    87    82\n  2:          2Ge+her The Hardest Part Of ...    91    87\n  3:     3 Doors Down              Kryptonite    81    70\n  4:     3 Doors Down                   Loser    76    76\n  5:         504 Boyz           Wobble Wobble    57    34\n ---                                                     \n313:      Yankee Grey    Another Nine Minutes    86    83\n314: Yearwood, Trisha         Real Live Woman    85    83\n315:  Ying Yang Twins Whistle While You Tw...    95    94\n316:    Zombie Nation           Kernkraft 400    99    99\n317:  matchbox twenty                    Bent    60    37"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Raft",
    "section": "",
    "text": "Seal of Approval: tidyfast\n\n\n\n\n\n\nseal of approval\n\n\nbridge package\n\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\nTyson S. Barrett\n\n\n\n\n\n\n\n\n\n\n\n\nSeal of Approval: dtplyr\n\n\n\n\n\n\nseal of approval\n\n\nbridge package\n\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nSeal of Approval: nc\n\n\n\n\n\n\nseal of approval\n\n\nextension package\n\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\nToby Dylan Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncement: The ‘Seal of Approval’\n\n\n\n\n\n\nannouncements\n\n\ngrant\n\n\ncommunity\n\n\nseal of approval\n\n\n\n\n\n\n\n\n\nJul 31, 2024\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nTwo Roads Diverged\n\n\n\n\n\n\nopinion\n\n\n\n\n\n\n\n\n\nJun 4, 2024\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nTesting infrastructure for data.table\n\n\n\n\n\n\ngrant\n\n\ntesting\n\n\ndeveloper\n\n\n\n\n\n\n\n\n\nMar 10, 2024\n\n\nToby Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nCommunity interviews about data.table\n\n\n\n\n\n\ncommunity\n\n\ngrant\n\n\n\n\n\n\n\n\n\nMar 6, 2024\n\n\nAnirban Chetia\n\n\n\n\n\n\n\n\n\n\n\n\nResults of the 2023 survey\n\n\n\n\n\n\ncommunity\n\n\nguest post\n\n\ngovernance\n\n\n\n\n\n\n\n\n\nFeb 25, 2024\n\n\nAljaž Sluga\n\n\n\n\n\n\n\n\n\n\n\n\nColumn assignment and reference semantics in data.table\n\n\n\n\n\n\ntips\n\n\ntutorials\n\n\ndeveloper\n\n\n\n\n\n\n\n\n\nFeb 18, 2024\n\n\nToby Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nThe Benefits of data.table Syntax\n\n\n\n\n\n\ntips\n\n\ntutorials\n\n\ndocumentation\n\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\nTyson Barrett\n\n\n\n\n\n\n\n\n\n\n\n\nNew governance, release with new features\n\n\n\n\n\n\ngovernance\n\n\nreleases\n\n\n\n\n\n\n\n\n\nJan 30, 2024\n\n\nToby Dylan Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nPiping data.tables\n\n\n\n\n\n\ntips\n\n\ntutorials\n\n\ndocumentation\n\n\nguest post\n\n\n\n\n\n\n\n\n\nJan 28, 2024\n\n\nElio Campitelli\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncement: Jan Gorecki, data.table Ambassador\n\n\n\n\n\n\nannouncements\n\n\ngrant\n\n\nambassadors\n\n\n\n\n\n\n\n\n\nJan 14, 2024\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nSummary of LatinR conference\n\n\n\n\n\n\nconferences\n\n\n\n\n\n\n\n\n\nNov 19, 2023\n\n\nToby Dylan Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncement: The data.table Ambassadors Travel Grant\n\n\n\n\n\n\nannouncements\n\n\ngrant\n\n\nfunding opportunity\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncement: data.table translation projects\n\n\n\n\n\n\nannouncements\n\n\ngrant\n\n\nfunding opportunity\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to the data.table ecosystem project!\n\n\nAn NSF-POSE funded venture.\n\n\n\nannouncements\n\n\ngrant\n\n\n\n\n\n\n\n\n\nOct 15, 2023\n\n\nToby Hocking\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nSeal of Approval: dtplyr\n\n\n\n\n\n\nseal of approval\n\n\nbridge package\n\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\n2 min\n\n\n\n\n\n\n\nSeal of Approval: nc\n\n\n\n\n\n\nseal of approval\n\n\nextension package\n\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\n4 min\n\n\n\n\n\n\n\nSeal of Approval: tidyfast\n\n\n\n\n\n\nseal of approval\n\n\nbridge package\n\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\n4 min\n\n\n\n\n\n\n\nAnnouncement: The ‘Seal of Approval’\n\n\n\n\n\n\nannouncements\n\n\ngrant\n\n\ncommunity\n\n\nseal of approval\n\n\n\n\n\n\n\n\n\nJul 31, 2024\n\n\n2 min\n\n\n\n\n\n\n\nTwo Roads Diverged\n\n\n\n\n\n\nopinion\n\n\n\n\n\n\n\n\n\nJun 4, 2024\n\n\n8 min\n\n\n\n\n\n\n\nTesting infrastructure for data.table\n\n\n\n\n\n\ngrant\n\n\ntesting\n\n\ndeveloper\n\n\n\n\n\n\n\n\n\nMar 10, 2024\n\n\n8 min\n\n\n\n\n\n\n\nCommunity interviews about data.table\n\n\n\n\n\n\ncommunity\n\n\ngrant\n\n\n\n\n\n\n\n\n\nMar 6, 2024\n\n\n17 min\n\n\n\n\n\n\n\nResults of the 2023 survey\n\n\n\n\n\n\ncommunity\n\n\nguest post\n\n\ngovernance\n\n\n\n\n\n\n\n\n\nFeb 25, 2024\n\n\n6 min\n\n\n\n\n\n\n\nColumn assignment and reference semantics in data.table\n\n\n\n\n\n\ntips\n\n\ntutorials\n\n\ndeveloper\n\n\n\n\n\n\n\n\n\nFeb 18, 2024\n\n\n5 min\n\n\n\n\n\n\n\nThe Benefits of data.table Syntax\n\n\n\n\n\n\ntips\n\n\ntutorials\n\n\ndocumentation\n\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\n5 min\n\n\n\n\n\n\n\nNew governance, release with new features\n\n\n\n\n\n\ngovernance\n\n\nreleases\n\n\n\n\n\n\n\n\n\nJan 30, 2024\n\n\n4 min\n\n\n\n\n\n\n\nPiping data.tables\n\n\n\n\n\n\ntips\n\n\ntutorials\n\n\ndocumentation\n\n\nguest post\n\n\n\n\n\n\n\n\n\nJan 28, 2024\n\n\n5 min\n\n\n\n\n\n\n\nAnnouncement: Jan Gorecki, data.table Ambassador\n\n\n\n\n\n\nannouncements\n\n\ngrant\n\n\nambassadors\n\n\n\n\n\n\n\n\n\nJan 14, 2024\n\n\n2 min\n\n\n\n\n\n\n\nSummary of LatinR conference\n\n\n\n\n\n\nconferences\n\n\n\n\n\n\n\n\n\nNov 19, 2023\n\n\n3 min\n\n\n\n\n\n\n\nAnnouncement: The data.table Ambassadors Travel Grant\n\n\n\n\n\n\nannouncements\n\n\ngrant\n\n\nfunding opportunity\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\n3 min\n\n\n\n\n\n\n\nAnnouncement: data.table translation projects\n\n\n\n\n\n\nannouncements\n\n\ngrant\n\n\nfunding opportunity\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\n4 min\n\n\n\n\n\n\n\nWelcome to the data.table ecosystem project!\n\n\nAn NSF-POSE funded venture.\n\n\n\nannouncements\n\n\ngrant\n\n\n\n\n\n\n\n\n\nOct 15, 2023\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-10-15-intro_to_grant-toby_hocking/index.html",
    "href": "posts/2023-10-15-intro_to_grant-toby_hocking/index.html",
    "title": "Welcome to the data.table ecosystem project!",
    "section": "",
    "text": "Hi! My name is Toby Dylan Hocking, and I have been using R since 2003, which means 20 years, can you believe it?\nI work as an Assistant Professor of Computer Science, and my research expertise is machine learning, the modern branch of artificial intelligence which uses big data. R is an important tool in my machine learning work, and in the work of many people in academia/industry/government, because it provides so many useful functions for handling big data."
  },
  {
    "objectID": "posts/2023-10-15-intro_to_grant-toby_hocking/index.html#the-data.table-package",
    "href": "posts/2023-10-15-intro_to_grant-toby_hocking/index.html#the-data.table-package",
    "title": "Welcome to the data.table ecosystem project!",
    "section": "The {data.table} package",
    "text": "The {data.table} package\nSince 2015, I have been using the R package data.table to do large parts of data processing before and after running machine learning algorithms - to get the raw data into the right format for the algorithm, and also to get the results in the right format for visualization/interpretation.\n\n\n\nThe {data.table} hex sticker\n\n\ndata.table is highly valued for its long-term stability and its lightning-fast speed in large data calculations."
  },
  {
    "objectID": "posts/2023-10-15-intro_to_grant-toby_hocking/index.html#ecosystem-expansion",
    "href": "posts/2023-10-15-intro_to_grant-toby_hocking/index.html#ecosystem-expansion",
    "title": "Welcome to the data.table ecosystem project!",
    "section": "Ecosystem expansion",
    "text": "Ecosystem expansion\nWith these use cases in mind, I proposed a project “Expanding the data.table ecosystem for efficient big data manipulation in R,” and I am excited to announce that it has been funded by the National Science Foundation’s “Pathways to Enable Open Source Ecosystems” (POSE) grant, for work between September 2023 and August 2025.\n\n\n\n\n\nOur project attempts to address three issues with the current state of the data.table project:\n\nInformal governance\n\nThe data.table package was originally created by Matt Dowle in 2008. His brilliant use of efficient algorithms and C implementations brought the world a package that has stood the test of time, and is now one of the most-used R packages available.\nHowever, the growth of this incredible package will require more leaders than Dowle himself to help build, review, test, and organize new contributions.\nThus, one goal of this grant is to bring together data.table developers and contributors to propose a new governance structure for the package’s source code.\n\nLimited centralized testing infrastructure\n\nSince the primary draw of data.table is the speed of its algorithmic implementations, adding new functionality to the package is not simple. In particular, new elements must be heavily tested to ensure that they do not interfere with the core computations.\nIn this grant, we plan to develop software to automate the testing of new package contributions, to smooth the growth process. This part of the project includes a centralized reverse dependency checking system, new benchmarks comparing data.table with other systems such as polars and arrow, and new performance testing software.\n\nLimited documentation and outreach\n\nTo encourage more people to learn and adopt data.table, we will be massively expanding the number of tutorials, documentations, and guides for how to use the package effectively. Part of this will be translation projects so that data.table will be more accessible in foreign languages. This grant will also include travel awards, to support selected speakers to travel to conferences and share data.table updates and usage.\nInterested in contributing a tutorial/vignette or blog post? Email r.data.table@gmail.com!"
  },
  {
    "objectID": "posts/2023-10-15-intro_to_grant-toby_hocking/index.html#how-you-can-get-involved",
    "href": "posts/2023-10-15-intro_to_grant-toby_hocking/index.html#how-you-can-get-involved",
    "title": "Welcome to the data.table ecosystem project!",
    "section": "How you can get involved",
    "text": "How you can get involved\nInterested in helping grow the data.table ecosystem? There are so many ways to get involved!\n\n\n\nSea lions say “R!” “R!” “R!”\n\n\n\nFollow us for updates on social media:\n\n\nMastodon: r_data_table@fosstodon.org\nBlueSky: rdatatable.bsky.social\nTwitter/X: r_data_table\n\n\nSubscribe to this blog, The Raft\nTake the Community Survey to weigh in on next steps.\nParticipate in the deep discussions on GitHub:\n\n\nWhat should be the structure of the formal governance document for the package?\nWhat are the core principles of the data.table package?\n\n\nEmail r.data.table@gmail.com to:\n\n\nBe added to the community Slack.\nPropose a guest blog for The Raft.\nAsk questions, make suggestions, or volunteer your expertise.\n\n\nApply for the upcoming Travel Grants and Translation Projects - watch this blog for more information!"
  },
  {
    "objectID": "posts/2023-11-01-travel_grant_announcement-community_team/index.html",
    "href": "posts/2023-11-01-travel_grant_announcement-community_team/index.html",
    "title": "Announcement: The data.table Ambassadors Travel Grant",
    "section": "",
    "text": "We on the community team are very excited to announce another major funding opportunity!\nAs you may know, the National Science Foundation (NSF) has provided funds to support the project “Expanding the data.table ecosystem for efficient big data manipulation in R.” One goal of this project is to better disseminate information about the development and progress of the data.table package.\nTo this end, applications are now open for the data.table Ambassadors Grant to fund conference travel for presentations related to data.table."
  },
  {
    "objectID": "posts/2023-11-01-travel_grant_announcement-community_team/index.html#ambassadors",
    "href": "posts/2023-11-01-travel_grant_announcement-community_team/index.html#ambassadors",
    "title": "Announcement: The data.table Ambassadors Travel Grant",
    "section": "Ambassadors",
    "text": "Ambassadors\nIn order to become a funded ambassador, applicants must meet the following requirements:\n\nPresent a talk, poster, or tutorial at a relevant conference or meeting that is in some way related to the data.table package.\nWrite a blog post about the talk/presentation for The Raft.\n\nFunding is in the form of a flat $2700 stipend towards travel and registration expenses, to be paid upon completion of the presentation and blog post.\nThis program will fund up to 4 Ambassadors per year for the next three years. We are particularly interested in facilitating travel for applicants who may not have as much opportunity; such as early-career researchers, or those from historically marginalized communities.\n\n\n\nThe data.table community is everywhere!"
  },
  {
    "objectID": "posts/2023-11-01-travel_grant_announcement-community_team/index.html#applying",
    "href": "posts/2023-11-01-travel_grant_announcement-community_team/index.html#applying",
    "title": "Announcement: The data.table Ambassadors Travel Grant",
    "section": "Applying",
    "text": "Applying\n\nFill out the Google Form here to apply for the Ambassador Grant.\n\nYou will be asked to provide:\n\nInformation about your presentation:\n\nA max 1-page abstract for your proposed talk or workshop.\nA categorization of the talk: Is it a technical contribution to data.table or related packages? An applied use case for data.table in industry or education? A tutorial or workshop teaching data.table skills to attendees?\nThe conference or meeting at which your presentation has been accepted. (Presentations that are submitted, but not yet accepted, may apply; in exceptional cases, Grant funding may be offered conditional on acceptance.)\nA max 1-page statement addressing the following prompt: How does your proposed talk or presentation further the goals of the NSF-POSE Grant Program? How will it enhance the data.table community and ecosystem?\n\n\n\nPersonal Statements:\n\nA brief statement regarding your personal connection to the data.table community, and why you are motivated to become an Ambassador.\nOptionally, materials or links showing your prior work and connection to data.table.\nOptionally, your self-identification as a member of a historically marginalized community or an early-career researcher.\n\n\n\nWe can’t wait to hear from you!\n\n\n\n“Water” you waiting for? Apply now!"
  },
  {
    "objectID": "posts/2023-11-01-travel_grant_announcement-community_team/index.html#questions",
    "href": "posts/2023-11-01-travel_grant_announcement-community_team/index.html#questions",
    "title": "Announcement: The data.table Ambassadors Travel Grant",
    "section": "Questions",
    "text": "Questions\nNot sure if you should apply? Wondering if there are still Ambassador slots left in the year? Email r.data.table@gmail.com with any and all questions!"
  },
  {
    "objectID": "posts/2024-08-01-seal_of_approval-dtplyr/index.html",
    "href": "posts/2024-08-01-seal_of_approval-dtplyr/index.html",
    "title": "Seal of Approval: dtplyr",
    "section": "",
    "text": "dtplyr hex sticker\n\n\n\nAuthor(s): Hadley Wickham, Maximilian Girlich, Mark Fairbanks, Ryan Dickerson, Posit Software PBC\nMaintainer: Hadley Wickham (hadley@posit.co)\nProvides a data.table backend for dplyr. The goal of dtplyr is to allow you to write dplyr code that is automatically translated to the equivalent, but usually much faster, data.table code."
  },
  {
    "objectID": "posts/2024-08-01-seal_of_approval-dtplyr/index.html#dtplyr",
    "href": "posts/2024-08-01-seal_of_approval-dtplyr/index.html#dtplyr",
    "title": "Seal of Approval: dtplyr",
    "section": "",
    "text": "dtplyr hex sticker\n\n\n\nAuthor(s): Hadley Wickham, Maximilian Girlich, Mark Fairbanks, Ryan Dickerson, Posit Software PBC\nMaintainer: Hadley Wickham (hadley@posit.co)\nProvides a data.table backend for dplyr. The goal of dtplyr is to allow you to write dplyr code that is automatically translated to the equivalent, but usually much faster, data.table code."
  },
  {
    "objectID": "posts/2024-08-01-seal_of_approval-dtplyr/index.html#relationship-with-data.table",
    "href": "posts/2024-08-01-seal_of_approval-dtplyr/index.html#relationship-with-data.table",
    "title": "Seal of Approval: dtplyr",
    "section": "Relationship with data.table",
    "text": "Relationship with data.table\ndtplyr is a bridge for users who are more comfortable with the dplyr syntax, but who want to take advantage of the speed and efficiency benefits of data.table. This package exactly duplicates the core functions of dplyr, but replaces the back-end source code (originally in Base R) with data.table operations."
  },
  {
    "objectID": "posts/2024-08-01-seal_of_approval-dtplyr/index.html#overview",
    "href": "posts/2024-08-01-seal_of_approval-dtplyr/index.html#overview",
    "title": "Seal of Approval: dtplyr",
    "section": "Overview",
    "text": "Overview\nExcerpted from the dtplyr vignette\nTo use dtplyr, you must at least load dtplyr and dplyr. You may also want to load data.table so you can access the other goodies that it provides:\n\nlibrary(data.table)\nlibrary(dtplyr)\nlibrary(dplyr, warn.conflicts = FALSE)\n\nThen use lazy_dt() to create a “lazy” data.table object that tracks the operations performed on it.\n\nmtcars2 &lt;- lazy_dt(mtcars)\n\nYou can preview the transformation (including the generated data.table code) by printing the result:\n\nmtcars2 %&gt;% \n  filter(wt &lt; 5) %&gt;% \n  mutate(l100k = 235.21 / mpg) %&gt;% # liters / 100 km\n  group_by(cyl) %&gt;% \n  summarise(l100k = mean(l100k))\n\nSource: local data table [3 x 2]\nCall:   `_DT1`[wt &lt; 5][, `:=`(l100k = 235.21/mpg)][, .(l100k = mean(l100k)), \n    keyby = .(cyl)]\n\n    cyl l100k\n  &lt;dbl&gt; &lt;dbl&gt;\n1     4  9.05\n2     6 12.0 \n3     8 14.9 \n\n# Use as.data.table()/as.data.frame()/as_tibble() to access results\n\n\nBut generally you should reserve this only for debugging, and use as.data.table(), as.data.frame(), or as_tibble() to indicate that you’re done with the transformation and want to access the results:\n\nmtcars2 %&gt;% \n  filter(wt &lt; 5) %&gt;% \n  mutate(l100k = 235.21 / mpg) %&gt;% # liters / 100 km\n  group_by(cyl) %&gt;% \n  summarise(l100k = mean(l100k)) %&gt;% \n  as_tibble()\n\n# A tibble: 3 × 2\n    cyl l100k\n  &lt;dbl&gt; &lt;dbl&gt;\n1     4  9.05\n2     6 12.0 \n3     8 14.9"
  },
  {
    "objectID": "posts/2024-07-31-seal_of_approval_announcement-community_team/index.html",
    "href": "posts/2024-07-31-seal_of_approval_announcement-community_team/index.html",
    "title": "Announcement: The ‘Seal of Approval’",
    "section": "",
    "text": "The Community Team, alongside a group of regular data.table contributors, is very pleased to announce a new Seal of Approval program!\nOur goal is to collect packages that significantly support, extend, or rely on data.table. We identify four broad categories of “sister” packages:\n\nExtension packages: These add to the internal functionality of data.table objects or functions.\nApplication packages: These use data.table, often “under the hood” to efficiently accomplish a particular task or analysis.\nBridge packages: These translate data.table syntax to different syntax, or provides helper functions for transitioning between data.table and another object type.\nPartner packages: These are not necessarily directly connected to data.table, but they consciously and deliberately are designed to follow the core philosophies of data.table.\n\nApproved packages may be found on the data.table GitHub; and will also appear on this blog.\n\n\n\nThis AI-generated seal definitely approves.\n\n\nTo submit a package for the Seal, please create a Pull Request on the Raft GitHub, making sure to follow all instructions carefully in the PR template.\n(You need not be the author or maintainer of a package to submit it for approval, but you must notify the current maintainer if you do so.)\nOh, and if your package is approved - shoot us an email at rdatatable@gmail.com and we’ll send you a special, limited-edition Seal of Approval sticker!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeal of Approval: dtplyr\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nSeal of Approval: nc\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\nToby Dylan Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nSeal of Approval: tidyfast\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\nTyson S. Barrett\n\n\n\n\n\n\n\n\n\n\n\n\nTwo Roads Diverged\n\n\n\n\n\n\n\n\n\n\n\nJun 4, 2024\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nTesting infrastructure for data.table\n\n\n\n\n\n\n\n\n\n\n\nMar 10, 2024\n\n\nToby Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nCommunity interviews about data.table\n\n\n\n\n\n\n\n\n\n\n\nMar 6, 2024\n\n\nAnirban Chetia\n\n\n\n\n\n\n\n\n\n\n\n\nResults of the 2023 survey\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2024\n\n\nAljaž Sluga\n\n\n\n\n\n\n\n\n\n\n\n\nColumn assignment and reference semantics in data.table\n\n\n\n\n\n\n\n\n\n\n\nFeb 18, 2024\n\n\nToby Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nThe Benefits of data.table Syntax\n\n\n\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\nTyson Barrett\n\n\n\n\n\n\n\n\n\n\n\n\nNew governance, release with new features\n\n\n\n\n\n\n\n\n\n\n\nJan 30, 2024\n\n\nToby Dylan Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nPiping data.tables\n\n\n\n\n\n\n\n\n\n\n\nJan 28, 2024\n\n\nElio Campitelli\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncement: Jan Gorecki, data.table Ambassador\n\n\n\n\n\n\n\n\n\n\n\nJan 14, 2024\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nSummary of LatinR conference\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2023\n\n\nToby Dylan Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncement: The data.table Ambassadors Travel Grant\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncement: data.table translation projects\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to the data.table ecosystem project!\n\n\nAn NSF-POSE funded venture.\n\n\n\n\n\n\n\n\nOct 15, 2023\n\n\nToby Hocking\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-10-17-translation_announcement-toby_hocking/index.html",
    "href": "posts/2023-10-17-translation_announcement-toby_hocking/index.html",
    "title": "Announcement: data.table translation projects",
    "section": "",
    "text": "In 2023-2025, National Science Foundation (NSF) has provided funds to support the project “Expanding the data.table ecosystem for efficient big data manipulation in R.” One of the goals of this project is to create translations from English to other languages, in order to make data.table more accessible."
  },
  {
    "objectID": "posts/2023-10-17-translation_announcement-toby_hocking/index.html#motivation-make-data.table-more-accessible",
    "href": "posts/2023-10-17-translation_announcement-toby_hocking/index.html#motivation-make-data.table-more-accessible",
    "title": "Announcement: data.table translation projects",
    "section": "Motivation: make data.table more accessible",
    "text": "Motivation: make data.table more accessible\ndata.table is widely used (1400+ other R packages depend on it), so it is an essential part of the R data analysis ecosystem. One of the three goals of the NSF project is to create new documentation materials, to encourage a wider diversity of users and contributors (see the other post for an overview of project goals). In this project, an important part of the documentation efforts will be creating translations of data.table documentation, in languages other than English. The goal of this translation project is to make data.table easier and more accessible, for people who do not natively speak English.\nIn fact, data.table already has its source code strings (errors, warnings, etc) translated into Chinese, which is a good start. The goal of this project will be expanding the translations to other written materials (vignettes, slides, etc), as well as other languages with a substantial R user base. Based on a prior analysis by Michael Chirico, priority languages will include Chinese, Portuguese, Spanish, French, Russian, Arabic, Hindi."
  },
  {
    "objectID": "posts/2023-10-17-translation_announcement-toby_hocking/index.html#grant-supported-translations",
    "href": "posts/2023-10-17-translation_announcement-toby_hocking/index.html#grant-supported-translations",
    "title": "Announcement: data.table translation projects",
    "section": "Grant-supported translations",
    "text": "Grant-supported translations\nThe types of materials that we would like to translate are:\n\nerrors/warnings/messages defined in strings in R and C code. These are referred to as “messages” in the terminology of gettext. The potools package by Michael Chirico can help create the files in the required format. Some method of continuous updating/maintenance will be encouraged, such as github, or the R project weblate.\nmost important vignettes (intro, import, reshape)\nother documentation (cheat sheets, slides, etc)\n\nOver the course of the NSF project (Sep 2023 to Aug 2025), there will be 20 translation awards, each US$500."
  },
  {
    "objectID": "posts/2023-10-17-translation_announcement-toby_hocking/index.html#how-to-apply",
    "href": "posts/2023-10-17-translation_announcement-toby_hocking/index.html#how-to-apply",
    "title": "Announcement: data.table translation projects",
    "section": "How to apply",
    "text": "How to apply\nApplications should be submitted to toby.hocking@r-project.org, and should include the following information:\n\nwho is the project leader? Projects which include two or more people are encouraged, in order to proof-read translations, and ensure quality. The project leader should be a trusted member of the R community, who will be responsible for proof-reading and approving other project members’ translations.\nhow many people will be translating, and how many US$500 awards are you requesting?\nwhich non-English language?\nhow many regional dialects are represented on your team? For example, Spanish dialects: Mexican, Chilean, etc.\nfor each person in your project, how many years using R and data.table?\nwhat documents/materials do you propose to translate? (messages/vignettes/other)\nwhat is your proposed timeline for completing the translation of these documents/materials?\nwhat are your plans for ensuring that your translation is consistent with other R translations? (for example, base R messages, vignettes in other packages) In detail, there are several possible ways to translate any given text from English. For example, “computer” could be translated to Spanish as “ordinator” or “computador” or “computadora” but for consistency, only one of these should be used across all R documentation.\nafter the initial translation is complete, what is your plan for continued maintenance? In other words, when the corresponding English source files are updated on GitHub, what is your plan for being aware of those updates, and making corresponding updates of your translation, for a future release of data.table?\nhow will your translation project help create an expanded data.table ecosystem of users and contributors? You should make an argument that there are a large number of native speakers of your target language, using data such as in a prior analysis by Michael Chirico.\n\nApplications will be reviewed on a monthly basis, using the following criteria:\n\nWhat is the probability of success of this translation project, and its continued maintenace?\nWill this translation project help create an expanded ecosystem of users and contributors?"
  },
  {
    "objectID": "posts/2024-02-25-survey_2023-aljaz_sluga/index.html",
    "href": "posts/2024-02-25-survey_2023-aljaz_sluga/index.html",
    "title": "Results of the 2023 survey",
    "section": "",
    "text": "Thanks to everyone who helped create, shared, or filled out the first data.table survey! The survey was officially open between October 17 and December 1 and it received 391 responses during this time.\nThis post provides a partial summary of the results. It covers all close-ended questions & includes short, informal summaries of the answers to some of the open-ended questions.\nI encourage you to explore the data yourself - you can find it here."
  },
  {
    "objectID": "posts/2024-02-25-survey_2023-aljaz_sluga/index.html#respondents",
    "href": "posts/2024-02-25-survey_2023-aljaz_sluga/index.html#respondents",
    "title": "Results of the 2023 survey",
    "section": "Respondents",
    "text": "Respondents\nA typical respondent was:\n\nan experienced user of R (87.2% having four or more years under their belt) and data.table (-||- 59.5%),\nusing the package for data manipulation (95.9%) and statistical analysis (65.3%),\nin a professional context (80%),\non a daily (46.3%) or at least weekly (28.1%) basis.\n\nFor a richer summary, here are the corresponding bar charts:"
  },
  {
    "objectID": "posts/2024-02-25-survey_2023-aljaz_sluga/index.html#the-good-and-the-bad",
    "href": "posts/2024-02-25-survey_2023-aljaz_sluga/index.html#the-good-and-the-bad",
    "title": "Results of the 2023 survey",
    "section": "The good and the bad",
    "text": "The good and the bad\nWhat do users appreciate most about data.table? A scan of the answers to this open-ended question quickly reveals a clear winner: performance. Nearly every answer brings up speed or memory efficiency.\n\n“Speed! When I need speed, I turn directly to data.table.”\n\nThe runner-up is syntax, with users praising its concision and expressiveness. At the same time, however, syntax often appears in the answers to the question about the biggest challenges in using data.table. For some users it is too concise or difficult to remember. Some users highlighted specific functionality that they find difficult to use: reshaping (dcast/melt) is brought up most often, followed by joining.\n\n“Some queries are so surprisingly simple for complex operations”\n\n\n“Still can’t get used to the syntax, have to look it up every time”\n\nWe explored this topic in a more structured way as well, by asking about the following areas:\n\nPerformance (speed & ability to handle large datasets)\nCode readability\nConcise syntax\nFew changes that break old code\nMinimal dependencies\nError messages\nDocumentation\n\nThe possible answers were Very dissatisfied, Somewhat dissatisfied, Neither satisfied nor dissatisfied, Somewhat satisfied, Very satisfied, which I mapped to -2:2 below. The majority of users are Very satisfied with performance (86.2%), minimal dependencies (77.8%), backward compatibility (60.8%), and syntax concision (57.1%). Syntax readability (35.5%), error messages (29.0%), and documentation (30.2%) lag behind.\n Does this pattern hold across all levels of data.table experience? The following plot shows the average (vertical red line) in addition to the distribution of answers across the different levels of experience.\n\nA way to contextualize these results is to consider how important the different areas are. Another grid question featured this same set of areas, but asked about their importance to the user. I standardized the satisfaction & importance scores and plot the averages below. The two areas that score relatively high in importance but relatively low in satisfaction are syntax readability and quality of documentation.\n\nAnother grid question asked about users’ satisfaction with:\n\nimporting & exporting\nfiltering\nmanipulation & aggregation\nreshaping\njoining/merging\n\nWhile Very satisfied was the dominant response for every area, the results are consistent with earlier qualitative observations in that the share of users selecting this response is substantially lower for reshaping (49.2%) and joining (45.9%) than the other areas (manipulation & aggregation 59.9%, import/export 62.7%, filtering 71.3%).\n The next plot considers variation across levels of data.table experience. One area where beginners (less than 2 years of experience) are less satisfied compared to other users is importing & exporting."
  },
  {
    "objectID": "posts/2024-02-25-survey_2023-aljaz_sluga/index.html#desired-functionality",
    "href": "posts/2024-02-25-survey_2023-aljaz_sluga/index.html#desired-functionality",
    "title": "Results of the 2023 survey",
    "section": "Desired functionality",
    "text": "Desired functionality\nWhat extra functionality would users like to see? The answers to this question covered a lot of different ground, but the three clear winners (with at least 10 mentions each) were:\n\nsupport for out-of-memory processing,\nricher import/export functionality (parquet was mentioned most often, followed by xlsx), and\nintegration with the pipe operator.\n\nPipe integration was also the subject of a later question in the survey, with the majority of users (69.4%) indicating they would find a helper function for working with the pipe useful.\n Another specific question asked about the alias for the walrus operator (:=). Interestingly, set() (47.3%) outperformed let() (39.2%), with setj() (13.6%) far behind."
  },
  {
    "objectID": "posts/2024-02-25-survey_2023-aljaz_sluga/index.html#contributing-to-data.table",
    "href": "posts/2024-02-25-survey_2023-aljaz_sluga/index.html#contributing-to-data.table",
    "title": "Results of the 2023 survey",
    "section": "Contributing to data.table",
    "text": "Contributing to data.table\nGood news for data.table is that many users indicated interest in contributing to the project. In particular, 80 respondents (20.8%) said Yes, and a further 191 (49.6%) respondents answered Maybe.\nWe followed up on this question by asking for interest in specific activities. The orange bars in the following plot represent interest in contributing, whereas the darker parts indicate actual contribution in the past.\n\nWhat would make contributing to data.table easier or more appealing? Setting aside personal reasons, such as lack of time or skill, the following areas were mentioned at least a few times each:\n\nDeveloper documentation. Probably the most common suggestion was documentation that would make it easier for new contributors to understand the codebase.\n\n“Documentation explaining the code of data.table. I mean the big picture, choices made but also some details.”\n\nGitHub issue & PR backlog. Shrinking the number of open GitHub issues and pull requests was another common suggestion.\n\n“data.table has to many open issues to efficiently search for existing issues.”\n\n\n“Getting rid of the current PR backlog would be a big step forward. I feel that a number of good PRs have died on the vine without good reason.”\n\nFast turnaround. A related suggestion was quicker reviews and evaluation of pull requests.\n\n“Some PRs take forever to be approved so this is disheartening for someone to get involved”\n\nSource code. A couple of users suggested that the source code could be restructured to make it easier to work with.\n\n“Better structured code (right now the [.data.table function is more than 2000 LOC; very hard to read and modify!). More structured test suite (right now it is a single long file with numbered tests; adding more tests can be cumbersome).”"
  },
  {
    "objectID": "posts/2024-02-25-survey_2023-aljaz_sluga/index.html#conclusion",
    "href": "posts/2024-02-25-survey_2023-aljaz_sluga/index.html#conclusion",
    "title": "Results of the 2023 survey",
    "section": "Conclusion",
    "text": "Conclusion\nThe responses to this survey make the value of data.table clear, but some users fear that the package may be abandoned or stagnating. Fear not, the project is again picking up steam! An important release with many new features and bug fixes recently landed on CRAN, and the project now has a governance document, which includes information on the different roles you can take. New contributions are very welcome, so check out the guidelines and take a look at the open issues - those labeled beginner-task are a particularly great place to start!"
  },
  {
    "objectID": "posts/2024-02-25-survey_2023-aljaz_sluga/index.html#what-can-you-do",
    "href": "posts/2024-02-25-survey_2023-aljaz_sluga/index.html#what-can-you-do",
    "title": "Results of the 2023 survey",
    "section": "What can you do?",
    "text": "What can you do?\nAre you interested in learning more, or helping grow the data.table community and infrastructure? Here are some places to start:\n\nSubmit a blog post to The Raft with your ideas, insights, or use cases of data.table.\nWeigh in on issues and community discussions at the data.table github page.\nContribute to beginner task updates or documentation needs for the package.\nApply for the data.table travel award to give a talk at a conference.\nEmail r.data.table@gmail.com to reach the NSF Grant steering committee with your thoughts and ideas."
  },
  {
    "objectID": "posts/2024-03-06-interviews-anirban_chetia/index.html",
    "href": "posts/2024-03-06-interviews-anirban_chetia/index.html",
    "title": "Community interviews about data.table",
    "section": "",
    "text": "One stipulation of NSF POSE funded projects like this one was to conduct several interviews under NSF’s I-CORPS program (Winter 2024 Cohort), to gather information as to how data.table as an open-source project can improve and remain sustainable. For four weeks starting on the 17th of January, I conducted a total of 60 interviews with R Users and data.table contributors. Issue#5880 on the data.table GitHub mentions this, and has a link to the Google Doc that contains the list of people interviewed.\n\nProject PI Toby Hocking assigned me to do these interviews and serve as the EL (Entrepreneurial Lead) for the data.table team. In addition to the interviews, this position involves tasks such as making and giving various presentations. Having successfully completed the program and conducted the interviews, it’s time to share the insights I gathered from them as a source of open-ended knowledge for the community.\nBut before you head below to read those parts, I would like to convey a big Thank you! to everyone who took part in this; from making availability and scheduling, to providing comprehensive feedback, and all the while being extremely communicative. I sincerely appreciate it. Not just for the value of your insights brought to the table, but also for being great people to talk with in general!"
  },
  {
    "objectID": "posts/2024-03-06-interviews-anirban_chetia/index.html#whats-it-all-about",
    "href": "posts/2024-03-06-interviews-anirban_chetia/index.html#whats-it-all-about",
    "title": "Community interviews about data.table",
    "section": "",
    "text": "One stipulation of NSF POSE funded projects like this one was to conduct several interviews under NSF’s I-CORPS program (Winter 2024 Cohort), to gather information as to how data.table as an open-source project can improve and remain sustainable. For four weeks starting on the 17th of January, I conducted a total of 60 interviews with R Users and data.table contributors. Issue#5880 on the data.table GitHub mentions this, and has a link to the Google Doc that contains the list of people interviewed.\n\nProject PI Toby Hocking assigned me to do these interviews and serve as the EL (Entrepreneurial Lead) for the data.table team. In addition to the interviews, this position involves tasks such as making and giving various presentations. Having successfully completed the program and conducted the interviews, it’s time to share the insights I gathered from them as a source of open-ended knowledge for the community.\nBut before you head below to read those parts, I would like to convey a big Thank you! to everyone who took part in this; from making availability and scheduling, to providing comprehensive feedback, and all the while being extremely communicative. I sincerely appreciate it. Not just for the value of your insights brought to the table, but also for being great people to talk with in general!"
  },
  {
    "objectID": "posts/2024-03-06-interviews-anirban_chetia/index.html#selected-quotes",
    "href": "posts/2024-03-06-interviews-anirban_chetia/index.html#selected-quotes",
    "title": "Community interviews about data.table",
    "section": "Selected Quotes",
    "text": "Selected Quotes\nWe begin with some direct (anonymous) quotes from the interviews giving positive feedback and general statements about data.table.\n\n“data.table would be one of the few arguments (in addition to Shiny) that I could bring forward to make people use R instead of Python”\n\n\n“Using data.table, it becomes easier to read, manipulate, and represent data in a more appropriate way for my needs”\n\n\n“If I give my script to somebody, knowing that only data.table needs to be installed is reassuring”\n\n\n“I like it for minimalism and since it’s backward compatible with data.frame”\n\n\n“data.table was small enough to put my data into RAM (noticeable copy reduction compared to dplyr) and do analyses on my old laptop”\n\n\n“Very convenient to operate on lists as columns, wherein the base structure in which mlr3 is programmed around is in essence, a data.table”\n\n\n“Not something I’d recommend to everyone because of its peculiar syntax, but once you are used to it, I believe it can be very expressive while reducing lines of code dramatically”\n\n\n“I’m happy to see the community mobilization around this package since it brings such a valuable contribution to base R and is used by so many other packages.”\n\n\n“All the f-xyzfunctions are super useful. For example, it takes ages to read a big CSV with read_csv, while using fread sometimes doesn’t even allow me to grab a cup of coffee :)”\n\nNext we will summarize some consistent themes that will help guide the grant moving forward."
  },
  {
    "objectID": "posts/2024-03-06-interviews-anirban_chetia/index.html#theme-1-contribution-of-package-development",
    "href": "posts/2024-03-06-interviews-anirban_chetia/index.html#theme-1-contribution-of-package-development",
    "title": "Community interviews about data.table",
    "section": "Theme 1: Contribution of package development",
    "text": "Theme 1: Contribution of package development\nThe first theme of the interviews was what values motivate people or prevent people in contributing to the data.table project. These varied from person to person, but I summarize a few common answers below.\n\nPositive motivations\n\nFor work reasons, wherein their core goal is to help maintain their own software/package (downstream dependencies) by contributing back.\nFor visibility or for employment where data.table is a required/preferred tool of trade, and adds to their CV.\nUsing the software for not just work but for a personal project of interest where they derive usefulness from functions exported by data.table.\nMaking one a better programmer. This comes to be not just by contributing directly, but also by learning from others’ contributions.\nTrying to be a part of the data.table community, wherein the feeling of being a part of something big and crucial tends to be in play here. Making connections in the open-source community also tends to be an attraction.\nFinancial gain or incentives from working on the project. None of the people interviewed were paid to do so, but nearly all of them agreed that a paid position could be a win-win for the interested ones. For example, the author of data.table, Matt Dowle, was able to work on the package as part of his previous paid position at H20.ai.\n\n\n\nBarriers\n\nNot being a heavy user of the package. To both be reliant on active maintenance of the currently offered functionality and to see the value of time invested in contributing, one has to use data.table often enough - something not everyone does. The pressing need to rely on the tool needs to be existent for some to contribute.\nIt is not something they use in their current toolchain. This specifically applies to the people who were former contributors or users - For them data.table is a technology they used in the past for their former work/interests, and not something they require to or would consider using for their job at present. For some, they might need an entire career switch to even use R as well!\nLack of professional motivation and brevity. Some users are tightly occupied with work and do not have the time to contribute to open-source projects, let alone data.table (would be an addition to their list of competing priorities). For the few who do, they choose not to contribute either due to the lack of incentive, or due to the lack of motivation, primarily fueled by them thinking their contributions would not be satisfactory in comparison to what they would usually contribute to for the products they develop for their job.\nThe feeling of not having adequate knowledge to make meaningful contributions. For a handful, the codebase is overwhelming, and/or they are new to GitHub itself and not sure where to start digging, and/or they are not used to working with that type or level of code in R.\nLack of C programming knowledge. Although at the surface there is R, the core still has C and thus a fair proportion of the people interviewed mentioned that their inexperience with the language is one reason that holds them back from making contributions (especially ones that involve diving a bit deeper).\nTime in between submission and merging of a PR being rather long. This resonated with a few who contributed in the past, and some were just concerned for the ones who have or are contributing, as they may opt out of future contributions if their pull requests are left hanging for a long time. Given that it takes time to be thorough with changes introduced in pull requests (PRs), there is room for understanding if they are not merged timely or fast enough since in the long run, unintended consequences are always a possibility after the foreign code has been integrated (might break stuff and make it harder to debug later on). People do feel that taking the quick and easy route in merging PRs (especially big ones) can be challenging or that contributors and reviewing volunteers have limited time, however, they also think it can be faster (especially if more people are involved) and massive delays (ranging from several weeks to months) can be avoided. (especially since people would also lose context regarding their contributions and would need to revisit the discussion as a whole)\nSome users find the tone in the wording used to bring in people to be close-ended. They would be inclined and interested to contribute if the available reading material (such as the FAQ and Readme) is more inviting and friendly.\nDiversity and inclusion: For many, English is not their primary language, and a fair amount of jargon exists while going in the vignettes and documentation. The inclusion of more people through translations would be something to look forward to. A few also referenced wanting a welcoming culture in contribution. A broader approach might be required for newcomers to open issues and pull requests. Additionally, people tend to adopt the ‘contributor mode’ when sought upon - simple things like making it explicit on the GitHub repository that contributors are needed can pull them towards deciding to push a change.\n\n\n\nSummary and Takeaways\nSection added by Kelly Bodwin\nBased on these findings, we on the grant team see three major directions for encouraging more contribution to data.table:\n\nUse in projects: Interviewees reported adding their own functionality to data.table based on needs in personal or work projects. Others cited their own lack of data.table use as a reason not to be more involved. The more we can encourage practical adoption of data.table, where it can be useful to users, the more contribution we will see from users.\nFeeling of community and culture of inclusion: This is already a focus of the grant project, and it is great to hear that this is already valued by the users and community members! We hope to vastly expand the beginner-friendliness and language diversity of documentation.\nBeginner-friendliness and support: Interviewees reported not having the programming skills to add to data.table. Going forward, we hope to better denote and emphasize the areas of contribution for less experienced programmers, and to provide more supporting resources for new community members to learn about the structure of the package.\nFinancial and professional benefits: Contributors report that developing for data.table has positive impact on professional development and hireability, and that they would welcome financial incentives as well. I believe we should experiment with structure that help support our developers in concrete ways.\nPull Request process and timeline: We believe that the newly established Governance Document for the package will help clarify and streamline the contributor process for the future."
  },
  {
    "objectID": "posts/2024-03-06-interviews-anirban_chetia/index.html#theme-2-adoption-of-data.table",
    "href": "posts/2024-03-06-interviews-anirban_chetia/index.html#theme-2-adoption-of-data.table",
    "title": "Community interviews about data.table",
    "section": "Theme 2: Adoption of data.table",
    "text": "Theme 2: Adoption of data.table\nThe second theme is what drives people to be regular users of the data.table package. We mostly focused on barriers to adpotion.\n\nIndividual reasons to not adopt\nPeople cited various reasons for not utilizing or transitioning to data.table:\n\nLow rationale to switch to data.table when dealing with small datasets. In comparison to other R packages that achieve the same functionality, notable efficiency is mostly observed when the data being dealt with is considerably large (wherein the operations performed on them scale well to see a visible difference).\nOn the flip side, there are rare cases where people found data.table to be not scalable enough, and instead use database tools like DuckDB for their datasets which are increasingly larger than memory. (Some mentioned that making operations work on-disk would be something to look out for and implement in the future). An intermediate solution for them here would be to have a syntax translator tool that would translate data.table syntax to SQL queries, similar to what dbplyr is for dplyr commands, as they prefer the data.table way to write code and only require things to be more scalable.\nNot enough resources online to learn data.table in an easy yet detailed manner. This comes in stark contrast to abundant resources available for topics such as data.frame and dplyr for instance.\nLack of an integration with tidyverse.\nNot working with a group of collaborators/coworkers who primarily use data.table.\nFor a handful, the syntax does not come to be natural although they potentially benefit from the speed. They would stick to tidyverse or Base R in terms of being easier to use, unless they are running big computations where time is key.\nSome feel that data.table requires a certain level of understanding and experience in R prior to using it. They believe that it isn’t easy for newbies to adapt reasonably quickly, with regards to their own experiences in learning it. For reasons discussed above again, people with beginner-level experience in R or not enough reason to have code be the most efficient tend to stick with easier-to-use packages and functions, which is especially common in entry-level data science courses.\n\n\n\nAreas of improvement\nSpecific areas were identified by the interviewed population, including regular users, that they would like to see improvement in or be worked upon.\nIn terms of technical improvements:\n\nPeople miss fread being able to read fixed-width files, although there is iotools now.\nAn R core member gingerly pointed out that after several years of abundant reports of installation problems on MacOS, data.table still shows as being unable to detect OpenMP support and use multiple threads on the platform (while the same is not prevalent on Windows or Linux), even after including OpenMP run-time in CRAN R releases specifically for data.table. I noticed the same issue too on OS X being prevalent till date (and it has been there for a while as it appears, as I first encountered it more than two years ago).\nFew desired additional functionality for working with spatial geometries and mixed-model packages (such as glmmTMB and lme4).\nSome people found using the Walrus operator to be weird, especially given that not every data.table operation requires that. They don’t like keeping in mind the names of columns (they tend to move towards the set function for explicitly assigning) as well. The in-place assignment using := is acceptable as they feel, but they would ideally want to be able to do DT$x &lt;- y meaning DT[, x := y].\ndataset[get(\"categoricalColumn\")] could be optimized further (for reference, please check getDTeval or this paper).\nA few people find the syntax of dcast and melt to be confusing, and often end up making mistakes since those reshaping operations complement each other (long to wide and vice versa respectively) or are the reverse. Probably not best to change this given it would break things and is just something to be learned over time, but more examples might help.\nMore often a mild inconvenience than a common source of error, but to a few, the masking of functions from other packages (such as between, first, and last from dplyr, or transpose from purrr) is something they do not like.\n\nIn terms of the community revolving around data.table:\n\nDocumentation tends to be lacking, i.e. not enough well-documented data.table resources or online materials (blogs/articles, videos) exist. Even experienced developers feel that it isn’t entirely straightforward to find out how to do more complex things, so more extensive documentation and examples would be great, if not a necessity. Things that read as friendly and expressive while maintaining details are a go-to. Making the existing documentation more lucid and navigable is another point mentioned by a few.\nMore involvement is required on Stack Exchange or QA-oriented platforms that programmers and alike frequent. People mentioned that when they are looking for answers on Stack Overflow for questions that they come across in R, they find that data.table-based answers are lacking compared to dplyr.\nMore edu-centric approaches need to be undertaken. Most educational institutions do not resort to having data.table as part of their curriculum in R-based courses. While the coursework tends to be easier for newcomers, having more resources accessible can help people learn the more optimized version right from scratch and avoid learning it in the long run (almost all of the interviewees had to explore data.table on their own!) when efficiency or just the concise way of writing things is found to be better for some. Thus, it might help if instructors can start incorporating lessons using the package.\n\n\n\nSummary and Takeaways\nSection added by Kelly Bodwin\n\nEducation and Resources: It has been clear from the outset of this project that data.table could benefit from a lot more documentation, guides, tutorials, etc. This is always a tough issue, because creating such materials can be a thankless task with not a lot of concrete payoff. However, thanks to the grant, we are able to fund time for this project! Expect good things on the horizon in this category.\nSyntax and the R sub-languages: The diversity of R syntax is a blessing and a curse, and everyone has their favorite sytax style, from tidyverse to formula style to Base R to data.table, and every combination in between. Ultimately, our goal should be to be as flexible and possible and offer ways for data.table to interface smoothly with other styles, without losing it’s core syntax structure and personality! (dtplyr and tidyfast are lovely examples of such interfacing.)\nApplicability to the problem at hand: This is an interesting one. Can we do better at defining what dataset sizes and types are the best use cases for data.table? Can we provide more options for interfacing with databases, so that users can perhaps pull data using database tools, but analyze on-disk with data.table?"
  },
  {
    "objectID": "posts/2024-03-06-interviews-anirban_chetia/index.html#theme-3-open-source-sustainability",
    "href": "posts/2024-03-06-interviews-anirban_chetia/index.html#theme-3-open-source-sustainability",
    "title": "Community interviews about data.table",
    "section": "Theme 3: Open-source sustainability",
    "text": "Theme 3: Open-source sustainability\nFinally, we asked interviewees what might be necessary to give data.table long term sustainability.\n\nDevelopers and maintainers of packages dependent on data.table said they would be concerned if it was no longer maintained. Most of them said they don’t have the resources to maintain a fork and can’t depend on something that’s not developed as much. They would be happy to sponsor and help sustain the project.\nEven if data.table were to go into maintenance mode, people would continue using it as long as the existing functionality isn’t broken.\nOccasional users are not keen to become contributors. Heavy users are but would likely step back at one point. Any external contributors (non-users with generic contributions) are not sustainable. Most people were of the opinion that there will always be an influx of contributors given that data.table is a prominent package in the R ecosystem. Thus, stability can be achieved by a set of core (active) members plus a constant inflow of newcomers or periphery (drive-by) contributors.\nCommunity growth tends to be crucial in the long run, as people recognize that motivations to contribute are fluid and subject to change in the long run (career changes or simply switching to a different focus at some point in time). It becomes essential for maintainers to share knowledge to help onboard new active contributors, and for others to connect with them (a peer network is mutually beneficial).\nAcademia is one source for bringing in contributors, as professors or researchers can pay people to do services or research work. People believe there might be ample spare money from grants or educational funding.\nBecoming part of a foundation or community can help to share struggles and grant opportunities.\nBig companies look for people who are experts in a tool they use. Since data.table is in demand for data science and related roles in the job market, this might potentially bring in more contributors to learn and be good at the software.\nGitHub Sponsors is a convenient way for people on GitHub to fund the project. Individual developers and maintainers of data.table can have their pages as they deem necessary, or some other source of sponsorship (such as Patreon or Buy Me a Coffee).\nA fair proportion is willing to contribute to such a central fund. As for how and to whom will these funds be dispersed, or what would be a good way to distribute them proportionately, remains a question.\n\n\nSummary and Takeaways\nSection added by Kelly Bodwin\nI don’t have much to add to this one - it’s clear that we need more support structure for open-source maintenance, whether from private sources or public grants or community sponsorship."
  },
  {
    "objectID": "posts/2024-03-06-interviews-anirban_chetia/index.html#anis-roadmap",
    "href": "posts/2024-03-06-interviews-anirban_chetia/index.html#anis-roadmap",
    "title": "Community interviews about data.table",
    "section": "Ani’s Roadmap",
    "text": "Ani’s Roadmap\n\nHere is my list of ideas to potentially do or keep in mind for the agenda going forward:\n\nCreating a GitHub Sponsors page for the Rdatatable organization.\nApplying for funding from organizations such as NumFocus (5675) and being a part of communities such as rOpenSci.\nApplying for R Consortium grants, and using Kickstarter crowdfunding if required (for project maintenance and implementation of complex features) in the long run.\nGetting people involved in data.table-based projects for Summer/Winter of Code programs, such as Google Summer of Code (Interested? Check our page for this year if you would like to apply or take part!).\nBrainstorming and organizing Hackathons to spur interest, or some form of data.table-based events (like how tidy-dev-day exists for tidyverse).\nPromoting data.table via conferences and open-ended blogs/articles, showcasing its features and benefits of adoption.\nCreating videos and interactive tutorials.\nClassifying more of issues (if applicable) that fall under or are labelled as ‘beginner-task’."
  }
]