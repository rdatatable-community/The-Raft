[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Raft",
    "section": "",
    "text": "Advent of Code with data.table: Week One\n\n\n\n\n\n\ntutorials\n\n\ncommunity\n\n\n\n\n\n\n\n\n\nDec 7, 2024\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nComparing data.table reshape to duckdb and polars\n\n\n\n\n\n\ntips\n\n\ntutorials\n\n\ndeveloper\n\n\nbenchmarks\n\n\n\n\n\n\n\n\n\nOct 17, 2024\n\n\nToby Dylan Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing performance regression of data.table with atime\n\n\n\n\n\n\nperformance\n\n\ntesting\n\n\ndeveloper\n\n\n\n\n\n\n\n\n\nOct 10, 2024\n\n\nDoris Afriyie Amoakohene\n\n\n\n\n\n\n\n\n\n\n\n\nSeal of Approval: mlr3\n\n\n\n\n\n\nseal of approval\n\n\napplication package\n\n\n\n\n\n\n\n\n\nOct 1, 2024\n\n\nMaximilian Mücke\n\n\n\n\n\n\n\n\n\n\n\n\nSeal of Approval: collapse\n\n\n\n\n\n\nseal of approval\n\n\npartner package\n\n\n\n\n\n\n\n\n\nSep 21, 2024\n\n\nSebastian Krantz\n\n\n\n\n\n\n\n\n\n\n\n\nNewly awarded translation projects\n\n\n\n\n\n\nannouncements\n\n\ngrant\n\n\ntranslation\n\n\n\n\n\n\n\n\n\nAug 20, 2024\n\n\nToby Dylan Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nSeal of Approval: nc\n\n\n\n\n\n\nseal of approval\n\n\nextension package\n\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\nToby Dylan Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nSeal of Approval: tidyfast\n\n\n\n\n\n\nseal of approval\n\n\nbridge package\n\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\nTyson S. Barrett\n\n\n\n\n\n\n\n\n\n\n\n\nSeal of Approval: dtplyr\n\n\n\n\n\n\nseal of approval\n\n\nbridge package\n\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncement: The ‘Seal of Approval’\n\n\n\n\n\n\nannouncements\n\n\ngrant\n\n\ncommunity\n\n\nseal of approval\n\n\n\n\n\n\n\n\n\nJul 31, 2024\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncement: Paola Corrales, data.table Ambassador\n\n\n\n\n\n\nannouncements\n\n\ngrant\n\n\nambassadors\n\n\ntravel\n\n\n\n\n\n\n\n\n\nJun 12, 2024\n\n\nCommunity Team\n\n\n\n\n\n\n\n\n\n\n\n\nTwo Roads Diverged\n\n\n\n\n\n\nopinion\n\n\n\n\n\n\n\n\n\nJun 4, 2024\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nTesting infrastructure for data.table\n\n\n\n\n\n\ngrant\n\n\ntesting\n\n\ndeveloper\n\n\n\n\n\n\n\n\n\nMar 10, 2024\n\n\nToby Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nCommunity interviews about data.table\n\n\n\n\n\n\ncommunity\n\n\ngrant\n\n\n\n\n\n\n\n\n\nMar 6, 2024\n\n\nAnirban Chetia\n\n\n\n\n\n\n\n\n\n\n\n\nResults of the 2023 survey\n\n\n\n\n\n\ncommunity\n\n\nguest post\n\n\ngovernance\n\n\n\n\n\n\n\n\n\nFeb 25, 2024\n\n\nAljaž Sluga\n\n\n\n\n\n\n\n\n\n\n\n\nColumn assignment and reference semantics in data.table\n\n\n\n\n\n\ntips\n\n\ntutorials\n\n\ndeveloper\n\n\n\n\n\n\n\n\n\nFeb 18, 2024\n\n\nToby Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nThe Benefits of data.table Syntax\n\n\n\n\n\n\ntips\n\n\ntutorials\n\n\ndocumentation\n\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\nTyson Barrett\n\n\n\n\n\n\n\n\n\n\n\n\nNew governance, release with new features\n\n\n\n\n\n\ngovernance\n\n\nreleases\n\n\n\n\n\n\n\n\n\nJan 30, 2024\n\n\nToby Dylan Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nPiping data.tables\n\n\n\n\n\n\ntips\n\n\ntutorials\n\n\ndocumentation\n\n\nguest post\n\n\n\n\n\n\n\n\n\nJan 28, 2024\n\n\nElio Campitelli\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncement: Jan Gorecki, data.table Ambassador\n\n\n\n\n\n\nannouncements\n\n\ngrant\n\n\nambassadors\n\n\n\n\n\n\n\n\n\nJan 14, 2024\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nSummary of LatinR conference\n\n\n\n\n\n\nconferences\n\n\n\n\n\n\n\n\n\nNov 19, 2023\n\n\nToby Dylan Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncement: The data.table Ambassadors Travel Grant\n\n\n\n\n\n\nannouncements\n\n\ngrant\n\n\nfunding opportunity\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncement: data.table translation projects\n\n\n\n\n\n\nannouncements\n\n\ngrant\n\n\nfunding opportunity\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to the data.table ecosystem project!\n\n\nAn NSF-POSE funded venture.\n\n\n\nannouncements\n\n\ngrant\n\n\n\n\n\n\n\n\n\nOct 15, 2023\n\n\nToby Hocking\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-09-21-seal_of_approval-collapse/index.html",
    "href": "posts/2024-09-21-seal_of_approval-collapse/index.html",
    "title": "Seal of Approval: collapse",
    "section": "",
    "text": "Author(s): Sebastian Krantz\nMaintainer: Sebastian Krantz (sebastian.krantz@graduateinstitute.ch)\n\n\n\ncollapse hex sticker\n\n\ncollapse is a large C/C++-based infrastructure package facilitating complex statistical computing, data transformation, and exploration tasks in R - at outstanding levels of performance and memory efficiency. It also implements a class-agnostic approach to R programming supporting vector, matrix and data frame-like objects (including xts, tibble, data.table, and sf). It has a stable API, depends on Rcpp, and supports R versions &gt;= 3.4.0."
  },
  {
    "objectID": "posts/2024-09-21-seal_of_approval-collapse/index.html#collapse",
    "href": "posts/2024-09-21-seal_of_approval-collapse/index.html#collapse",
    "title": "Seal of Approval: collapse",
    "section": "",
    "text": "Author(s): Sebastian Krantz\nMaintainer: Sebastian Krantz (sebastian.krantz@graduateinstitute.ch)\n\n\n\ncollapse hex sticker\n\n\ncollapse is a large C/C++-based infrastructure package facilitating complex statistical computing, data transformation, and exploration tasks in R - at outstanding levels of performance and memory efficiency. It also implements a class-agnostic approach to R programming supporting vector, matrix and data frame-like objects (including xts, tibble, data.table, and sf). It has a stable API, depends on Rcpp, and supports R versions &gt;= 3.4.0."
  },
  {
    "objectID": "posts/2024-09-21-seal_of_approval-collapse/index.html#relationship-with-data.table",
    "href": "posts/2024-09-21-seal_of_approval-collapse/index.html#relationship-with-data.table",
    "title": "Seal of Approval: collapse",
    "section": "Relationship with data.table",
    "text": "Relationship with data.table\nAt the C-level, collapse took much inspiration from data.table, and leverages some of its core algorithms like radixsort, while adding significant statistical functionality and new algorithms within a class-agnostic programming framework that seamlessly supports data.table. Notably, collapse::qDT() is a highly efficient anything to data.table converter, and all manipulation functions in collapse return a valid data.table object when a data.table is passed, enabling subsequent reference operations (:=).\nIts added functionality includes a rich set of Fast Statistical Functions supporting vectorized (grouped, weighted) statistical operations on matrix-like objects. These are integrated with fast data manipulation functions in a way that also more complex statistical expressions can be vectorized across groups. It also adds flexible time series functions and classes supporting irregular series and panels, (panel-)data transformations, vectorized hash-joins, fast aggregation and recast pivots, (internal) support for variable labels, powerful descriptive tools, memory efficient programming tools, and recursive tools for heterogeneous nested data.\nIt is highly and interactively configurable. A navigable internal documentation/overview facilitates its use."
  },
  {
    "objectID": "posts/2024-09-21-seal_of_approval-collapse/index.html#overview",
    "href": "posts/2024-09-21-seal_of_approval-collapse/index.html#overview",
    "title": "Seal of Approval: collapse",
    "section": "Overview",
    "text": "Overview\nThe easiest way to load collapse and data.table together is via the fastverse package:\n\nlibrary(fastverse)\n\n-- Attaching packages ------------------------------------------------------------------------------- fastverse 0.3.4 --\n\n\nv data.table 1.16.2     v kit        0.0.19\nv magrittr   2.0.3      v collapse   2.0.18\n\n\nThis demonstrates collapse’s deep integration with data.table.\n\nmtcarsDT &lt;- qDT(mtcars)                # This creates a valid data.table (no deep copy)\nmtcarsDT[, new := mean(mpg), by = cyl] # Proof: no warning here\n\nThere are many reasons to use collapse, e.g., to compute advanced statistics very fast:\n\n# Fast tidyverse-like functions: one of the ways to code with collapse\nmtcDTagg &lt;- mtcarsDT |&gt; \n  fgroup_by(cyl, vs, am) |&gt; \n  fsummarise(mpg_wtd_median = fmedian(mpg, wt),             # Weighted median\n             mpg_wtd_p90 = fnth(mpg, 0.9, wt, ties = \"q8\"), # Weighted 90% quantile type 8\n             mpg_wtd_mode = fmode(mpg, wt, ties = \"max\"),   # Weighted maximum mode \n             mpg_range = fmax(mpg) %-=% fmin(mpg),          # Range: vectorized and memory efficient   \n             lm_mpg_carb = fsum(mpg, W(carb)) %/=% fsum(W(carb)^2)) # coef(lm(mpg ~ carb)): vectorized\n# Note: for increased parsimony, can abbreviate fgroup_by -&gt; gby, fsummarise -&gt; smr\nmtcDTagg[, new2 := 1][1:3] # Still a data.table\n\n     cyl    vs    am mpg_wtd_median mpg_wtd_p90 mpg_wtd_mode mpg_range lm_mpg_carb  new2\n   &lt;num&gt; &lt;num&gt; &lt;num&gt;          &lt;num&gt;       &lt;num&gt;        &lt;num&gt;     &lt;num&gt;       &lt;num&gt; &lt;num&gt;\n1:     4     0     1           26.0    26.00000         26.0       0.0         NaN     1\n2:     4     1     0           22.8    24.40000         24.4       2.9         2.1     1\n3:     4     1     1           30.4    33.80484         30.4      12.5        -1.7     1\n\n\nOr simply, convenience functions like collap() for fast multi-type aggregation:\n\n# World Development Dataset (see ?wlddev)\nhead(wlddev, 3) \n\n      country iso3c       date year decade     region     income  OECD PCGDP LIFEEX GINI       ODA     POP\n1 Afghanistan   AFG 1961-01-01 1960   1960 South Asia Low income FALSE    NA 32.446   NA 116769997 8996973\n2 Afghanistan   AFG 1962-01-01 1961   1960 South Asia Low income FALSE    NA 32.962   NA 232080002 9169410\n3 Afghanistan   AFG 1963-01-01 1962   1960 South Asia Low income FALSE    NA 33.471   NA 112839996 9351441\n\n# Population weighted mean for numeric and mode for non-numeric columns (multithreaded and \n# vectorized across groups and columns, the default in statistical functions is na.rm = TRUE)\nwlddev |&gt; collap(~ year + income, fmean, fmode, w = ~ POP, nthreads = 4) |&gt; ss(1:3)\n\n        country iso3c       date year decade                region              income  OECD      PCGDP   LIFEEX GINI\n1 United States   USA 1961-01-01 1960   1960 Europe & Central Asia         High income  TRUE 12768.7126 68.59372   NA\n2      Ethiopia   ETH 1961-01-01 1960   1960    Sub-Saharan Africa          Low income FALSE   658.4778 38.33382   NA\n3         India   IND 1961-01-01 1960   1960            South Asia Lower middle income FALSE   500.7932 45.26707   NA\n         ODA       POP\n1  911825661 749495030\n2  160457982 147355735\n3 3278899549 927990163\n\n\nWe can also use the low-level API for statistical programming:\n\n# Grouped mean\nfmean(mtcars$mpg, mtcars$g) \n\n       3        4        5 \n16.10667 24.53333 21.38000 \n\n# Grouping object from multiple columns\ng &lt;- GRP(mtcars, c(\"cyl\", \"vs\", \"am\"))\nfmean(mtcars$mpg, g)\n\n   4.0.1    4.1.0    4.1.1    6.0.1    6.1.0    8.0.0    8.0.1 \n26.00000 22.90000 28.37143 20.56667 19.12500 15.05000 15.40000 \n\nvars &lt;- c(\"carb\", \"hp\", \"qsec\") # columns to aggregate\n# Aggregating: weighted mean - vectorized across groups and columns \nadd_vars(g$groups, # Grouping columns\n  fmean(get_vars(mtcars, vars), g, \n        w = mtcars$wt, use.g.names = FALSE)\n)\n\n  cyl vs am     carb        hp     qsec\n1   4  0  1 2.000000  91.00000 16.70000\n2   4  1  0 1.720045  83.60420 21.04028\n3   4  1  1 1.416115  82.11819 18.75509\n4   6  0  1 4.670296 131.78463 16.33306\n5   6  1  0 2.522685 115.32202 19.21275\n6   8  0  0 3.186582 196.74988 17.20449\n7   8  0  1 6.118694 301.60682 14.55297\n\n# Let's aggregate a matrix \nm &lt;- matrix(abs(rnorm(32^2)), 32)\nm |&gt; fmean(g) |&gt; t() |&gt; fmean(g) |&gt; t()\n\n           4.0.1     4.1.0     4.1.1     6.0.1     6.1.0     8.0.0     8.0.1\n4.0.1 0.06123789 1.4724382 0.7459940 1.5902129 0.8873607 0.6604920 0.8391957\n4.1.0 0.78205486 0.8791056 1.2617126 0.8701933 1.1794070 0.7191204 0.7533241\n4.1.1 0.66639757 0.7604432 0.8743168 0.8242863 0.8504150 0.7627944 0.8825123\n6.0.1 0.71533372 0.4045359 0.8556836 0.8525144 0.9329643 0.7946364 0.8641836\n6.1.0 1.10214877 1.2206170 0.9442454 0.9216912 0.7367946 0.7187178 0.6150456\n8.0.0 0.47671550 0.7937906 0.7432943 0.9049254 0.6613901 0.7820188 0.8534884\n8.0.1 0.94090449 0.8689585 0.7382680 1.0496066 1.2714088 0.8370710 0.5039534\n\n# Normalizing the columns, by reference\nfsum(m, TRA = \"/\", set = TRUE)\nfsum(m) # Check\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n# Multiply the rows with a vector (by reference)\nsetop(m, \"*\", mtcars$mpg, rowwise = TRUE)\n# Replace some elements with a number\nsetv(m, 3:40, 5.76) # Could also use a vector to copy from\nwhichv(m, 5.76) # get the indices back...\n\n [1]  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40\n\n\nIt is also fairly easy to do more involved data exploration and manipulation:\n\n# Groningen Growth and Development Center 10 Sector Database (see ?GGDC10S)\nnamlab(GGDC10S, N = TRUE, Ndistinct = TRUE, class = TRUE)\n\n     Variable     Class    N Ndist                                                 Label\n1     Country character 5027    43                                               Country\n2  Regioncode character 5027     6                                           Region code\n3      Region character 5027     6                                                Region\n4    Variable character 5027     2                                              Variable\n5        Year   numeric 5027    67                                                  Year\n6         AGR   numeric 4364  4353                                          Agriculture \n7         MIN   numeric 4355  4224                                                Mining\n8         MAN   numeric 4355  4353                                         Manufacturing\n9          PU   numeric 4354  4237                                             Utilities\n10        CON   numeric 4355  4339                                          Construction\n11        WRT   numeric 4355  4344                         Trade, restaurants and hotels\n12        TRA   numeric 4355  4334                  Transport, storage and communication\n13       FIRE   numeric 4355  4349 Finance, insurance, real estate and business services\n14        GOV   numeric 3482  3470                                   Government services\n15        OTH   numeric 4248  4238               Community, social and personal services\n16        SUM   numeric 4364  4364                               Summation of sector GDP\n\n# Describe total Employment and Value-Added\ndescr(GGDC10S, SUM ~ Variable)\n\nDataset: GGDC10S, 1 Variables, N = 5027\nGrouped by: Variable [2]\n        N   Perc\nEMP  2516  50.05\nVA   2511  49.95\n------------------------------------------------------------------------------------------------------------------------\nSUM (numeric): Summation of sector GDP\nStatistics (N = 4364, 13.19% NAs)\n        N   Perc  Ndist         Mean          SD     Min             Max   Skew    Kurt\nEMP  2225  50.99   2225     36846.87    96318.65  173.88          764200   5.02   30.98\nVA   2139  49.01   2139  43'961639.1  358'350627       0  8.06794210e+09  15.77  289.46\n\nQuantiles\n         1%      5%      10%      25%        50%          75%          90%         95%         99%\nEMP  256.12  599.38  1599.27  3555.62    9593.98      24801.5     66975.01   152402.28    550909.6\nVA        0   25.01   444.54    21302  243186.47  1'396139.11  15'926968.3  104'405351  692'993893\n------------------------------------------------------------------------------------------------------------------------\n\n# Compute growth rate (Employment and VA, all sectors)\nGGDC10S_growth &lt;- tfmv(GGDC10S, AGR:SUM, fgrowth, # tfmv = transform variables. Alternatively: fmutate(across(...))\n                       g = list(Country, Variable), t = Year, # Internal grouping and ordering, passed to fgrowth()\n                       apply = FALSE) # apply = FALSE ensures we call fgrowth.data.frame\n\n# Recast the dataset, median growth rate across years, taking along variable labels \nGGDC_med_growth &lt;- pivot(GGDC10S_growth,\n  ids = c(\"Country\", \"Regioncode\", \"Region\"),\n  values = slt(GGDC10S, AGR:SUM, return = \"names\"), # slt = shorthand for fselect()\n  names = list(from = \"Variable\", to = \"Sectorcode\"),\n  labels = list(to = \"Sector\"), \n  FUN = fmedian,  # Fast function = vectorized\n  how = \"recast\"  # Recast (transposition) method\n) |&gt; qDT()\nGGDC_med_growth[1:3]\n\n   Country Regioncode             Region Sectorcode       Sector        VA       EMP\n    &lt;char&gt;     &lt;char&gt;             &lt;char&gt;     &lt;fctr&gt;       &lt;fctr&gt;     &lt;num&gt;     &lt;num&gt;\n1:     BWA        SSA Sub-saharan Africa        AGR Agriculture   8.790267 0.8921475\n2:     ETH        SSA Sub-saharan Africa        AGR Agriculture   6.664964 2.5876142\n3:     GHA        SSA Sub-saharan Africa        AGR Agriculture  28.215905 1.4045550\n\n# Finally, lets just join this to wlddev, enabling multiple matches (cartesian product)\n# -&gt; on average 61 years x 11 sectors = 671 records per unique (country) match\njoin(wlddev, GGDC_med_growth, on = c(\"iso3c\" = \"Country\"), \n     how = \"inner\", multiple = TRUE) |&gt; ss(1:3)\n\ninner join: wlddev[iso3c] 2379/13176 (18.1%) &lt;61:11&gt; GGDC_med_growth[Country] 429/473 (90.7%)\n\n\n    country iso3c       date year decade                    region              income  OECD    PCGDP LIFEEX GINI\n1 Argentina   ARG 1961-01-01 1960   1960 Latin America & Caribbean Upper middle income FALSE 5642.765 65.055   NA\n2 Argentina   ARG 1961-01-01 1960   1960 Latin America & Caribbean Upper middle income FALSE 5642.765 65.055   NA\n3 Argentina   ARG 1961-01-01 1960   1960 Latin America & Caribbean Upper middle income FALSE 5642.765 65.055   NA\n        ODA      POP Regioncode        Region Sectorcode        Sector       VA        EMP\n1 219809998 20481779        LAM Latin America        AGR  Agriculture  32.91968 -0.8646301\n2 219809998 20481779        LAM Latin America        MIN        Mining 25.72799  1.5627293\n3 219809998 20481779        LAM Latin America        MAN Manufacturing 26.66754  1.0801500\n\n\nIn summary: collapse provides flexible high-performance statistical and data manipulation tools, which extend and seamlessly integrate with data.table. The package follows a similar development philosophy emphasizing API stability, parsimonious syntax, and zero dependencies (apart from Rcpp). data.table users may wish to employ collapse for some of the advanced statistical and manipulation functionality showcased above, but also to efficiently manipulate other data frame-like objects, such as sf data frames."
  },
  {
    "objectID": "posts/2024-02-18-dt_particularities-toby_hocking/index.html",
    "href": "posts/2024-02-18-dt_particularities-toby_hocking/index.html",
    "title": "Column assignment and reference semantics in data.table",
    "section": "",
    "text": "The goal of this blog post is to explain some similarities and differences between the base R data.frame object type, and the data.table object type. We will focus on accessing and assigning values, and discuss two major differences:"
  },
  {
    "objectID": "posts/2024-02-18-dt_particularities-toby_hocking/index.html#difference-in-syntax",
    "href": "posts/2024-02-18-dt_particularities-toby_hocking/index.html#difference-in-syntax",
    "title": "Column assignment and reference semantics in data.table",
    "section": "Difference in syntax",
    "text": "Difference in syntax\nTo break down the similarities and differences in syntax, consider the data below,\n\n\nSee source code\nlibrary(data.table)\nlibrary(knitr)\n\nsyntax &lt;- function(type, name, columns, code){\n  mcall &lt;- match.call()\n  dt.args &lt;- lapply(as.list(mcall[-1]), paste)\n  do.call(data.table, dt.args)\n}\n\nsyntax.tab &lt;- rbind(\n  syntax(frame, literal, one, \"df$col_name &lt;- value\"),\n  syntax(table, literal, one, \"DT[, col_name := value]\"),\n  syntax(frame, variable, multiple, 'df[, col_names_list] &lt;- values'),\n  syntax(table, variable, multiple, 'DT[, (col_names_list) := values]'))\n\nsyntax.tab |&gt; kable()\n\n\n\n\n\ntype\nname\ncolumns\ncode\n\n\n\n\nframe\nliteral\none\ndf$col_name &lt;- value\n\n\ntable\nliteral\none\nDT[, col_name := value]\n\n\nframe\nvariable\nmultiple\ndf[, col_names_list] &lt;- values\n\n\ntable\nvariable\nmultiple\nDT[, (col_names_list) := values]\n\n\n\n\n\nThe table above defines the different syntax required to do column assignment in data tables (DT) and frames (df).\n\ntype indicates object type: frame or table.\nname indicates whether the column(s) to assign are literally written in the code (col_name), or if the names are stored in a variable (col_names_list).\ncolumns indicates whether only one or multiple (one or more) columns can be assigned using the syntax.\ncode is the exact syntax of the R code used for the assignment.\n\nNote that there are other ways to do column assignment. For example,\n\nDF[[\"col_name\"]] &lt;- value can also be used for single column assignment in a data frame.\nset(DT, j=col_name_list, value=values) is a more efficient version of column assignment for data tables, that is recommended for use in loops, as it avoids the overhead of the [.data.table method.\n\nBelow is a reshaped version of the table above, to facilitate easier comparison between frame and table versions:\n\n\nSee source code\noptions(width=100)\ndata.table::dcast(syntax.tab, name + columns ~ type, value.var=\"code\")  |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\nname\ncolumns\nframe\ntable\n\n\n\n\nliteral\none\ndf$col_name &lt;- value\nDT[, col_name := value]\n\n\nvariable\nmultiple\ndf[, col_names_list] &lt;- values\nDT[, (col_names_list) := values]\n\n\n\n\n\nThe table above shows the equivalent code for assignment of columns using either a data.frame or data.table. In fact, the code in the frame column above can also be used for assignment of a data.table, but it may be less efficient than the data table square brackets, as we will discuss in the next section.\nOne reason why data.table uses a custom assignment syntax is for consistency: the same syntax can be used, with square brackets and :=, for one or multiple column assignment. (Note the use parentheses around col_names_list in the second row of the table column above, to indicate that the left side of := is a variable storing column names or numbers, instead of a direct unquoted column name.)\nAnother reason why data.table uses a custom assignment syntax is for efficiency, as we see in the next section."
  },
  {
    "objectID": "posts/2024-02-18-dt_particularities-toby_hocking/index.html#base-copy-on-write-versus-data.table-reference-semantics",
    "href": "posts/2024-02-18-dt_particularities-toby_hocking/index.html#base-copy-on-write-versus-data.table-reference-semantics",
    "title": "Column assignment and reference semantics in data.table",
    "section": "Base “copy on write” versus data.table reference semantics",
    "text": "Base “copy on write” versus data.table reference semantics\nR has “copy on write” semantics, meaning that in base R if a variable is modified inside a function, a copy is made of the whole variable. For example, consider the code below\n\ndt_outside &lt;- data.table(x=1:3)\n\nbase_assign &lt;- function(dt_inside, variable, value){\n  dt_inside[1,variable] &lt;- value # makes a copy of input variable!\n}\n\nbase_assign(dt_outside, \"x\", 0)\n\ndt_outside\n\n       x\n   &lt;int&gt;\n1:     1\n2:     2\n3:     3\n\n\nIn the code above, we pass dt_outside to the base_assign function, which makes a copy called dt_inside before it is modified, so that the data in dt_outside is unchanged after the function is done. Compare with the code below,\n\ndt_assign &lt;- function(dt_inside, variable, value){\n  dt_inside[1, (variable) := value] # directly modifies input variable\n}\n\ndt_assign(dt_outside, \"x\", 0)\n\ndt_outside\n\n       x\n   &lt;int&gt;\n1:     0\n2:     2\n3:     3\n\n\nThe output above shows that by using the square brackets and := assignment, we can modify data.table objects in functions without copying them. Here, the variables dt_inside and dt_outside point to the same underlying data.\n\nEfficiency of reference semantics\nReference semantics mean that data.table assignment is potentially much more efficient than base R, in terms of time and memory usage. To demonstrate, we use the following benchmark. Assume we have a table with \\(N\\) rows, but we just want to modify one row. This should be a constant time/space operation (independent of \\(N\\)), but because of the base R copy on write semantics, it will be a linear time/space operation, \\(O(N)\\).\n\n\nSee source code\natime_result &lt;- atime::atime(\n  N = 10^seq(1, 7, by = 0.5),\n  setup = {\n    dt &lt;- data.table(x = 1:N)\n  },\n  dt_assign = dt_assign(dt, \"x\", 0),\n  base_assign = base_assign(dt, \"x\", 0))\n\nplot(atime_result)\n\n\n\n\n\n\n\n\n\nWe can see from the plot above that for base_assign, both time and space increase with \\(N\\), because the entire table is copied; whereas dt_assign is constant time/space, because only one row is modified with no copy necessary.\n\n\n\n\n\n\nNote\n\n\n\nThe code in this section used a data.table object in both function calls to illustrate the constant time/space assignment which is possible, but the visualized result also applies to other data structures.\nAs an exercise, add two more expressions to the atime benchmark: base_assign with a data.frame object and tibble object. You should see linear time/space for both."
  },
  {
    "objectID": "posts/2024-02-18-dt_particularities-toby_hocking/index.html#conclusions",
    "href": "posts/2024-02-18-dt_particularities-toby_hocking/index.html#conclusions",
    "title": "Column assignment and reference semantics in data.table",
    "section": "Conclusions",
    "text": "Conclusions\nIn this post we have explored the syntax and semantics for assignment using base R and data.table square brackets with :=, and we have seen how the reference semantics of data.table can be very beneficial for computational efficiency."
  },
  {
    "objectID": "posts/2024-06-09-ambassador_corrales-community_team/index.html",
    "href": "posts/2024-06-09-ambassador_corrales-community_team/index.html",
    "title": "Announcement: Paola Corrales, data.table Ambassador",
    "section": "",
    "text": "A round of applause!\n\n\nPaola is a professor teaching Data Science at Guillermo Brown University in Argentina, a developer of R packages and teaching materials, and a leader of the LatinR community. She has used data.table in her research work for over 6 years, and is an active participant in many Spanish translation projects for data.table and other R packages. You can find more of her projects, teaching, and blog posts - in Spanish and English - at https://paocorrales.github.io/. Find her on Mastodon at fosstodon.org/@jangorecki to say congratulations, or chat about all things data.table."
  },
  {
    "objectID": "posts/2024-06-09-ambassador_corrales-community_team/index.html#please-join-me-in-congratulting-our-new-data.table-ambassador-paola-corrales",
    "href": "posts/2024-06-09-ambassador_corrales-community_team/index.html#please-join-me-in-congratulting-our-new-data.table-ambassador-paola-corrales",
    "title": "Announcement: Paola Corrales, data.table Ambassador",
    "section": "",
    "text": "A round of applause!\n\n\nPaola is a professor teaching Data Science at Guillermo Brown University in Argentina, a developer of R packages and teaching materials, and a leader of the LatinR community. She has used data.table in her research work for over 6 years, and is an active participant in many Spanish translation projects for data.table and other R packages. You can find more of her projects, teaching, and blog posts - in Spanish and English - at https://paocorrales.github.io/. Find her on Mastodon at fosstodon.org/@jangorecki to say congratulations, or chat about all things data.table."
  },
  {
    "objectID": "posts/2024-06-09-ambassador_corrales-community_team/index.html#talks",
    "href": "posts/2024-06-09-ambassador_corrales-community_team/index.html#talks",
    "title": "Announcement: Paola Corrales, data.table Ambassador",
    "section": "Talks",
    "text": "Talks\nAs part of his Ambassador role, Paola will be conducting a pre-conference tutorial at the 2024 UseR! meetings in Salzburg, Austria.\n\nEfficient Data Analysis with data.table\n\ndata.table is one of the most efficient open-source in-memory data manipulation packages available today. It can summarise, compute new variables, re-arrange tables and perform group-wise operations quickly, and memory efficiently thanks to its highly optimised C code. It also provides fast alternatives to base R functions for reading and writing files. This three-hour tutorial will introduce participants to data.table’s basics. Through live coding sessions and hands-on exercises, participants will learn how to use data.table as part of their data analysis pipeline; from reading data into memory to writing the results back, including exploration, data manipulation and joins. The tutorial will also lay the foundations for learning more advanced features, such as special symbols and combined operations. We will finish the tutorial with an invitation to join the data.table community and learn how to contribute to the package."
  },
  {
    "objectID": "posts/2024-06-09-ambassador_corrales-community_team/index.html#become-an-ambassador",
    "href": "posts/2024-06-09-ambassador_corrales-community_team/index.html#become-an-ambassador",
    "title": "Announcement: Paola Corrales, data.table Ambassador",
    "section": "Become an Ambassador",
    "text": "Become an Ambassador\nDo you have ideas for talks about data.table? Do you want to be part of this new community movement? Apply now for the data.table Ambassadors Grant to fund conference travel for presentations related to data.table. More details on the Ambassadors program at this blog post, and more details on the grant project itself are here.\nQuestions? Email r.data.table@gmail.com with any and all questions!"
  },
  {
    "objectID": "posts/2024-07-31-seal_of_approval_announcement-community_team/index.html",
    "href": "posts/2024-07-31-seal_of_approval_announcement-community_team/index.html",
    "title": "Announcement: The ‘Seal of Approval’",
    "section": "",
    "text": "The Community Team, alongside a group of regular data.table contributors, is very pleased to announce a new Seal of Approval program!\nOur goal is to collect packages that significantly support, extend, or rely on data.table. We identify four broad categories of “sister” packages:\n\nExtension packages: These add to the internal functionality of data.table objects or functions.\nApplication packages: These use data.table, often “under the hood” to efficiently accomplish a particular task or analysis.\nBridge packages: These translate data.table syntax to different syntax, or provides helper functions for transitioning between data.table and another object type.\nPartner packages: These are not necessarily directly connected to data.table, but they consciously and deliberately are designed to follow the core philosophies of data.table.\n\nApproved packages may be found on the data.table GitHub; and will also appear on this blog.\n\n\n\nThis AI-generated seal definitely approves.\n\n\nTo submit a package for the Seal, please create a Pull Request on the Raft GitHub, making sure to follow all instructions carefully in the PR template.\n(You need not be the author or maintainer of a package to submit it for approval, but you must notify the current maintainer if you do so.)\nOh, and if your package is approved - shoot us an email at rdatatable@gmail.com and we’ll send you a special, limited-edition Seal of Approval sticker!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvent of Code with data.table: Week One\n\n\n\n\n\n\ntutorials\n\n\ncommunity\n\n\n\n\n\n\n\n\n\nDec 7, 2024\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nComparing data.table reshape to duckdb and polars\n\n\n\n\n\n\ntips\n\n\ntutorials\n\n\ndeveloper\n\n\nbenchmarks\n\n\n\n\n\n\n\n\n\nOct 17, 2024\n\n\nToby Dylan Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing performance regression of data.table with atime\n\n\n\n\n\n\nperformance\n\n\ntesting\n\n\ndeveloper\n\n\n\n\n\n\n\n\n\nOct 10, 2024\n\n\nDoris Afriyie Amoakohene\n\n\n\n\n\n\n\n\n\n\n\n\nSeal of Approval: mlr3\n\n\n\n\n\n\nseal of approval\n\n\napplication package\n\n\n\n\n\n\n\n\n\nOct 1, 2024\n\n\nMaximilian Mücke\n\n\n\n\n\n\n\n\n\n\n\n\nSeal of Approval: collapse\n\n\n\n\n\n\nseal of approval\n\n\npartner package\n\n\n\n\n\n\n\n\n\nSep 21, 2024\n\n\nSebastian Krantz\n\n\n\n\n\n\n\n\n\n\n\n\nNewly awarded translation projects\n\n\n\n\n\n\nannouncements\n\n\ngrant\n\n\ntranslation\n\n\n\n\n\n\n\n\n\nAug 20, 2024\n\n\nToby Dylan Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nSeal of Approval: dtplyr\n\n\n\n\n\n\nseal of approval\n\n\nbridge package\n\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nSeal of Approval: nc\n\n\n\n\n\n\nseal of approval\n\n\nextension package\n\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\nToby Dylan Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nSeal of Approval: tidyfast\n\n\n\n\n\n\nseal of approval\n\n\nbridge package\n\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\nTyson S. Barrett\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncement: Paola Corrales, data.table Ambassador\n\n\n\n\n\n\nannouncements\n\n\ngrant\n\n\nambassadors\n\n\ntravel\n\n\n\n\n\n\n\n\n\nJun 12, 2024\n\n\nCommunity Team\n\n\n\n\n\n\n\n\n\n\n\n\nTwo Roads Diverged\n\n\n\n\n\n\nopinion\n\n\n\n\n\n\n\n\n\nJun 4, 2024\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nTesting infrastructure for data.table\n\n\n\n\n\n\ngrant\n\n\ntesting\n\n\ndeveloper\n\n\n\n\n\n\n\n\n\nMar 10, 2024\n\n\nToby Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nCommunity interviews about data.table\n\n\n\n\n\n\ncommunity\n\n\ngrant\n\n\n\n\n\n\n\n\n\nMar 6, 2024\n\n\nAnirban Chetia\n\n\n\n\n\n\n\n\n\n\n\n\nResults of the 2023 survey\n\n\n\n\n\n\ncommunity\n\n\nguest post\n\n\ngovernance\n\n\n\n\n\n\n\n\n\nFeb 25, 2024\n\n\nAljaž Sluga\n\n\n\n\n\n\n\n\n\n\n\n\nColumn assignment and reference semantics in data.table\n\n\n\n\n\n\ntips\n\n\ntutorials\n\n\ndeveloper\n\n\n\n\n\n\n\n\n\nFeb 18, 2024\n\n\nToby Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nThe Benefits of data.table Syntax\n\n\n\n\n\n\ntips\n\n\ntutorials\n\n\ndocumentation\n\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\nTyson Barrett\n\n\n\n\n\n\n\n\n\n\n\n\nNew governance, release with new features\n\n\n\n\n\n\ngovernance\n\n\nreleases\n\n\n\n\n\n\n\n\n\nJan 30, 2024\n\n\nToby Dylan Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nPiping data.tables\n\n\n\n\n\n\ntips\n\n\ntutorials\n\n\ndocumentation\n\n\nguest post\n\n\n\n\n\n\n\n\n\nJan 28, 2024\n\n\nElio Campitelli\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncement: Jan Gorecki, data.table Ambassador\n\n\n\n\n\n\nannouncements\n\n\ngrant\n\n\nambassadors\n\n\n\n\n\n\n\n\n\nJan 14, 2024\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nSummary of LatinR conference\n\n\n\n\n\n\nconferences\n\n\n\n\n\n\n\n\n\nNov 19, 2023\n\n\nToby Dylan Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncement: The data.table Ambassadors Travel Grant\n\n\n\n\n\n\nannouncements\n\n\ngrant\n\n\nfunding opportunity\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncement: data.table translation projects\n\n\n\n\n\n\nannouncements\n\n\ngrant\n\n\nfunding opportunity\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to the data.table ecosystem project!\n\n\nAn NSF-POSE funded venture.\n\n\n\nannouncements\n\n\ngrant\n\n\n\n\n\n\n\n\n\nOct 15, 2023\n\n\nToby Hocking\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-02-05-tips_tricks_1-tyson_barrett/index.html",
    "href": "posts/2024-02-05-tips_tricks_1-tyson_barrett/index.html",
    "title": "The Benefits of data.table Syntax",
    "section": "",
    "text": "Among the many reasons to use data.table in your code (which includes the more common answers of speed, memory efficiency, etc.) is the syntax. The syntax is\nIn this post, I’d like to show how these features are beneficial and useful in working with data regardless of the size of the data. To do this, I’ll use two packages:\nlibrary(data.table)\nlibrary(palmerpenguins)\nand we’ll create a data.table of the penguins data set (and a data.frame version for other examples):\ndt &lt;- as.data.table(penguins)\ndf &lt;- as.data.frame(penguins)\nThis post assumes some familiarity with data.table syntax but even if you are new to it, there is likely a lot of information that is quite useful for you."
  },
  {
    "objectID": "posts/2024-02-05-tips_tricks_1-tyson_barrett/index.html#concise",
    "href": "posts/2024-02-05-tips_tricks_1-tyson_barrett/index.html#concise",
    "title": "The Benefits of data.table Syntax",
    "section": "Concise",
    "text": "Concise\nThe syntax ultimately is built around the concise dt[i, j, by] framework (built on the core functionality of data frames, see the R-centric section below). This syntax allows you to:\n\nSubset (“filter”) your data using the i argument.\n\n\n# Subset to only Adelie species\ndt[species == \"Adelie\"]\n\n     species    island bill_length_mm bill_depth_mm flipper_length_mm\n      &lt;fctr&gt;    &lt;fctr&gt;          &lt;num&gt;         &lt;num&gt;             &lt;int&gt;\n  1:  Adelie Torgersen           39.1          18.7               181\n  2:  Adelie Torgersen           39.5          17.4               186\n  3:  Adelie Torgersen           40.3          18.0               195\n  4:  Adelie Torgersen             NA            NA                NA\n  5:  Adelie Torgersen           36.7          19.3               193\n ---                                                                 \n148:  Adelie     Dream           36.6          18.4               184\n149:  Adelie     Dream           36.0          17.8               195\n150:  Adelie     Dream           37.8          18.1               193\n151:  Adelie     Dream           36.0          17.1               187\n152:  Adelie     Dream           41.5          18.5               201\n     body_mass_g    sex  year\n           &lt;int&gt; &lt;fctr&gt; &lt;int&gt;\n  1:        3750   male  2007\n  2:        3800 female  2007\n  3:        3250 female  2007\n  4:          NA   &lt;NA&gt;  2007\n  5:        3450 female  2007\n ---                         \n148:        3475 female  2009\n149:        3450 female  2009\n150:        3750   male  2009\n151:        3700 female  2009\n152:        4000   male  2009\n\n\nOther ways to do this include the more redundant base R approach\n\ndf[df$species == \"Adele\"]\n\ndata frame with 0 columns and 344 rows\n\n\nand the more verbose approach in the tidyverse.\n\nlibrary(tidyverse)\ndf %&gt;% \n  filter(species == \"Adele\")\n\n\nMutate or transform your variables using the j argument. Note that the use of := mutates in place so no need for other assignment (e.g., &lt;-).\n\n\n# change body_mass_g to pounds\ndt[, body_mass_lbs := body_mass_g*0.00220462]\n\n\n\n       species body_mass_lbs\n        &lt;fctr&gt;         &lt;num&gt;\n  1:    Adelie      8.267325\n  2:    Adelie      8.377556\n  3:    Adelie      7.165015\n  4:    Adelie            NA\n  5:    Adelie      7.605939\n ---                        \n340: Chinstrap      8.818480\n341: Chinstrap      7.495708\n342: Chinstrap      8.322441\n343: Chinstrap      9.038942\n344: Chinstrap      8.322441\n\n\nWe could also do this in base R a number of ways, all of which are more redundant:\n\ndf$body_mass_lbs &lt;- df$body_mass_g*0.00220462\ndf[, \"body_mass_lbs\"] &lt;- df[, \"body_mass_g\"]*0.00220462\ndf[[\"body_mass_lbs\"]] &lt;- df[[\"body_mass_g\"]]*0.00220462\n\n\nDo all sorts of data work on groups using the by argument.\n\n\n# create a new variable that is the average of the body mass by species\ndt[, avg_mass_lbs := mean(body_mass_lbs, na.rm=TRUE), by = sex]\n\n\n\n       species    sex avg_mass_lbs\n        &lt;fctr&gt; &lt;fctr&gt;        &lt;num&gt;\n  1:    Adelie   male    10.021507\n  2:    Adelie female     8.514844\n  3:    Adelie female     8.514844\n  4:    Adelie   &lt;NA&gt;     8.830728\n  5:    Adelie female     8.514844\n ---                              \n340: Chinstrap   male    10.021507\n341: Chinstrap female     8.514844\n342: Chinstrap   male    10.021507\n343: Chinstrap   male    10.021507\n344: Chinstrap female     8.514844\n\n\nThis is more difficult, but possible, in base R to get a summary and add it to the existing data.frame:\n\ntapply(df$body_mass_lbs, df$sex, mean, na.rm=TRUE) # doesn't keep all rows\n\n# does keep all rows but complicated code\ndf &lt;- \n  by(df, INDICES = df$sex,                           \n     FUN = function(x){\n       x$avg_mass_lbs &lt;- mean(x$body_mass_lbs)\n       return(x)\n  })\ndf &lt;- do.call(\"rbind\", df)\n\nand can definitely be done in the tidyverse.\n\ndf &lt;- df %&gt;% \n  group_by(sex) %&gt;% \n  mutate(avg_mass_lbs = mean(body_mass_lbs, na.rm=TRUE)) %&gt;% \n  ungroup()\n\nIn each example, you can see a lot of work can be done in a single line of code with minimal redundancy. Although in each situation base R and tidyverse equivalents exist (often with a lot of powerful flexibility in the tidyverse approaches), the concise nature of data.table syntax can make writing and reading the code quicker."
  },
  {
    "objectID": "posts/2024-02-05-tips_tricks_1-tyson_barrett/index.html#predictable",
    "href": "posts/2024-02-05-tips_tricks_1-tyson_barrett/index.html#predictable",
    "title": "The Benefits of data.table Syntax",
    "section": "Predictable",
    "text": "Predictable\nThe syntax is naturally predictable without being verbose. For instance, whenever you use :=, it’s going to keep the same shape as the current data (“mutate”) while the use of .(var = fun(x)) will summarize to the fewest number of rows appropriate (1 row for non-grouped expressions and x rows for x number of unique groups).\nTo get an idea of how this predictability manifests in the code, we’ll use an example. Here, we can grab the average bill length by sex. We could do this two ways. The first is mutating in place where the data do not change size or shape. Note, the .() function is shorthand for list().\n\ndt[, avg_bill_length := mean(bill_length_mm, na.rm=TRUE), by = sex]\n\nThis gives us a new variable in the original data.\n\n\n       species    sex avg_bill_length\n        &lt;fctr&gt; &lt;fctr&gt;           &lt;num&gt;\n  1:    Adelie   male        45.85476\n  2:    Adelie female        42.09697\n  3:    Adelie female        42.09697\n  4:    Adelie   &lt;NA&gt;        41.30000\n  5:    Adelie female        42.09697\n ---                                 \n340: Chinstrap   male        45.85476\n341: Chinstrap female        42.09697\n342: Chinstrap   male        45.85476\n343: Chinstrap   male        45.85476\n344: Chinstrap female        42.09697\n\n\nHowever, sometimes we just want the data summarized. We can use the syntax below for that (notice no :=).\n\ndt[, .(avg_bill_length = mean(bill_length_mm, na.rm=TRUE)), by = sex]\n\n      sex avg_bill_length\n   &lt;fctr&gt;           &lt;num&gt;\n1:   male        45.85476\n2: female        42.09697\n3:   &lt;NA&gt;        41.30000\n\n\nWe can always assign this so we can access it later.\n\navg_bill &lt;- dt[, .(avg_bill_length = mean(bill_length_mm, na.rm=TRUE)), by = sex]\n\nOne way data.table makes the code predictable is that the data operations happen all within the square brackets without lingering attributes that may produce surprising results. That is, whatever I put in the brackets will be run together and then done. For example, I may have several grouping variables that I use to modify some variables, and only do it for a subset of the data.\n\ndt[species == \"Adelie\", max_bill := max(bill_length_mm, na.rm=TRUE), by = .(species, sex)]\n\nThe new variable max_bill is made for the data but is only applicable to the Adelie species and is done by both species as sex. Once this operation is done, the grouping variables are just normal variables again and we still have access to the full data.\n\n\n       species    sex max_bill\n        &lt;fctr&gt; &lt;fctr&gt;    &lt;num&gt;\n  1:    Adelie   male     46.0\n  2:    Adelie female     42.2\n  3:    Adelie female     42.2\n  4:    Adelie   &lt;NA&gt;     42.0\n  5:    Adelie female     42.2\n ---                          \n340: Chinstrap   male       NA\n341: Chinstrap female       NA\n342: Chinstrap   male       NA\n343: Chinstrap   male       NA\n344: Chinstrap female       NA"
  },
  {
    "objectID": "posts/2024-02-05-tips_tricks_1-tyson_barrett/index.html#r-centric",
    "href": "posts/2024-02-05-tips_tricks_1-tyson_barrett/index.html#r-centric",
    "title": "The Benefits of data.table Syntax",
    "section": "R-centric",
    "text": "R-centric\nAll of the main functionality in data.table is structured around vectors, lists, and (a modified form) of data frames. These core structures in R can be seeing throughout the syntax and design of the package. Even the dt[i, j, by] syntax is designed to mirror (and simplify) data frames. For new users, this can be particularly useful: no additional data structures are needed to work with the data and do both simple and complicated data operations."
  },
  {
    "objectID": "posts/2024-10-17-duckdb_polars_reshape-toby_hocking/index.html",
    "href": "posts/2024-10-17-duckdb_polars_reshape-toby_hocking/index.html",
    "title": "Comparing data.table reshape to duckdb and polars",
    "section": "",
    "text": "One element of the NSF POSE grant for data.table is to create benchmarks which can inform users about when data.table could be more performant than similar software. Two examples of similar software are duckdb and polars, which each provide in-memory database operations. This post explores the differences in computational requirements, and in functionality, for data reshaping operations."
  },
  {
    "objectID": "posts/2024-10-17-duckdb_polars_reshape-toby_hocking/index.html#wide-to-long-data-reshape-unpivot-using-data.tablemelt",
    "href": "posts/2024-10-17-duckdb_polars_reshape-toby_hocking/index.html#wide-to-long-data-reshape-unpivot-using-data.tablemelt",
    "title": "Comparing data.table reshape to duckdb and polars",
    "section": "Wide-to-long data reshape (unpivot) using data.table::melt",
    "text": "Wide-to-long data reshape (unpivot) using data.table::melt\nWide-to-long reshape is often necessary before plotting. It is perhaps best explained using a simple example. Here we consider the iris data, which has four numeric columns:\n\nlibrary(data.table)\n(iris.wide &lt;- data.table(iris))\n\n     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n            &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;    &lt;fctr&gt;\n  1:          5.1         3.5          1.4         0.2    setosa\n  2:          4.9         3.0          1.4         0.2    setosa\n  3:          4.7         3.2          1.3         0.2    setosa\n  4:          4.6         3.1          1.5         0.2    setosa\n  5:          5.0         3.6          1.4         0.2    setosa\n ---                                                            \n146:          6.7         3.0          5.2         2.3 virginica\n147:          6.3         2.5          5.0         1.9 virginica\n148:          6.5         3.0          5.2         2.0 virginica\n149:          6.2         3.4          5.4         2.3 virginica\n150:          5.9         3.0          5.1         1.8 virginica\n\n\nWhat if we wanted to make a facetted histogram of the numeric iris data columns, with one panel/facet for each column? With ggplots we would use geom_histogram(aes(numeric_variable)), where numeric_variable would be the column name of a data table containing all of the numbers that we want to show in the histogram. To construct that table, we would have to first reshape to “long” (or unpivoted) format. To easily understand what the reshape operation does, we show a subset of the data (first and last rows) below:\n\n(two.iris.wide &lt;- iris.wide[c(1,.N)])\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n          &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;    &lt;fctr&gt;\n1:          5.1         3.5          1.4         0.2    setosa\n2:          5.9         3.0          5.1         1.8 virginica\n\n\nNote the table above has 8 numbers, arranged into a table of 2 rows and 4 columns. To reshape these data to “long” (or unpivoted) format, we can use data.table::melt, as in the code below.\n\nmelt(two.iris.wide, measure.vars=measure(part, dim, sep=\".\"))\n\n     Species   part    dim value\n      &lt;fctr&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt;\n1:    setosa  Sepal Length   5.1\n2: virginica  Sepal Length   5.9\n3:    setosa  Sepal  Width   3.5\n4: virginica  Sepal  Width   3.0\n5:    setosa  Petal Length   1.4\n6: virginica  Petal Length   5.1\n7:    setosa  Petal  Width   0.2\n8: virginica  Petal  Width   1.8\n\n\nNote the table above has the same 8 numbers, but arranged into 1 column in a table with 8 rows, which is the desired input for ggplots. Also note that the reshaped column names (Petal.Length, Sepal.Width, etc) each consist of two components, which become two different columns in the output: part (Sepal or Petal) and dim (Length or Width). In the code above, we used sep=\".\" to specify that we want to split all of the iris column names using a dot, and then reshape all of the columns whose names split into the max number of items. The corresponding column names of the output are specified as the arguments of measure(), and for more info about this functionality, please read its man page.\nBelow we do the same reshape with the full iris data set, this time using a regular expression (instead of the sep argument used above),\n\n(iris.long &lt;- melt(iris.wide, measure.vars=measure(part, dim, pattern=\"(.*)[.](.*)\")))\n\n       Species   part    dim value\n        &lt;fctr&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt;\n  1:    setosa  Sepal Length   5.1\n  2:    setosa  Sepal Length   4.9\n  3:    setosa  Sepal Length   4.7\n  4:    setosa  Sepal Length   4.6\n  5:    setosa  Sepal Length   5.0\n ---                              \n596: virginica  Petal  Width   2.3\n597: virginica  Petal  Width   1.9\n598: virginica  Petal  Width   2.0\n599: virginica  Petal  Width   2.3\n600: virginica  Petal  Width   1.8\n\n\nIn the code above, the pattern argument is a Perl-compatible regular expression, and columns that match the pattern will be reshaped. The pattern must contain the same number of capture groups (parentheses) as the number of other arguments to melt (part and dim), which are used for output column names. After reshaping, we plot the data in a histogram:\n\nlibrary(ggplot2)\nggplot()+\n  geom_histogram(aes(\n    value),\n    bins=50,\n    data=iris.long)+\n  facet_grid(part ~ dim, labeller=label_both)\n\n\n\n\n\n\n\n\nWe can see in the plot above that there is a top strip for each dim and a right strip for each part, and each facet/panel contains a histogram of the corresponding subset of data."
  },
  {
    "objectID": "posts/2024-10-17-duckdb_polars_reshape-toby_hocking/index.html#wide-to-long-reshape-via-unpivot-in-polars",
    "href": "posts/2024-10-17-duckdb_polars_reshape-toby_hocking/index.html#wide-to-long-reshape-via-unpivot-in-polars",
    "title": "Comparing data.table reshape to duckdb and polars",
    "section": "Wide-to-long reshape via unpivot in polars",
    "text": "Wide-to-long reshape via unpivot in polars\n\n\n\nWe’re all friends here.\n\n\npolars is an implementation of data frames in Rust, with bindings in R and Python. In polars, the wide-to-long data reshape operation is documented on the man page for unpivot, which explains that we must specify index and/or on (no support for separator, nor regex). In our case, we use the code below:\n\n(iris.long.polars &lt;- polars::as_polars_df(iris)$unpivot(\n  index=\"Species\",\n  on=c(\"Sepal.Length\",\"Petal.Length\",\"Sepal.Width\",\"Petal.Width\"),\n  variable_name=\"part.dim\",\n  value_name=\"cm\"))\n\nshape: (600, 3)\n┌───────────┬──────────────┬─────┐\n│ Species   ┆ part.dim     ┆ cm  │\n│ ---       ┆ ---          ┆ --- │\n│ cat       ┆ str          ┆ f64 │\n╞═══════════╪══════════════╪═════╡\n│ setosa    ┆ Sepal.Length ┆ 5.1 │\n│ setosa    ┆ Sepal.Length ┆ 4.9 │\n│ setosa    ┆ Sepal.Length ┆ 4.7 │\n│ setosa    ┆ Sepal.Length ┆ 4.6 │\n│ setosa    ┆ Sepal.Length ┆ 5.0 │\n│ …         ┆ …            ┆ …   │\n│ virginica ┆ Petal.Width  ┆ 2.3 │\n│ virginica ┆ Petal.Width  ┆ 1.9 │\n│ virginica ┆ Petal.Width  ┆ 2.0 │\n│ virginica ┆ Petal.Width  ┆ 2.3 │\n│ virginica ┆ Petal.Width  ┆ 1.8 │\n└───────────┴──────────────┴─────┘\n\n\nThe output above is analogous to the result from data.table::melt, but with one column named part.dim instead of the two columns named part and dim, because polars does not support splitting the reshaped column names into more than one output column. So with polars, if we wanted separate part and dim columns, we would have to specify that in a separate step, after the reshape. Or we could just use facet_wrap instead of facet_grid, as in the code below:\n\nggplot()+\n  geom_histogram(aes(\n    cm),\n    bins=50,\n    data=iris.long.polars)+\n  facet_wrap(. ~ part.dim, labeller=label_both)\n\n\n\n\n\n\n\n\nWe can see in the plot above that there is a facet for each of the variables, but only one part.dim strip for each, instead of two strips (part and dim), as was the case for the previous plot.\n\nWide-to-long reshape via UNPIVOT in duckdb\n\n\n\n(Image generated with Adobe Firefly.)\n\n\nduckdb is a column-oriented database implemented in C++, with an R package that supports a DBI-compliant SQL interface. That means that we use R functions like DBI::dbGetQuery to get results, just like we would with any other database (Postgres, MySQL, etc). This is documented in the duckdb R API docs, which explain how to create a database connection, and then copy data from R to the database, as in the code below,\n\ncon &lt;- DBI::dbConnect(duckdb::duckdb(), dbdir = \":memory:\")\nDBI::dbWriteTable(con, \"iris_wide\", iris)\n\nThe duckdb unpivot man page explains how to do wide-to-long reshape operations, which requires specifying names of columns to reshape (no support for separator, nor regex). In our case, we use the code below:\n\niris.long.duckdb &lt;- DBI::dbGetQuery(con, '\nUNPIVOT iris_wide\nON \"Sepal.Length\", \"Petal.Length\", \"Sepal.Width\", \"Petal.Width\" \nINTO NAME part_dim \nVALUE cm')\nstr(iris.long.duckdb)\n\n'data.frame':   600 obs. of  3 variables:\n $ Species : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ part_dim: chr  \"Sepal.Length\" \"Petal.Length\" \"Sepal.Width\" \"Petal.Width\" ...\n $ cm      : num  5.1 1.4 3.5 0.2 4.9 1.4 3 0.2 4.7 1.3 ...\n\n\nAbove we use str to show a brief summary of the structure of the output, which is a data.frame with 600 rows. With duckdb, the output has one column named part_dim (dots in column names are not allowed so we use an underscore here instead), because it does not support splitting the reshaped column names into more than one output column. So with duckdb, if we wanted separate part and dim columns, we would have to specify that in a separate step, after the reshape."
  },
  {
    "objectID": "posts/2024-10-17-duckdb_polars_reshape-toby_hocking/index.html#creating-part-and-dim-columns",
    "href": "posts/2024-10-17-duckdb_polars_reshape-toby_hocking/index.html#creating-part-and-dim-columns",
    "title": "Comparing data.table reshape to duckdb and polars",
    "section": "Creating part and dim columns",
    "text": "Creating part and dim columns\nBoth polars and duckdb are not capable of producing the separate part and dim columns during the reshape operation, but we can always do it as a post-processing step. One way to do that, by specifying a separator, would be via data.table::tstrsplit, as in the code below:\n\ndata.table(iris.long.duckdb)[\n, c(\"part\",\"dim\") := tstrsplit(part_dim, split=\"[.]\")\n][]\n\n       Species     part_dim    cm   part    dim\n        &lt;fctr&gt;       &lt;char&gt; &lt;num&gt; &lt;char&gt; &lt;char&gt;\n  1:    setosa Sepal.Length   5.1  Sepal Length\n  2:    setosa Petal.Length   1.4  Petal Length\n  3:    setosa  Sepal.Width   3.5  Sepal  Width\n  4:    setosa  Petal.Width   0.2  Petal  Width\n  5:    setosa Sepal.Length   4.9  Sepal Length\n ---                                           \n596: virginica  Petal.Width   2.3  Petal  Width\n597: virginica Sepal.Length   5.9  Sepal Length\n598: virginica Petal.Length   5.1  Petal Length\n599: virginica  Sepal.Width   3.0  Sepal  Width\n600: virginica  Petal.Width   1.8  Petal  Width\n\n\nThe code above first converts to data.table, then uses the square brackets to assign new columns. Inside the square brackets, there is a walrus assignment:\n\n, comma because there is no first argument (no subset, use all rows)\nc(\"part\",\"dim\") is the left side of the walrus := assignment, which specifies the new column names to create.\non the right side of the walrus, the result of tstrsplit(part_dim,   split=\"[.]\") is used as the value to assign to the new columns (part_dim is the column to split, and \"[.]\" is the regex to use for splitting).\nSince tstrsplit returns a list of two character vectors, there will be two new columns.\n\nFinally after the walrus square brackets, we use another empty square brackets [] to enable printing (there is no printing immediately after assigning new columns using the walrus operator).\nAnother way of doing that, by specifying a regex, would be via nc::capture_first_df (recently given the data.table Seal of Approval), as in the code below:\n\nnc::capture_first_df(iris.long.duckdb, part_dim=list(\n  part=\".*\",\n  \"[.]\",\n  dim=\".*\"))\n\n       Species     part_dim    cm   part    dim\n        &lt;fctr&gt;       &lt;char&gt; &lt;num&gt; &lt;char&gt; &lt;char&gt;\n  1:    setosa Sepal.Length   5.1  Sepal Length\n  2:    setosa Petal.Length   1.4  Petal Length\n  3:    setosa  Sepal.Width   3.5  Sepal  Width\n  4:    setosa  Petal.Width   0.2  Petal  Width\n  5:    setosa Sepal.Length   4.9  Sepal Length\n ---                                           \n596: virginica  Petal.Width   2.3  Petal  Width\n597: virginica Sepal.Length   5.9  Sepal Length\n598: virginica Petal.Length   5.1  Petal Length\n599: virginica  Sepal.Width   3.0  Sepal  Width\n600: virginica  Petal.Width   1.8  Petal  Width\n\n\nThe code above specifies:\n\ncapture_first_df, a function for applying capturing regex to columns of a data frame;\niris.long.duckdb is the input data frame, in which there is the part_dim column to split;\npart=\".*\", \"[.]\", dim=\".*\" makes the capturing regex; R argument names are used to define the new column names, based on the text captured in the corresponding regex (\".*\" means zero or more non-newline characters).\n\nBoth results above are data tables with extra cols part and dim. For visualization, these data tables could be used with either facet_grid or facet_wrap, similar to the examples above."
  },
  {
    "objectID": "posts/2024-10-17-duckdb_polars_reshape-toby_hocking/index.html#reshape-into-multiple-columns",
    "href": "posts/2024-10-17-duckdb_polars_reshape-toby_hocking/index.html#reshape-into-multiple-columns",
    "title": "Comparing data.table reshape to duckdb and polars",
    "section": "Reshape into multiple columns",
    "text": "Reshape into multiple columns\nAnother kind of wide-to-long reshape involves reshaping into multiple columns. For example, in the iris data, we may wonder whether sepals are larger than petals (in terms of both length and width). To answer that question, we could make a scatterplot of y=Sepal versus x=Petal, with a facet/panel for each dimension (Length and Width). In the ggplot system, we would need to compute a data table with columns Sepal, Petal, and dim, and we can do that by specifying the value.name keyword to measure(), as in the code below:\n\n(iris.long.parts &lt;- melt(iris.wide, measure.vars=measure(value.name, dim, sep=\".\")))\n\n       Species    dim Sepal Petal\n        &lt;fctr&gt; &lt;char&gt; &lt;num&gt; &lt;num&gt;\n  1:    setosa Length   5.1   1.4\n  2:    setosa Length   4.9   1.4\n  3:    setosa Length   4.7   1.3\n  4:    setosa Length   4.6   1.5\n  5:    setosa Length   5.0   1.4\n ---                             \n296: virginica  Width   3.0   2.3\n297: virginica  Width   2.5   1.9\n298: virginica  Width   3.0   2.0\n299: virginica  Width   3.4   2.3\n300: virginica  Width   3.0   1.8\n\n\nAgain, the measure() function in the code above operates by splitting the input column names using sep, which results in two groups (Sepal.Width split into Sepal and Width, etc) for each of the measured columns. The value.name keyword indicates that each unique value in the first group (Sepal and Petal) should be used as the name of an output column. This functionality can be very convenient for some data reshaping tasks, but it is neither supported in polars, nor in duckdb. Going back to our original motivating problem, we can make the scatterplot using the code below,\n\nggplot()+\n  theme_bw()+\n  geom_abline(slope=1, intercept=0, color=\"grey\")+\n  geom_point(aes(\n    Petal, Sepal),\n    data=iris.long.parts)+\n  facet_grid(. ~ dim, labeller=label_both)+\n  coord_equal()\n\n\n\n\n\n\n\n\nFrom the plot above, we see that all of the data points (black) are above the y=x line (grey), so we can conclude that sepals are indeed larger than petals, in terms of both length and width."
  },
  {
    "objectID": "posts/2024-10-17-duckdb_polars_reshape-toby_hocking/index.html#wide-to-long-performance-comparison",
    "href": "posts/2024-10-17-duckdb_polars_reshape-toby_hocking/index.html#wide-to-long-performance-comparison",
    "title": "Comparing data.table reshape to duckdb and polars",
    "section": "Wide-to-long performance comparison",
    "text": "Wide-to-long performance comparison\nWe may also wonder which data reshaping functions work fastest for large data. To answer that question, we will use atime, which is an R package that allows us to see how much time/memory is required for computations in R, as a function of data size N. In the setup argument of the code below, we repeat the iris data for a certain number of rows N. The code in the other arguments is run for the time/memory measurement, and is very similar to the code presented in previous sections. One difference is that for data.table we use id.vars instead of measure(), to more closely match the arguments provided to the other unpivot functions (for a more fair comparison).\n\nseconds.limit &lt;- 0.1\nunpivot.res &lt;- atime::atime(\n  N=2^seq(1,50),\n  setup={\n    (row.id.vec &lt;- 1+(seq(0,N-1) %% nrow(iris)))\n    N.df &lt;- iris[row.id.vec,]\n    N.dt &lt;- data.table(N.df)\n    polars_df &lt;- polars::as_polars_df(N.df)\n    duckdb::dbWriteTable(con, \"iris_table\", N.df, overwrite=TRUE)\n  },\n  seconds.limit=seconds.limit,\n  \"duckdb\\nUNPIVOT\"=DBI::dbGetQuery(con, 'UNPIVOT iris_table ON \"Sepal.Length\", \"Petal.Length\", \"Sepal.Width\", \"Petal.Width\" INTO NAME part_dim VALUE cm'),\n  \"polars\\nunpivot\"=polars_df$unpivot(index=\"Species\", value_name=\"cm\"),\n  \"data.table\\nmelt\"=melt(N.dt, id.vars=\"Species\", value.name=\"cm\"))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\nunpivot.refs &lt;- atime::references_best(unpivot.res)\nunpivot.pred &lt;- predict(unpivot.refs)\nplot(unpivot.pred)+coord_cartesian(xlim=c(1e1,1e7))\n\nLoading required namespace: directlabels\n\n\nWarning in ggplot2::scale_x_log10(\"N\", breaks = meas[,\n10^seq(ceiling(min(log10(N))), : log-10 transformation introduced infinite\nvalues.\n\n\n\n\n\n\n\n\n\nIn the plot above, the computation time in seconds is plotted as a function of N, the number of input rows to reshape. The horizontal reference line is drawn at 0.1 seconds, and the N highlighted corresponds to the throughput given that time limit. When we compare the N values shown for the different methods, we see that data.table is comparable to polars (within 2x), and both are much faster than duckdb (about 10x).\nAbove there are several confounding factors in the comparison, most notably that data must be copied to duckdb and polars before and after processing. In contrast, data.table provides setDT and setDF functions, which can convert to/from data tables, without copying. So when data originates in R, or needs to come back to R, we should include the copy time for a more fair comparison. Below we run that comparison:\n\nseconds.limit &lt;- 0.1\nunpivot.copy.res &lt;- atime::atime(\n  N=2^seq(1,50),\n  setup={\n    (row.id.vec &lt;- 1+(seq(0,N-1) %% nrow(iris)))\n    N.df &lt;- iris[row.id.vec,]\n  },\n  seconds.limit=seconds.limit,\n  \"duckdb\\ncopy+UNPIVOT\"={\n    duckdb::dbWriteTable(con, \"iris_table\", N.df, overwrite=TRUE)\n    DBI::dbGetQuery(con, 'UNPIVOT iris_table ON \"Sepal.Length\", \"Petal.Length\", \"Sepal.Width\", \"Petal.Width\" INTO NAME part_dim VALUE cm')\n  },\n  \"polars\\ncopy+unpivot\"={\n    polars_df &lt;- polars::as_polars_df(N.df)\n    polars_unpivot &lt;- polars_df$unpivot(index=\"Species\", value_name=\"cm\")\n    as.data.frame(polars_unpivot)\n  },\n  \"data.table\\nset+melt\"=setDF(melt(setDT(N.df), id.vars=\"Species\", value.name=\"cm\")))\nunpivot.copy.refs &lt;- atime::references_best(unpivot.copy.res)\nunpivot.copy.pred &lt;- predict(unpivot.copy.refs)\nplot(unpivot.copy.pred)+coord_cartesian(xlim=c(1e1,1e7))\n\nWarning in ggplot2::scale_x_log10(\"N\", breaks = meas[,\n10^seq(ceiling(min(log10(N))), : log-10 transformation introduced infinite\nvalues.\n\n\n\n\n\n\n\n\n\nThe result above shows that data.table is most efficient in terms of computation time. In this comparison, data.table is clearly faster than polars (about 10x), and much faster than duckdb (about 100x)."
  },
  {
    "objectID": "posts/2024-10-17-duckdb_polars_reshape-toby_hocking/index.html#wide-to-long-summary-of-functionality",
    "href": "posts/2024-10-17-duckdb_polars_reshape-toby_hocking/index.html#wide-to-long-summary-of-functionality",
    "title": "Comparing data.table reshape to duckdb and polars",
    "section": "Wide-to-long summary of functionality",
    "text": "Wide-to-long summary of functionality\n\n\n\nWide to long.\n\n\nIn this section, we showed that data.table provides an efficient and feature-rich implementation of wide-to-long data reshaping.\n\nmeasure() allows specification of columns to reshape using either a separator or a regular expression pattern. In contrast, duckdb nor polars require specifying input column names (no support for separator, nor regex), and output column post-processing, which is less convenient.\nThe value.name keyword can be used to reshape into multiple output columns, which is required for some kinds of reshape operations (no way to do that in duckdb/polars).\nsetDT and setDF can be used to avoid un-necessary copies with data.table. In contrast, duckdb/polars require copies to/from regular R memory, which can add significant time/memory requirements.\ndata.table was fastest and most memory efficient in the comparisons we examined (both with and without consideration of copying).\n\nThe table below summarizes support for different features in each software package (dash - means no support).\n\n\n\n\n\n\n\n\n\nhow to specify\ndata.table\npolars\nduckdb\n\n\n\n\nfunction\nmelt\nunpivot\nUNPIVOT\n\n\nreshape cols\nmeasure.vars\non\nON\n\n\nother cols\nid.vars\nindex\n-\n\n\noutput name (data)\nvalue.name\nvalue_name\nVALUE\n\n\noutput name (columns)\nvariable.name\nvariable_name\nINTO NAME\n\n\nseparator\nsep\n-\n-\n\n\nregex\npattern\n-\n-\n\n\nmultiple outputs\nvalue.name\n-\n-\n\n\navoid copies\nsetDT, setDF\n-\n-"
  },
  {
    "objectID": "posts/2024-10-17-duckdb_polars_reshape-toby_hocking/index.html#long-to-wide-data-reshape-using-data.tabledcast",
    "href": "posts/2024-10-17-duckdb_polars_reshape-toby_hocking/index.html#long-to-wide-data-reshape-using-data.tabledcast",
    "title": "Comparing data.table reshape to duckdb and polars",
    "section": "Long-to-wide data reshape using data.table::dcast",
    "text": "Long-to-wide data reshape using data.table::dcast\nHere we continue with the iris data example. We will present three different reshape operations involving the iris data. The code below adds a column flower which contains the row number.\n\niris.wide[, flower := .I][]\n\n     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species flower\n            &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;    &lt;fctr&gt;  &lt;int&gt;\n  1:          5.1         3.5          1.4         0.2    setosa      1\n  2:          4.9         3.0          1.4         0.2    setosa      2\n  3:          4.7         3.2          1.3         0.2    setosa      3\n  4:          4.6         3.1          1.5         0.2    setosa      4\n  5:          5.0         3.6          1.4         0.2    setosa      5\n ---                                                                   \n146:          6.7         3.0          5.2         2.3 virginica    146\n147:          6.3         2.5          5.0         1.9 virginica    147\n148:          6.5         3.0          5.2         2.0 virginica    148\n149:          6.2         3.4          5.4         2.3 virginica    149\n150:          5.9         3.0          5.1         1.8 virginica    150\n\n\nThen we do a wide-to-long reshape using the code below (same as previous section),\n\n(iris.long.i &lt;- melt(iris.wide, measure.vars=measure(part, dim, sep=\".\")))\n\n       Species flower   part    dim value\n        &lt;fctr&gt;  &lt;int&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt;\n  1:    setosa      1  Sepal Length   5.1\n  2:    setosa      2  Sepal Length   4.9\n  3:    setosa      3  Sepal Length   4.7\n  4:    setosa      4  Sepal Length   4.6\n  5:    setosa      5  Sepal Length   5.0\n ---                                     \n596: virginica    146  Petal  Width   2.3\n597: virginica    147  Petal  Width   1.9\n598: virginica    148  Petal  Width   2.0\n599: virginica    149  Petal  Width   2.3\n600: virginica    150  Petal  Width   1.8\n\n\nThe table above has an additional column for flower, which we use in the code below on the left side of the formula (used to define output rows), along with part + dim on the right side of the formula (used to define output columns). The code below can therefore be used to reshape the data back into their original wide format:\n\ndcast(# wide reshape 1\n  data=iris.long.i,\n  formula=flower + Species ~ part + dim,\n  sep=\".\")\n\nKey: &lt;flower, Species&gt;\n     flower   Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n      &lt;int&gt;    &lt;fctr&gt;        &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;\n  1:      1    setosa          1.4         0.2          5.1         3.5\n  2:      2    setosa          1.4         0.2          4.9         3.0\n  3:      3    setosa          1.3         0.2          4.7         3.2\n  4:      4    setosa          1.5         0.2          4.6         3.1\n  5:      5    setosa          1.4         0.2          5.0         3.6\n ---                                                                   \n146:    146 virginica          5.2         2.3          6.7         3.0\n147:    147 virginica          5.0         1.9          6.3         2.5\n148:    148 virginica          5.2         2.0          6.5         3.0\n149:    149 virginica          5.4         2.3          6.2         3.4\n150:    150 virginica          5.1         1.8          5.9         3.0\n\n\nWe can see that the result above is almost the same as the original iris data (but with the columns in a different order). Another kind of reshape involves computing an aggregation function, such as mean. Note in the code below that . on the right side of the formula indicates a single output column.\n\ndcast(# wide reshape 2\n  data=iris.long.i,\n  formula=Species + part + dim ~ .,\n  fun.aggregate=mean,\n  sep=\".\")\n\nKey: &lt;Species, part, dim&gt;\n       Species   part    dim     .\n        &lt;fctr&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt;\n 1:     setosa  Petal Length 1.462\n 2:     setosa  Petal  Width 0.246\n 3:     setosa  Sepal Length 5.006\n 4:     setosa  Sepal  Width 3.428\n 5: versicolor  Petal Length 4.260\n 6: versicolor  Petal  Width 1.326\n 7: versicolor  Sepal Length 5.936\n 8: versicolor  Sepal  Width 2.770\n 9:  virginica  Petal Length 5.552\n10:  virginica  Petal  Width 2.026\n11:  virginica  Sepal Length 6.588\n12:  virginica  Sepal  Width 2.974\n\n\nThe output above has a row for every unique combination of Species, part, and dim, and a column (.)` for the mean of the corresponding data. The more complex reshape below involves multiple aggregations, and multiple value variables.\n\noptions(width=100)\ndcast(# wide reshape 3\n  data=iris.long.parts,\n  formula=dim ~ Species,\n  fun.aggregate=list(mean,sd),\n  value.var=c(\"Sepal\",\"Petal\"))\n\nKey: &lt;dim&gt;\n      dim Sepal_mean_setosa Sepal_mean_versicolor Sepal_mean_virginica Petal_mean_setosa\n   &lt;char&gt;             &lt;num&gt;                 &lt;num&gt;                &lt;num&gt;             &lt;num&gt;\n1: Length             5.006                 5.936                6.588             1.462\n2:  Width             3.428                 2.770                2.974             0.246\n   Petal_mean_versicolor Petal_mean_virginica Sepal_sd_setosa Sepal_sd_versicolor\n                   &lt;num&gt;                &lt;num&gt;           &lt;num&gt;               &lt;num&gt;\n1:                 4.260                5.552       0.3524897           0.5161711\n2:                 1.326                2.026       0.3790644           0.3137983\n   Sepal_sd_virginica Petal_sd_setosa Petal_sd_versicolor Petal_sd_virginica\n                &lt;num&gt;           &lt;num&gt;               &lt;num&gt;              &lt;num&gt;\n1:          0.6358796       0.1736640           0.4699110          0.5518947\n2:          0.3224966       0.1053856           0.1977527          0.2746501\n\n\nThe output above includes two rows, and a column for every unique combination of value.var (Sepal or Petal), of fun.aggregate (mean or sd), and of Species (setosa, versicolor, virginica)."
  },
  {
    "objectID": "posts/2024-10-17-duckdb_polars_reshape-toby_hocking/index.html#long-to-wide-reshape-in-polars",
    "href": "posts/2024-10-17-duckdb_polars_reshape-toby_hocking/index.html#long-to-wide-reshape-in-polars",
    "title": "Comparing data.table reshape to duckdb and polars",
    "section": "Long-to-wide reshape in polars",
    "text": "Long-to-wide reshape in polars\npolars supports long-to-wide reshape via the pivot method, as in the code below.\n\n(polars.wide &lt;- polars::as_polars_df(\n  iris.long.i\n)$pivot(# wide reshape 1\n  on=c(\"part\",\"dim\"),\n  index=c(\"flower\",\"Species\"),\n  values=\"value\"))\n\nshape: (150, 6)\n┌────────┬───────────┬───────────────────┬───────────────────┬──────────────────┬──────────────────┐\n│ flower ┆ Species   ┆ {\"Sepal\",\"Length\" ┆ {\"Sepal\",\"Width\"} ┆ {\"Petal\",\"Length ┆ {\"Petal\",\"Width\" │\n│ ---    ┆ ---       ┆ }                 ┆ ---               ┆ \"}               ┆ }                │\n│ i32    ┆ cat       ┆ ---               ┆ f64               ┆ ---              ┆ ---              │\n│        ┆           ┆ f64               ┆                   ┆ f64              ┆ f64              │\n╞════════╪═══════════╪═══════════════════╪═══════════════════╪══════════════════╪══════════════════╡\n│ 1      ┆ setosa    ┆ 5.1               ┆ 3.5               ┆ 1.4              ┆ 0.2              │\n│ 2      ┆ setosa    ┆ 4.9               ┆ 3.0               ┆ 1.4              ┆ 0.2              │\n│ 3      ┆ setosa    ┆ 4.7               ┆ 3.2               ┆ 1.3              ┆ 0.2              │\n│ 4      ┆ setosa    ┆ 4.6               ┆ 3.1               ┆ 1.5              ┆ 0.2              │\n│ 5      ┆ setosa    ┆ 5.0               ┆ 3.6               ┆ 1.4              ┆ 0.2              │\n│ …      ┆ …         ┆ …                 ┆ …                 ┆ …                ┆ …                │\n│ 146    ┆ virginica ┆ 6.7               ┆ 3.0               ┆ 5.2              ┆ 2.3              │\n│ 147    ┆ virginica ┆ 6.3               ┆ 2.5               ┆ 5.0              ┆ 1.9              │\n│ 148    ┆ virginica ┆ 6.5               ┆ 3.0               ┆ 5.2              ┆ 2.0              │\n│ 149    ┆ virginica ┆ 6.2               ┆ 3.4               ┆ 5.4              ┆ 2.3              │\n│ 150    ┆ virginica ┆ 5.9               ┆ 3.0               ┆ 5.1              ┆ 1.8              │\n└────────┴───────────┴───────────────────┴───────────────────┴──────────────────┴──────────────────┘\n\nnames(polars.wide)\n\n[1] \"flower\"                 \"Species\"                \"{\\\"Sepal\\\",\\\"Length\\\"}\"\n[4] \"{\\\"Sepal\\\",\\\"Width\\\"}\"  \"{\\\"Petal\\\",\\\"Length\\\"}\" \"{\\\"Petal\\\",\\\"Width\\\"}\" \n\n\nThe output above is consistent with the results from data.table::dcast, and the original iris data, although the names are unusual (with curly braces and double quotes). The next reshape example below shows that we need to create a dummy variable to use as the on argument.\n\npolars::as_polars_df(\n  iris.long.i[, dummy := \".\"]\n)$pivot(# wide reshape 2\n  on=\"dummy\", # have to create dummy var for on.\n  index=c(\"Species\",\"part\",\"dim\"),\n  values=\"value\",\n  aggregate_function=\"mean\")\n\nshape: (12, 4)\n┌────────────┬───────┬────────┬───────┐\n│ Species    ┆ part  ┆ dim    ┆ .     │\n│ ---        ┆ ---   ┆ ---    ┆ ---   │\n│ cat        ┆ str   ┆ str    ┆ f64   │\n╞════════════╪═══════╪════════╪═══════╡\n│ setosa     ┆ Sepal ┆ Length ┆ 5.006 │\n│ versicolor ┆ Sepal ┆ Length ┆ 5.936 │\n│ virginica  ┆ Sepal ┆ Length ┆ 6.588 │\n│ setosa     ┆ Sepal ┆ Width  ┆ 3.428 │\n│ versicolor ┆ Sepal ┆ Width  ┆ 2.77  │\n│ …          ┆ …     ┆ …      ┆ …     │\n│ versicolor ┆ Petal ┆ Length ┆ 4.26  │\n│ virginica  ┆ Petal ┆ Length ┆ 5.552 │\n│ setosa     ┆ Petal ┆ Width  ┆ 0.246 │\n│ versicolor ┆ Petal ┆ Width  ┆ 1.326 │\n│ virginica  ┆ Petal ┆ Width  ┆ 2.026 │\n└────────────┴───────┴────────┴───────┘\n\n\nThe output above is consistent with the results from data.table::dcast. Currently polars only supports a single aggregation function, so we can not calculate both mean and sd at the same time, but we can at least do the mean for multiple values in the code below:\n\npolars::as_polars_df(\n  iris.long.parts\n)$pivot(# wide reshape 3\n  on=\"Species\",\n  index=\"dim\",\n  values=c(\"Sepal\",\"Petal\"),\n  aggregate_function=\"mean\")#multiple agg not supported.\n\nshape: (2, 7)\n┌────────┬──────────────┬──────────────┬──────────────┬──────────────┬──────────────┬──────────────┐\n│ dim    ┆ Sepal_setosa ┆ Sepal_versic ┆ Sepal_virgin ┆ Petal_setosa ┆ Petal_versic ┆ Petal_virgin │\n│ ---    ┆ ---          ┆ olor         ┆ ica          ┆ ---          ┆ olor         ┆ ica          │\n│ str    ┆ f64          ┆ ---          ┆ ---          ┆ f64          ┆ ---          ┆ ---          │\n│        ┆              ┆ f64          ┆ f64          ┆              ┆ f64          ┆ f64          │\n╞════════╪══════════════╪══════════════╪══════════════╪══════════════╪══════════════╪══════════════╡\n│ Length ┆ 5.006        ┆ 5.936        ┆ 6.588        ┆ 1.462        ┆ 4.26         ┆ 5.552        │\n│ Width  ┆ 3.428        ┆ 2.77         ┆ 2.974        ┆ 0.246        ┆ 1.326        ┆ 2.026        │\n└────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┘\n\n\nAbove we see the result only has 6 columns (for mean), whereas the analogous result from data.table::dcast above had 12 columns (with additionally the sd)."
  },
  {
    "objectID": "posts/2024-10-17-duckdb_polars_reshape-toby_hocking/index.html#long-to-wide-reshape-in-duckdb",
    "href": "posts/2024-10-17-duckdb_polars_reshape-toby_hocking/index.html#long-to-wide-reshape-in-duckdb",
    "title": "Comparing data.table reshape to duckdb and polars",
    "section": "Long-to-wide reshape in duckdb",
    "text": "Long-to-wide reshape in duckdb\nduckdb supports long-to-wide reshape via the SQL PIVOT command, which can be used to recover the original iris data via the command below:\n\nduckdb::dbWriteTable(con, \"iris_long_i\", iris.long.i, overwrite=TRUE)\niris.wide.again.duckdb &lt;- DBI::dbGetQuery(# wide reshape 1\n  con, '\nPIVOT iris_long_i \nON part,dim \nUSING sum(value) \nGROUP BY flower,Species \nORDER BY flower')\nstr(iris.wide.again.duckdb)\n\n'data.frame':   150 obs. of  6 variables:\n $ flower      : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ Petal_Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal_Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Sepal_Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal_Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n\n\nWe can see that the result above is consistent with the previous sections. The code below uses mean as an aggregation function.\n\nDBI::dbGetQuery(# wide reshape 2\n  con, '\nPIVOT iris_long_i \nUSING mean(value) \nAS \".\" \nGROUP BY Species,part,dim')\n\n      Species  part    dim     .\n1  versicolor Petal Length 4.260\n2  versicolor Petal  Width 1.326\n3      setosa Sepal Length 5.006\n4      setosa Sepal  Width 3.428\n5   virginica Petal Length 5.552\n6   virginica Petal  Width 2.026\n7  versicolor Sepal Length 5.936\n8   virginica Sepal Length 6.588\n9  versicolor Sepal  Width 2.770\n10  virginica Sepal  Width 2.974\n11     setosa Petal Length 1.462\n12     setosa Petal  Width 0.246\n\n\nThe result above is consistent with previous results. Finally, we can do multiple aggregations via the code below, which requires enumerating each combination of aggregation function and input column to aggregate.\n\nduckdb::dbWriteTable(con, \"iris_long_parts\", iris.long.parts, overwrite=TRUE)\nDBI::dbGetQuery(# wide reshape 3\n  con, '\nPIVOT iris_long_parts \nON Species \nUSING \n mean(Sepal) AS Sepal_mean, \n stddev(Sepal) AS Sepal_sd, \n mean(Petal) AS Petal_mean, \n stddev(Petal) AS Petal_sd \nGROUP BY dim')\n\n     dim setosa_Sepal_mean setosa_Sepal_sd setosa_Petal_mean setosa_Petal_sd versicolor_Sepal_mean\n1 Length             5.006       0.3524897             1.462       0.1736640                 5.936\n2  Width             3.428       0.3790644             0.246       0.1053856                 2.770\n  versicolor_Sepal_sd versicolor_Petal_mean versicolor_Petal_sd virginica_Sepal_mean\n1           0.5161711                 4.260           0.4699110                6.588\n2           0.3137983                 1.326           0.1977527                2.974\n  virginica_Sepal_sd virginica_Petal_mean virginica_Petal_sd\n1          0.6358796                5.552          0.5518947\n2          0.3224966                2.026          0.2746501\n\n\nThe result above is consistent with the result from data.table::dcast. Because all combinations of aggregation/columns must be enumerated, the duckdb code is a bit more repetitive than the corresponding data.table code (which is more convenient)."
  },
  {
    "objectID": "posts/2024-10-17-duckdb_polars_reshape-toby_hocking/index.html#long-to-wide-performance-comparison",
    "href": "posts/2024-10-17-duckdb_polars_reshape-toby_hocking/index.html#long-to-wide-performance-comparison",
    "title": "Comparing data.table reshape to duckdb and polars",
    "section": "Long-to-wide performance comparison",
    "text": "Long-to-wide performance comparison\nBelow we conduct an atime benchmark to measure the computation time of the reshape operation (without controlling for the copy operation).\n\nseconds.limit &lt;- 0.1\npivot.res &lt;- atime::atime(\n  N=2^seq(1,50),\n  setup={\n    (row.id.vec &lt;- 1+(seq(0,N-1) %% nrow(iris.long.i)))\n    N.dt &lt;- iris.long.i[row.id.vec]\n    N.df &lt;- data.frame(N.dt)\n    N_polars &lt;- polars::as_polars_df(N.df)\n    duckdb::dbWriteTable(con, \"iris_long_i\", N.df, overwrite=TRUE)\n  },\n  seconds.limit=seconds.limit,\n  \"duckdb\\nPIVOT\"=DBI::dbGetQuery(con, 'PIVOT iris_long_i USING mean(value) AS \".\" GROUP BY Species,part,dim'),\n  \"polars\\npivot\"=N_polars$pivot(on=\"dummy\", index=c(\"Species\",\"part\",\"dim\"), values=\"value\", aggregate_function=\"mean\"),\n  \"data.table\\ndcast\"=dcast(N.dt, Species + part + dim ~ ., mean))\npivot.refs &lt;- atime::references_best(pivot.res)\npivot.pred &lt;- predict(pivot.refs)\nplot(pivot.pred)+coord_cartesian(xlim=c(1e1,1e7))\n\nWarning in ggplot2::scale_x_log10(\"N\", breaks = meas[, 10^seq(ceiling(min(log10(N))), : log-10\ntransformation introduced infinite values.\n\n\n\n\n\n\n\n\n\nThe result above shows that data.table::dcast is about as fast as the others (bottom facet), although duckdb is slightly faster, and polars is slightly slower (less than 2x). Below we run a more complex benchmark which also measures computation time for the copy operation (in addition to the reshape).\n\nseconds.limit &lt;- 0.1\npivot.copy.res &lt;- atime::atime(\n  N=2^seq(1,50),\n  setup={\n    (row.id.vec &lt;- 1+(seq(0,N-1) %% nrow(iris.long.i)))\n    N.df &lt;- data.frame(iris.long.i[row.id.vec])\n  },\n  seconds.limit=seconds.limit,\n  \"duckdb\\ncopy+PIVOT\"={\n    duckdb::dbWriteTable(con, \"iris_long_i\", N.df, overwrite=TRUE)\n    DBI::dbGetQuery(con, 'PIVOT iris_long_i USING mean(value) AS \".\" GROUP BY Species,part,dim')\n  },\n  \"polars\\ncopy+pivot\"={\n    polars_pivot &lt;- polars::as_polars_df(\n      N.df\n    )$pivot(# wide reshape 2\n      on=\"dummy\", # have to create dummy var for on.\n      index=c(\"Species\",\"part\",\"dim\"),\n      values=\"value\",\n      aggregate_function=\"mean\")\n    as.data.frame(polars_pivot)\n  },\n  \"data.table\\nset+dcast\"=setDF(dcast(setDT(N.df), Species + part + dim ~ ., mean)))\npivot.copy.refs &lt;- atime::references_best(pivot.copy.res)\npivot.copy.pred &lt;- predict(pivot.copy.refs)\nplot(pivot.copy.pred)+coord_cartesian(xlim=c(1e1,1e7))\n\nWarning in ggplot2::scale_x_log10(\"N\", breaks = meas[, 10^seq(ceiling(min(log10(N))), : log-10\ntransformation introduced infinite values.\n\n\n\n\n\n\n\n\n\nThe result above shows that data.table is quite a bit faster than the others (5x or more)."
  },
  {
    "objectID": "posts/2024-10-17-duckdb_polars_reshape-toby_hocking/index.html#summary-of-long-to-wide-reshaping",
    "href": "posts/2024-10-17-duckdb_polars_reshape-toby_hocking/index.html#summary-of-long-to-wide-reshaping",
    "title": "Comparing data.table reshape to duckdb and polars",
    "section": "Summary of long-to-wide reshaping",
    "text": "Summary of long-to-wide reshaping\nIn this section, we showed that data.table provides an efficient and feature-rich implementation of long-to-wide data reshaping. * The formula interface allows specifying a dot (.) which is a convenient way to specify output of only one row/column. In contrast, polars requires creating a dummy variable to do that. * The fun.aggregate argument may be a list of functions, each of which will be used on each of the value.var (a convenient way of specifying all combinations). In contrast, duckdb requires specifying each combination separately (more tedious/error-prone), and polars only supports one aggregation function (not a list).\n\n\n\n\n\n\n\n\n\nhow to specify\ndata.table\npolars\nduckdb\n\n\n\n\nfunction\ndcast\npivot\nPIVOT\n\n\nrows\nLHS of formula\nindex\nGROUP BY\n\n\ncolumns\nRHS of formula\non\nON\n\n\nno columns\ndot .\ndummy variable\nomit ON\n\n\nvalues\nvalue.var\nvalues\nUSING\n\n\naggregation\naggregate.fun\naggregate_function\nUSING\n\n\nmultiple agg.\nall combinations\none function\nspecified combinations"
  },
  {
    "objectID": "posts/2024-10-17-duckdb_polars_reshape-toby_hocking/index.html#attribution",
    "href": "posts/2024-10-17-duckdb_polars_reshape-toby_hocking/index.html#attribution",
    "title": "Comparing data.table reshape to duckdb and polars",
    "section": "Attribution",
    "text": "Attribution\nParts of this blog post were copied from my more extensive comparison blog."
  },
  {
    "objectID": "posts/2024-10-17-duckdb_polars_reshape-toby_hocking/index.html#session-info",
    "href": "posts/2024-10-17-duckdb_polars_reshape-toby_hocking/index.html#session-info",
    "title": "Comparing data.table reshape to duckdb and polars",
    "section": "Session info",
    "text": "Session info\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.1.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Los_Angeles\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] ggplot2_3.5.1     data.table_1.16.2\n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6           jsonlite_1.8.9         dplyr_1.1.4            compiler_4.4.2        \n [5] tidyselect_1.2.1       directlabels_2024.1.21 scales_1.3.0           yaml_2.3.10           \n [9] fastmap_1.2.0          lattice_0.22-6         R6_2.5.1               labeling_0.4.3        \n[13] generics_0.1.3         knitr_1.49             htmlwidgets_1.6.4      tibble_3.2.1          \n[17] polars_0.21.0          munsell_0.5.1          atime_2024.11.29       DBI_1.2.3             \n[21] pillar_1.9.0           rlang_1.1.4            utf8_1.2.4             xfun_0.49             \n[25] quadprog_1.5-8         cli_3.6.3              withr_3.0.2            magrittr_2.0.3        \n[29] digest_0.6.37          grid_4.4.2             rstudioapi_0.17.1      nc_2024.9.20          \n[33] lifecycle_1.0.4        vctrs_0.6.5            bench_1.1.3            evaluate_1.0.1        \n[37] glue_1.8.0             farver_2.1.2           duckdb_1.1.3           codetools_0.2-20      \n[41] profmem_0.6.0          fansi_1.0.6            colorspace_2.1-1       rmarkdown_2.29        \n[45] tools_4.4.2            pkgconfig_2.0.3        htmltools_0.5.8.1"
  },
  {
    "objectID": "posts/2024-12-07-advent_of_code-kelly_bodwin/index.html",
    "href": "posts/2024-12-07-advent_of_code-kelly_bodwin/index.html",
    "title": "Advent of Code with data.table: Week One",
    "section": "",
    "text": "Happy December, R friends!\nOne of my favorite traditions in the R community is the Advent of Code, a series of puzzles released at midnight EST from December 1st through 25th, to be solved through programming in the language of your choosing. I usually do a few of them each year, and once tried to do every single one at the moment it released!\n\nThis year, I know I won’t be able to do it daily, but I’m going to do as many as I can using just data.table solutions.\nI’ll allow myself to use other packages when there isn’t any data.table equivalent, but my solutions must be as data.table-y as possible.\nI’m going to abuse the blog post structure and update this file throughout the week.\n\nlibrary(data.table)\n\n\nDecember 1st\n\nPart One\n\nd1 &lt;- fread(\"day1_dat1.txt\")\n\n\nd1[, V1 := sort(V1)]\nd1[, V2 := sort(V2)]\nd1[, diff := abs(V1-V2)]\n\nsum(d1$diff)\n\n[1] 2815556\n\n\n\n\nPart Two\n\nd1[, similarity := sum(V1 == d1$V2)*V1, by = V1]\n\nsum(d1$similarity)\n\n[1] 23927637\n\n\n\n\n\nDecember 2nd\n\nPart One\n\nd1 &lt;- fread(\"day2_dat1.txt\", fill = TRUE)\n\n\ncheck_report &lt;- function(vec) {\n  \n  vec &lt;- na.omit(vec)\n  \n  has_neg &lt;- vec &lt; 0\n  has_pos &lt;- vec &gt; 0\n  \n  inc_dec &lt;- sum(has_neg) == length(vec) | sum(has_pos) == length(vec)\n\n  too_big &lt;- max(abs(vec)) &gt; 3\n  \n  return(inc_dec & !too_big)\n}\n\n\nd1t &lt;- transpose(d1)\ndeltas &lt;- d1t[-nrow(d1t)] - d1t[2:nrow(d1t)]\n\nres &lt;- apply(deltas, 2, \"check_report\")\n\nsum(res)\n\n[1] 479\n\n\n\n\nPart Two\n\ntest_reports &lt;- function(dat) {\n\n  deltas &lt;- dat[-nrow(dat)] - dat[2:nrow(dat)]\n\n  res &lt;- apply(deltas, 2, \"check_report\")\n\n  res\n}\n\n\nres &lt;- test_reports(d1t)\n\nfor (i in 1:nrow(d1t)) {\n  \n  res &lt;- res | test_reports(d1t[-i,])\n  \n  \n}\n\nsum(res)\n\n[1] 531\n\n\n\n\nJust for fun\nI found the use of apply deeply unsatisfying, even though it was fast, so just for fun:\n\nd1t &lt;- transpose(d1)\ndeltas &lt;- d1t[-nrow(d1t)] - d1t[2:nrow(d1t)]\n\nis_not_pos &lt;- deltas &lt;= 0\nis_not_neg &lt;- deltas &gt;= 0\nis_big &lt;- abs(deltas) &gt; 3\n\nres_inc &lt;- colSums(is_not_neg | is_big, na.rm = TRUE)\n\nres_dec &lt;- colSums(is_not_pos | is_big, na.rm = TRUE)\n\nsum(res_inc == 0) + sum(res_dec == 0)\n\n[1] 479\n\n\nYay. :)\n\n\n\nDecember 3rd-7th\nNothing data.table-y in this one; you can see my solution here if you want.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparing data.table reshape to duckdb and polars\n\n\n\n\n\n\ntips\n\n\ntutorials\n\n\ndeveloper\n\n\nbenchmarks\n\n\n\n\n\n\n\n\n\nOct 17, 2024\n\n\nToby Dylan Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing performance regression of data.table with atime\n\n\n\n\n\n\nperformance\n\n\ntesting\n\n\ndeveloper\n\n\n\n\n\n\n\n\n\nOct 10, 2024\n\n\nDoris Afriyie Amoakohene\n\n\n\n\n\n\n\n\n\n\n\n\nSeal of Approval: mlr3\n\n\n\n\n\n\nseal of approval\n\n\napplication package\n\n\n\n\n\n\n\n\n\nOct 1, 2024\n\n\nMaximilian Mücke\n\n\n\n\n\n\n\n\n\n\n\n\nSeal of Approval: collapse\n\n\n\n\n\n\nseal of approval\n\n\npartner package\n\n\n\n\n\n\n\n\n\nSep 21, 2024\n\n\nSebastian Krantz\n\n\n\n\n\n\n\n\n\n\n\n\nNewly awarded translation projects\n\n\n\n\n\n\nannouncements\n\n\ngrant\n\n\ntranslation\n\n\n\n\n\n\n\n\n\nAug 20, 2024\n\n\nToby Dylan Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nSeal of Approval: dtplyr\n\n\n\n\n\n\nseal of approval\n\n\nbridge package\n\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nSeal of Approval: nc\n\n\n\n\n\n\nseal of approval\n\n\nextension package\n\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\nToby Dylan Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nSeal of Approval: tidyfast\n\n\n\n\n\n\nseal of approval\n\n\nbridge package\n\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\nTyson S. Barrett\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncement: The ‘Seal of Approval’\n\n\n\n\n\n\nannouncements\n\n\ngrant\n\n\ncommunity\n\n\nseal of approval\n\n\n\n\n\n\n\n\n\nJul 31, 2024\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncement: Paola Corrales, data.table Ambassador\n\n\n\n\n\n\nannouncements\n\n\ngrant\n\n\nambassadors\n\n\ntravel\n\n\n\n\n\n\n\n\n\nJun 12, 2024\n\n\nCommunity Team\n\n\n\n\n\n\n\n\n\n\n\n\nTwo Roads Diverged\n\n\n\n\n\n\nopinion\n\n\n\n\n\n\n\n\n\nJun 4, 2024\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nTesting infrastructure for data.table\n\n\n\n\n\n\ngrant\n\n\ntesting\n\n\ndeveloper\n\n\n\n\n\n\n\n\n\nMar 10, 2024\n\n\nToby Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nCommunity interviews about data.table\n\n\n\n\n\n\ncommunity\n\n\ngrant\n\n\n\n\n\n\n\n\n\nMar 6, 2024\n\n\nAnirban Chetia\n\n\n\n\n\n\n\n\n\n\n\n\nResults of the 2023 survey\n\n\n\n\n\n\ncommunity\n\n\nguest post\n\n\ngovernance\n\n\n\n\n\n\n\n\n\nFeb 25, 2024\n\n\nAljaž Sluga\n\n\n\n\n\n\n\n\n\n\n\n\nColumn assignment and reference semantics in data.table\n\n\n\n\n\n\ntips\n\n\ntutorials\n\n\ndeveloper\n\n\n\n\n\n\n\n\n\nFeb 18, 2024\n\n\nToby Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nThe Benefits of data.table Syntax\n\n\n\n\n\n\ntips\n\n\ntutorials\n\n\ndocumentation\n\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\nTyson Barrett\n\n\n\n\n\n\n\n\n\n\n\n\nNew governance, release with new features\n\n\n\n\n\n\ngovernance\n\n\nreleases\n\n\n\n\n\n\n\n\n\nJan 30, 2024\n\n\nToby Dylan Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nPiping data.tables\n\n\n\n\n\n\ntips\n\n\ntutorials\n\n\ndocumentation\n\n\nguest post\n\n\n\n\n\n\n\n\n\nJan 28, 2024\n\n\nElio Campitelli\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncement: Jan Gorecki, data.table Ambassador\n\n\n\n\n\n\nannouncements\n\n\ngrant\n\n\nambassadors\n\n\n\n\n\n\n\n\n\nJan 14, 2024\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nSummary of LatinR conference\n\n\n\n\n\n\nconferences\n\n\n\n\n\n\n\n\n\nNov 19, 2023\n\n\nToby Dylan Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncement: The data.table Ambassadors Travel Grant\n\n\n\n\n\n\nannouncements\n\n\ngrant\n\n\nfunding opportunity\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncement: data.table translation projects\n\n\n\n\n\n\nannouncements\n\n\ngrant\n\n\nfunding opportunity\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to the data.table ecosystem project!\n\n\nAn NSF-POSE funded venture.\n\n\n\nannouncements\n\n\ngrant\n\n\n\n\n\n\n\n\n\nOct 15, 2023\n\n\nToby Hocking\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-10-15-intro_to_grant-toby_hocking/index.html",
    "href": "posts/2023-10-15-intro_to_grant-toby_hocking/index.html",
    "title": "Welcome to the data.table ecosystem project!",
    "section": "",
    "text": "Hi! My name is Toby Dylan Hocking, and I have been using R since 2003, which means 20 years, can you believe it?\nI work as an Assistant Professor of Computer Science, and my research expertise is machine learning, the modern branch of artificial intelligence which uses big data. R is an important tool in my machine learning work, and in the work of many people in academia/industry/government, because it provides so many useful functions for handling big data."
  },
  {
    "objectID": "posts/2023-10-15-intro_to_grant-toby_hocking/index.html#the-data.table-package",
    "href": "posts/2023-10-15-intro_to_grant-toby_hocking/index.html#the-data.table-package",
    "title": "Welcome to the data.table ecosystem project!",
    "section": "The {data.table} package",
    "text": "The {data.table} package\nSince 2015, I have been using the R package data.table to do large parts of data processing before and after running machine learning algorithms - to get the raw data into the right format for the algorithm, and also to get the results in the right format for visualization/interpretation.\n\n\n\nThe {data.table} hex sticker\n\n\ndata.table is highly valued for its long-term stability and its lightning-fast speed in large data calculations."
  },
  {
    "objectID": "posts/2023-10-15-intro_to_grant-toby_hocking/index.html#ecosystem-expansion",
    "href": "posts/2023-10-15-intro_to_grant-toby_hocking/index.html#ecosystem-expansion",
    "title": "Welcome to the data.table ecosystem project!",
    "section": "Ecosystem expansion",
    "text": "Ecosystem expansion\nWith these use cases in mind, I proposed a project “Expanding the data.table ecosystem for efficient big data manipulation in R,” and I am excited to announce that it has been funded by the National Science Foundation’s “Pathways to Enable Open Source Ecosystems” (POSE) grant, for work between September 2023 and August 2025.\n\n\n\n\n\nOur project attempts to address three issues with the current state of the data.table project:\n\nInformal governance\n\nThe data.table package was originally created by Matt Dowle in 2008. His brilliant use of efficient algorithms and C implementations brought the world a package that has stood the test of time, and is now one of the most-used R packages available.\nHowever, the growth of this incredible package will require more leaders than Dowle himself to help build, review, test, and organize new contributions.\nThus, one goal of this grant is to bring together data.table developers and contributors to propose a new governance structure for the package’s source code.\n\nLimited centralized testing infrastructure\n\nSince the primary draw of data.table is the speed of its algorithmic implementations, adding new functionality to the package is not simple. In particular, new elements must be heavily tested to ensure that they do not interfere with the core computations.\nIn this grant, we plan to develop software to automate the testing of new package contributions, to smooth the growth process. This part of the project includes a centralized reverse dependency checking system, new benchmarks comparing data.table with other systems such as polars and arrow, and new performance testing software.\n\nLimited documentation and outreach\n\nTo encourage more people to learn and adopt data.table, we will be massively expanding the number of tutorials, documentations, and guides for how to use the package effectively. Part of this will be translation projects so that data.table will be more accessible in foreign languages. This grant will also include travel awards, to support selected speakers to travel to conferences and share data.table updates and usage.\nInterested in contributing a tutorial/vignette or blog post? Email r.data.table@gmail.com!"
  },
  {
    "objectID": "posts/2023-10-15-intro_to_grant-toby_hocking/index.html#how-you-can-get-involved",
    "href": "posts/2023-10-15-intro_to_grant-toby_hocking/index.html#how-you-can-get-involved",
    "title": "Welcome to the data.table ecosystem project!",
    "section": "How you can get involved",
    "text": "How you can get involved\nInterested in helping grow the data.table ecosystem? There are so many ways to get involved!\n\n\n\nSea lions say “R!” “R!” “R!”\n\n\n\nFollow us for updates on social media:\n\n\nMastodon: r_data_table@fosstodon.org\nBlueSky: rdatatable.bsky.social\nTwitter/X: r_data_table\n\n\nSubscribe to this blog, The Raft\nTake the Community Survey to weigh in on next steps.\nParticipate in the deep discussions on GitHub:\n\n\nWhat should be the structure of the formal governance document for the package?\nWhat are the core principles of the data.table package?\n\n\nEmail r.data.table@gmail.com to:\n\n\nBe added to the community Slack.\nPropose a guest blog for The Raft.\nAsk questions, make suggestions, or volunteer your expertise.\n\n\nApply for the upcoming Travel Grants and Translation Projects - watch this blog for more information!"
  },
  {
    "objectID": "posts/2024-01-30-new_governance_new_release-toby_hocking/index.html",
    "href": "posts/2024-01-30-new_governance_new_release-toby_hocking/index.html",
    "title": "New governance, release with new features",
    "section": "",
    "text": "I am proud to report that today, the first major new data.table features in several years have been released to CRAN!\nThis new release, version 1.15.0, is remarkable because it is the first new feature release using the new community governance, which was adopted last month.\nHere is a brief timeline of the recent activities."
  },
  {
    "objectID": "posts/2024-01-30-new_governance_new_release-toby_hocking/index.html#governance-discussion",
    "href": "posts/2024-01-30-new_governance_new_release-toby_hocking/index.html#governance-discussion",
    "title": "New governance, release with new features",
    "section": "Governance discussion",
    "text": "Governance discussion\n\n\n\nAn ongoing discussion\n\n\nIn Aug 2023, I started a discussion in issue#5676 about the process and goal of creating a formal governance document. 17 people commented on that issue, and the consensus was to publish a first draft community governance document in Nov 2023. Discussion in related issues included:\n\nWhat are the guiding principles of the project? issue#5693\nWhat is within scope for features? issue#5722\nWhat roles/permissions should we define, and how can people obtain them? My proposal.\nWhat conventions should we use for version numbers? issue#5715\nWhat code of conduct should we adopt? issue#5708\nWhat communication should we expect between the CRAN maintainer and the rest of the dev team? issue#5714\n\nAt first, it was not clear that commenters on that issue would agree to adopt a community governance structure, out of respect for the original creator, Matt Dowle, who had not yet expressed his approval of the process. However, that changed on 11 Sep 2023, when I posted Matt’s letters of collaboration that he signed in support of my NSF POSE project (after I asked Matt over email, and he agreed that I post them publicly). After that, there was a much stronger support of the proposed process."
  },
  {
    "objectID": "posts/2024-01-30-new_governance_new_release-toby_hocking/index.html#governance-draft-and-adoption",
    "href": "posts/2024-01-30-new_governance_new_release-toby_hocking/index.html#governance-draft-and-adoption",
    "title": "New governance, release with new features",
    "section": "Governance draft and adoption",
    "text": "Governance draft and adoption\n\n\n\nGovernor Sea Lion\n\n\nAfter much discussion in the above linked issues, I published an initial draft of the governance document on 27 Nov 2023 in PR#5772. That PR was extensively reviewed by four of the current/active contributors: Michael Chirico, Jan Gorecki, Tyson Barrett and Ben Schwendinger After several rounds of comments and revisions, Jan merged the PR on 14 Dec 2023, which signaled the official adoption of the new governance of the project. It can be viewed in the GOVERNANCE.md file in the git repo.\nImportantly, the new governance defines five roles for people involved in the project:\n Contributor: Any member of the public at large who participates in issue discussions, code reviews, or pull requests for data.table.\n Project member: Anyone who has contributed a substantial accepted update - whether technical or documentation based - to data.table.\n Reviewer: A project member who volunteers to help review other contributions.\n Committer: Given merge permissions on main GitHub branch; responsible for reviewing and incorporating updates. Currently: myself, Jan, and Michael.\nCRAN maintainer: Responsible for organizing new releases on GitHub and CRAN. Currently Tyson Barrett.\nInterestingly, CRAN maintainer and Committer permissions are largely orthogonal, and in fact Tyson does not currently have the Committer role permissions.\nAny of these roles is possible to obtain, using the process described in the governance document. If you are at all interested, we could definitely use your help! Please get in contact by commenting on issues/PRs on GitHub."
  },
  {
    "objectID": "posts/2024-01-30-new_governance_new_release-toby_hocking/index.html#release-1.15.0-with-new-features",
    "href": "posts/2024-01-30-new_governance_new_release-toby_hocking/index.html#release-1.15.0-with-new-features",
    "title": "New governance, release with new features",
    "section": "Release 1.15.0 with new features",
    "text": "Release 1.15.0 with new features\n\n\n\nThe new measure function\n\n\nAs outlined in the governance document, section CRAN updates, each release should be discussed and approved by consensus in an issue. The issue that we used for this 1.15.0 release is issue#5823. As can be seen in the NEWS.md file, this new release includes 20 NOTES, 55 BUG FIXES, and 41 NEW FEATURES, and 1 BREAKING CHANGE. Among the new features, I am most excited about one that I implemented: the new measure() function, which makes it easier to do complex wide-to-long reshape operations, as below:\n\nlibrary(data.table)\n(iris.dt &lt;- data.table(iris)[1])\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;  &lt;fctr&gt;\n1:          5.1         3.5          1.4         0.2  setosa\n\nmelt(iris.dt, measure.vars=measure(part, dim, sep=\".\"))\n\n   Species   part    dim value\n    &lt;fctr&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt;\n1:  setosa  Sepal Length   5.1\n2:  setosa  Sepal  Width   3.5\n3:  setosa  Petal Length   1.4\n4:  setosa  Petal  Width   0.2\n\nmelt(iris.dt, measure.vars=measure(value.name, dim, sep=\".\"))\n\n   Species    dim Sepal Petal\n    &lt;fctr&gt; &lt;char&gt; &lt;num&gt; &lt;num&gt;\n1:  setosa Length   5.1   1.4\n2:  setosa  Width   3.5   0.2"
  },
  {
    "objectID": "posts/2024-01-30-new_governance_new_release-toby_hocking/index.html#outlook-for-the-future",
    "href": "posts/2024-01-30-new_governance_new_release-toby_hocking/index.html#outlook-for-the-future",
    "title": "New governance, release with new features",
    "section": "Outlook for the future",
    "text": "Outlook for the future\nSince adopting the community governance document, there has been a lot of new activity on GitHub, and I am looking forward to seeing even more in the months to come. For example, now that we have adopted a code of conduct, we are eligible to apply for NumFOCUS funding, see discussion in issue#5676. Finally, if you use data.table, and are interested to contribute toward our next release, we could use your help, so please contact us in an issue/PR."
  },
  {
    "objectID": "posts/2024-01-28-piping_data_tables-elio_campitelli/index.html",
    "href": "posts/2024-01-28-piping_data_tables-elio_campitelli/index.html",
    "title": "Piping data.tables",
    "section": "",
    "text": "Like a devoted plumber, modern R loves pipes. The magrittr pipe has a long history and it’s fair share of detractors, but with the implementation of the native pipe operator released in May 2021 it’s clear that chaining operations is now part of R vernacular.\nSo it’s no surprise that people often wonder how can you use pipes with data.table, as one participant of the recent data.table tutorial during LatinR 2023. The surprising answer is that data.table has supported pipelines since its inception in 2006. Furthermore, you can easily use either the magrittr or native pipes."
  },
  {
    "objectID": "posts/2024-01-28-piping_data_tables-elio_campitelli/index.html#the-data.table-pipe",
    "href": "posts/2024-01-28-piping_data_tables-elio_campitelli/index.html#the-data.table-pipe",
    "title": "Piping data.tables",
    "section": "The data.table “pipe”",
    "text": "The data.table “pipe”\nInstead of passing data to functions, data.table syntax is all about operating inside the [ operator1 .\n \n\nDT[rows, columns, by]\n\n\nWhere DT is a data.table object, the rows argument is used for filtering and joining operations, the columns argument can summarise and mutate, and the by argument defines the groups to which to apply these operations.\nSo, to get only Chinstrap penguins from the penguins dataset, instead of using base::subset() or dplyr::filter() you would do\n\npenguins_chinstrap &lt;- penguins[species == \"Chinstrap\"]\n\nOr, to get the mean mean flipper length of these penguins for each island and sex, you could summarise the data like this:\n\npenguins_chinstrap[, .(mean_flipper_length = mean(flipper_length_mm)), by = .(sex, island)]\n\nBut because the output of the first operation is a data.table, you can add another [ operator after the first to chain both operations:\n\npenguins[species == \"Chinstrap\"][, .(mean_flipper_length = mean(flipper_length_mm)), by = .(sex, island)]\n\nI usually call this the ][ pipe.\nYou might have noticed that for just two operations, this line of code is already too long, so for even moderately long chains it’s usually advisable to put each operation in its own line. There’s some controversy on how to break the ][ pipe into lines and indent it. One option is to add a new line after the second [, which has the advantage of actually writing the ][ pipe explicitly.\n\npenguins[species == \"Chinstrap\" ][\n  , .(mean_flipper_length = mean(flipper_length_mm)), by = .(sex, island)]\n\nA second options is to add the new line before the end of the first operation like so:\n\npenguins[species == \"Chinstrap\" \n       ][ , .(mean_flipper_length = mean(flipper_length_mm)), by = .(sex, island)]\n\nPersonally, I don’t like this syntax very much. No matter how you slice it, you always get what feels to me as incomplete lines. Also, RStudio doesn’t correctly indent the second syntax automatically.\nAlternatively, the ][ pipe can go in its own line like so:\n\npenguins[species == \"Chinstrap\" \n][ \n  , .(mean_flipper_length = mean(flipper_length_mm)), by = .(sex, island)\n]\n\nThis is indented correctly by RStudio and has the advantage of making easy to comment out each individual step:\n\npenguins[species == \"Chinstrap\" \n][ \n  # , .(mean_flipper_length = mean(flipper_length_mm)), by = .(sex, island)\n]"
  },
  {
    "objectID": "posts/2024-01-28-piping_data_tables-elio_campitelli/index.html#data.table-and-magrittr",
    "href": "posts/2024-01-28-piping_data_tables-elio_campitelli/index.html#data.table-and-magrittr",
    "title": "Piping data.tables",
    "section": "data.table and magrittr",
    "text": "data.table and magrittr\nUntil the introduction of the native pipe, I used to write long data.table pipelines using magrittr. To do this, I took advantage of the . placeholder which, within a magrittr pipe, refers to the result of the previous step.\n\nlibrary(magrittr)\n\npenguins[species == \"Chinstrap\"] %&gt;%\n  .[ , .(mean_flipper_length = mean(flipper_length_mm)), by = .(sex, island)]\n\nI really like this syntax as it’s very clean. Each line of code is a complete operation without dangling parts and it’s easy to comment out single steps.\nThe only downside is that the dot here has two meanings: as the placeholder for the previous result in .[, and as an alias for list in .(mean_flipper_length = mean(flipper_length_mm)). It’s not a huge issue, though, since I tend to read .[ as a single entity, but it can trip up some people."
  },
  {
    "objectID": "posts/2024-01-28-piping_data_tables-elio_campitelli/index.html#using-the-native-pipe",
    "href": "posts/2024-01-28-piping_data_tables-elio_campitelli/index.html#using-the-native-pipe",
    "title": "Piping data.tables",
    "section": "Using the native pipe",
    "text": "Using the native pipe\nThe native pipe at first didn’t have a placeholder and it didn’t chaining to [, so this so the above syntax wasn’t directly applicable. But you could cheat by creating an alias for [ and use that alias as a regular function. So this works:\n\nDT &lt;- `[`\n\npenguins[species == \"Chinstrap\"] |&gt; \n  DT( , .(mean_flipper_length = mean(flipper_length_mm)), by = .(sex, island))\n\nThis worked so well that data.table officially added the DT() function (currently only in the development version), so if you’re using the latest development version you don’t even need the first line2.\nThis syntax is fine but I don’t like that I need ro write one more character and the closing character being a ) can get confusing because it adds to the closing ) that you usually have in the by argument.\nFrom R 4.3.0 onwards, the native pipe supports a _ placeholder to the right-hand side fo the pipe. So now the magrittr syntax can be directly translated to\n\npenguins[species == \"Chinstrap\"] |&gt; \n  _[ , .(mean_flipper_length = mean(flipper_length_mm)), by = .(sex, island)]\n\nI like this syntax even more than the original magrittr one because it solves the double meaning problem and operations get hugged by a pair of brackets."
  },
  {
    "objectID": "posts/2024-01-28-piping_data_tables-elio_campitelli/index.html#the-four-pipes-of-data.table",
    "href": "posts/2024-01-28-piping_data_tables-elio_campitelli/index.html#the-four-pipes-of-data.table",
    "title": "Piping data.tables",
    "section": "The four pipes of data.table",
    "text": "The four pipes of data.table\nSo, there you are, 4 different ways you can pipe your data.tables.\nUse the ][ pipe if you want your code to have minimal dependencies and work in older versions of R. Use the %&gt;% pipe if you want your code to work in older versions of R and don’t mind the extra dependency. Use any version of the |&gt; pipe if you want minimal dependencies and don’t mind depending on R &gt;= 4.3.0.\n\npenguins[species == \"Chinstrap\" ][\n  , .(mean_flipper_length = mean(flipper_length_mm)), by = .(sex, island)]\n\npenguins[species == \"Chinstrap\"] %&gt;%\n  .[ , .(mean_flipper_length = mean(flipper_length_mm)), by = .(sex, island)]\n\npenguins[species == \"Chinstrap\"] |&gt; \n  DT( , .(mean_flipper_length = mean(flipper_length_mm)), by = .(sex, island))\n\npenguins[species == \"Chinstrap\"] |&gt; \n  _[ , .(mean_flipper_length = mean(flipper_length_mm)), by = .(sex, island)]\n\nImage by storyset on Freepik"
  },
  {
    "objectID": "posts/2024-01-28-piping_data_tables-elio_campitelli/index.html#footnotes",
    "href": "posts/2024-01-28-piping_data_tables-elio_campitelli/index.html#footnotes",
    "title": "Piping data.tables",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTechnically, the [ operator is itself a function, but the syntax is not function-like.↩︎\nThis function also allows the user to use data.table syntax to data.frames and tibbles retaining the class of the output.↩︎"
  },
  {
    "objectID": "posts/2023-11-19-LatinR_summary-toby_hocking/index.html",
    "href": "posts/2023-11-19-LatinR_summary-toby_hocking/index.html",
    "title": "Summary of LatinR conference",
    "section": "",
    "text": "Last month, I (Toby) went to the LatinR conference in Montevideo, Uruguay. I had two goals: to teach about data.table in a tutorial, and to find people to work on translations."
  },
  {
    "objectID": "posts/2023-11-19-LatinR_summary-toby_hocking/index.html#tutorial-about-data.table",
    "href": "posts/2023-11-19-LatinR_summary-toby_hocking/index.html#tutorial-about-data.table",
    "title": "Summary of LatinR conference",
    "section": "Tutorial about data.table",
    "text": "Tutorial about data.table\nI presented a tutorial on the first day of the LatinR meeting, to an audience of about 50 students.\n\n\n\nPhoto from the LatinR data.table tutorial\n\n\nThe google slides that I used are online, and I also created a GitHub repo with the source files that I used for creating the figures in the slides. During the talk, Elio Campitelli and Paola Corrales were there to help people in the audience with individual/technical questions (for example, installation of data.table from github master was difficult for some people using windows). I did not have enough time to do all of the exercises, but I did spend about 10 minutes at the end of my talk, to invite people to participate in the translation projects and travel awards.\nOne of the students in the audience was Mara Destefanis, who said she was very interested to participate in the translation project. In fact, since the conference, she has sent me several emails, updating me about a Spanish translation of my tutorial slides."
  },
  {
    "objectID": "posts/2023-11-19-LatinR_summary-toby_hocking/index.html#translation-workshop",
    "href": "posts/2023-11-19-LatinR_summary-toby_hocking/index.html#translation-workshop",
    "title": "Summary of LatinR conference",
    "section": "Translation workshop",
    "text": "Translation workshop\nDuring the conference, there was a translation workshop.\n\n\n\nPhoto from the LatinR translation workshop\n\n\nI learned that there has been a lot of progress recently, about translations in R.\n\nDuring the R project sprint in summer 2023, there was an effort to crowdsource translations of messages, using a new weblate server for R.\nElio Campitelli proposed a R Consortium project about translating Rd pages. Currently, there is only one Rd page for help on any given topic, and it is possible in theory to write an Rd page in multiple languages, but in practice most are in English only.\nNestor Montano told me that he recorded an online workshop about data.table, in Spanish! Slides, Youtube.\nRiva Quiroga told me about the R Para Ciencia de Datos project, which is a Spanish translation of the popular R for Data Science online textbook (an important reference about tidyverse). Part of that project involved creating new data sets in the datos package, with column names and man pages translated to Spanish. For example vuelos.rd is the man page for the translated version of flights data. Riva told me about her experience leading this translation project. For our NSF data.table project, she suggested several important criteria to consider, that I had not written in my original call for translation projects. After discussing with her, I wrote a PR to revise and improve the call for translation projects.\n\nOverall I felt that going to LatinR was a very worthwhile experience, because of the successful outreach in my tutorial, and the great networking at the translation workshop."
  },
  {
    "objectID": "posts/2023-11-19-LatinR_summary-toby_hocking/index.html#communcations-since-then",
    "href": "posts/2023-11-19-LatinR_summary-toby_hocking/index.html#communcations-since-then",
    "title": "Summary of LatinR conference",
    "section": "Communcations since then",
    "text": "Communcations since then\nSince then, I have been in email contact with several people who have expressed interest in the data.table translation project.\n\nFor Spanish, Riva Quiroga (Chile) told me that she would like to lead, with several other people who expressed interest to participate: Nati Labadie (Argentina), Andrea Gomez Vargas (Colombia), Emanuel Ciardullo (Argentina), Nestor Montano (Ecuador), Mara Destefanis (Uruguay), …\nFor French, Philippe Grojean told me that he would be interested to lead, and submit an application in the next few months.\nFor Portuguese, Leonardo Ferreira Fontenelle told me that he would be interested to lead, and submit an application soon.\n\nI look forward to seeing these projects develop in the coming months!"
  },
  {
    "objectID": "posts/2024-03-10-testing_plan-toby_hocking/index.html",
    "href": "posts/2024-03-10-testing_plan-toby_hocking/index.html",
    "title": "Testing infrastructure for data.table",
    "section": "",
    "text": "One major element of the NSF POSE grant for data.table is to create more documentation and testing infrastructure, in order to help expand the data.table ecosystem. This blog post explains what we proposed to do to improve the testing infrastructure."
  },
  {
    "objectID": "posts/2024-03-10-testing_plan-toby_hocking/index.html#current-testing-infrastructure",
    "href": "posts/2024-03-10-testing_plan-toby_hocking/index.html#current-testing-infrastructure",
    "title": "Testing infrastructure for data.table",
    "section": "Current testing infrastructure",
    "text": "Current testing infrastructure\nCurrent testing is limited to package checks that run on CI:\n\nGithub actions runs R CMD check on Ubuntu for each PR.\nAppVeyor runs R CMD check on Windows for each PR.\nCodeCov is used to track code coverage for each PR.\nGitLab runs R CMD check on ten different platforms, for each push to master."
  },
  {
    "objectID": "posts/2024-03-10-testing_plan-toby_hocking/index.html#new-asymptotic-performance-testing-framework",
    "href": "posts/2024-03-10-testing_plan-toby_hocking/index.html#new-asymptotic-performance-testing-framework",
    "title": "Testing infrastructure for data.table",
    "section": "New asymptotic performance testing framework",
    "text": "New asymptotic performance testing framework\nCurrent performance testing is informal. For example before release devs run test.data.table(memtest=TRUE) to examine how much memory is used during tests. If too much memory is used, then that could result in a check failure on CRAN. Sometimes git bisect is used to find the commit which caused a performance regression. But there are no systematic performance tests that are regularly done, even though performance is a major feature of data.table.\nThe lack of a performance testing framework to run on CI often results in performance regressions. Because an emphasis in data.table is on handling large data sets, fast and memory-efficient code is essential, and that is a primary reason why people use data.table for their data analyses. Consequently, when new changes introduce performance regressions (for example, increased computation time), users regularly file issues related to performance. As of October 2022, a search for the keyword performance yields 161 closed and 97 open issues and pull requests. Currently, there is no systematic performance testing framework in place for data.table developers, which unfortunately makes it easy to inadvertently introduce changes that adversely affect performance, and this is a barrier to accepting code contributions.\nWe propose a GitHub Action for comparing the asymptotic performance of a pull request with its parent branch. We, therefore, propose to develop a new infrastructure for systematic empirical asymptotic performance testing, so that data.table developers will be able to easily detect and prevent performance regressions. This will facilitate expanding the data.table ecosystem by making both existing developers and new contributors more confident that their code does not result in performance regressions. We plan to build a solution that uses R package atime, which we created in order to facilitate comparing the empirical asymptotic performance of different R package versions. The main idea is that the user defines some R code that depends on an input data size N, and then atime keeps increasing the data size N until it reaches some time limit, for example, 1 second. Time and memory usage is measured for each data size and R package version, so it is easy to see if there are any significant performance differences. For example, we used atime with git bisect to find the commit which was responsible for the slowdown in a recent issue.\nWe have previous experience building GitHub actions for continuous integration testing of R packages, via Rperform and RcppDeepState, so we plan to adapt these existing GitHub actions for empirical asymptotic performance testing of data.table. The two people responsible for implementing this part of the project are Doris Amoakohene and Anirban Chetia, who will build (1) an asymptotic performance test suite that formalizes a set of computations for which efficiency is important, and (2) a GitHub action that runs each test in the suite systematically for each development branch, and creates/updates a comment in the corresponding pull request. If there are any significant differences in empirical asymptotic performance measurements, then the comment will contain a figure showing three empirical asymptotic performance curves:\n\nmost recent commit on the development branch,\nthe most recent commit on the main branch, and\nthe best common ancestor commit (also known as merge base).\n\nThe overall result will make it easy for data.table developers to do systematic asymptotic performance testing of each pull request, thereby reducing the chance of performance regressions, and increasing the security/confidence of accepting new code contributions, which will encourage the data.table contributor ecosystem to expand."
  },
  {
    "objectID": "posts/2024-03-10-testing_plan-toby_hocking/index.html#new-db-benchmark",
    "href": "posts/2024-03-10-testing_plan-toby_hocking/index.html#new-db-benchmark",
    "title": "Testing infrastructure for data.table",
    "section": "New db-benchmark",
    "text": "New db-benchmark\nBecause a major feature of data.table is its efficiency (small time and memory requirements), users and developers are interested to know how its performance compares with similar software tools (in R and in other languages). Until 2021, data.table contributor Jan Gorecki maintained the db-benchmark, that compared computation times of various data manipulation libraries on various different tasks. The most recently computed benchmark result from 2021 shows that data.table is among the fastest software. Other similar benchmarks have been created by developers of other libraries such as polars and duckdb.\nOne of the goals of this project is to get these benchmarks running on a regular basis (every week) on a variety of computing platforms. On one hand, we would like to run benchmarks on Amazon EC2, because that is a public computing resource that anyone can use to verify/reproduce the results. On the other hand, doing all those benchmarks on Amazon EC2 would be prohibitively expensive for most people (the last run was 163 hours, which would cost over $500 per run on a c6i.16xlarge virtual machine with 64 CPUs and 128GB of memory. We, therefore, propose a compromise, where we run the complete set of benchmarks on the NAU Monsoon cluster (which may take over 100 hours, and is free to use for this project, but not reproducible for other groups), and publish them on a web page every week. We will also develop a new db-benchmark-small that can run on Amazon EC2 at a much lower cost (for easy/cheap reproducibility). Our goal will be to have a single Amazon EC2 benchmark run cost $10–$100, which means 3–30 hours of computation time on c6i.16xlarge virtual machines. The result of this project activity will be a new infrastructure for regularly comparing the performance of data.table with similar software libraries, which will be useful for identifying areas where data.table has advantages or could be improved."
  },
  {
    "objectID": "posts/2024-03-10-testing_plan-toby_hocking/index.html#continuous-reverse-dependency-checking-on-nau-monsoon-cluster",
    "href": "posts/2024-03-10-testing_plan-toby_hocking/index.html#continuous-reverse-dependency-checking-on-nau-monsoon-cluster",
    "title": "Testing infrastructure for data.table",
    "section": "Continuous reverse dependency checking on NAU Monsoon cluster",
    "text": "Continuous reverse dependency checking on NAU Monsoon cluster\nReverse dependencies are other R packages that require functionality from data.table (over a thousand R packages). A new version of data.table released to CRAN must be compatible with the example and test code in these reverse dependencies. Therefore, before submitting an update to CRAN, each reverse dependency must be checked to ensure that there are no new errors. This involves significant computation time, to run the example/test code in thousands of R packages, and also significant developer time, to investigate any regressions. During this project, we, therefore, propose to create a new infrastructure for continuous reverse dependency checking. In detail, we plan to run regular nightly checks of the data.table main branch on the NAU Monsoon cluster, which is freely available for use by the PI for this project. The results of the checks will be compared with the current check results from the previous release version of data.table on CRAN, and any regressions will be highlighted on a web page for easy identification by data.table developers. Overall the result will be a new shared infrastructure for continuous reverse dependency checking, which will make it much easier for the data.table project to provide more frequent releases.\nActually, this part of the project is almost complete. We have implemented a system on NAU Monsoon which begins a new check every morning just after midnight, and publishes the results to a web page, usually before noon on the same day. There are currently 1400+ revdeps, which would take about two weeks to check, if we run each revdep in sequence on a single CPU. Luckily, we get the results on Monsoon in just a few hours, which is approximately a 30x speedup. A recent result is shown below,\n\n\n\nsignificant differences table\n\n\nThe main result is the table above, which has a row for each significant difference found, when comparing a revdep check using data.table CRAN release, to current data.table master. The Rvers column indicates the version of base R which was used (devel or release), and typical revdep issues show up using both versions of base R. The table is sorted by the first column, which is the SHA1 hash of the first commit which was found to have the issue, according to git bisect. The links lead to the corresponding commit on github (first.bad.commit), the full revdep check log file (Package), and current CRAN check result (CRAN).\nThe revdep check system has been working for over a year now, and has been very helpful in preparing the upcoming release of data.table 1.15.0 (which had several dozen revdep issues that needed to be fixed). There are currently some package installation issues which may cause some false negatives (real revdep issues which are not reported), but at least these installation issues are displayed on the result web page, and we are currently working to resolve them. The source code for the revdep check system is available in the tdhock/data.table-revdeps repository on GitHub."
  },
  {
    "objectID": "posts/2024-03-10-testing_plan-toby_hocking/index.html#conclusion",
    "href": "posts/2024-03-10-testing_plan-toby_hocking/index.html#conclusion",
    "title": "Testing infrastructure for data.table",
    "section": "Conclusion",
    "text": "Conclusion\nWe have discussed the plan for augmenting the testing infrastructure available for data.table (performance testing, benchmarking, and revdep checking). Hopefully the new testing infrastructure will allow contributors to be more confident about merging PRs with bug fixes and new features."
  },
  {
    "objectID": "posts/2024-08-01-seal_of_approval-nc/index.html",
    "href": "posts/2024-08-01-seal_of_approval-nc/index.html",
    "title": "Seal of Approval: nc",
    "section": "",
    "text": "nc hex sticker\n\n\n\nMaintainer: Toby Dylan Hocking (toby.hocking@r-project.org)\nUser-friendly functions for extracting a data table (row for each match, column for each group) from non-tabular text data using regular expressions, and for melting columns that match a regular expression. Patterns are defined using a readable syntax that makes it easy to build complex patterns in terms of simpler, re-usable sub-patterns. Named R arguments are translated to column names in the output, thereby providing a standard interface to three regular expression ‘C’ libraries (‘PCRE’, ‘RE2’, ‘ICU’). Output can also include numeric columns via user-specified type conversion functions."
  },
  {
    "objectID": "posts/2024-08-01-seal_of_approval-nc/index.html#nc",
    "href": "posts/2024-08-01-seal_of_approval-nc/index.html#nc",
    "title": "Seal of Approval: nc",
    "section": "",
    "text": "nc hex sticker\n\n\n\nMaintainer: Toby Dylan Hocking (toby.hocking@r-project.org)\nUser-friendly functions for extracting a data table (row for each match, column for each group) from non-tabular text data using regular expressions, and for melting columns that match a regular expression. Patterns are defined using a readable syntax that makes it easy to build complex patterns in terms of simpler, re-usable sub-patterns. Named R arguments are translated to column names in the output, thereby providing a standard interface to three regular expression ‘C’ libraries (‘PCRE’, ‘RE2’, ‘ICU’). Output can also include numeric columns via user-specified type conversion functions."
  },
  {
    "objectID": "posts/2024-08-01-seal_of_approval-nc/index.html#relationship-with-data.table",
    "href": "posts/2024-08-01-seal_of_approval-nc/index.html#relationship-with-data.table",
    "title": "Seal of Approval: nc",
    "section": "Relationship with data.table",
    "text": "Relationship with data.table\nWhereas data.table provides several functions such as patterns() and measure() which support some regex engines (PCRE, TRE), nc interfaces with two other engines (RE2, ICU). nc imports data.table, and always returns regex match results as a data.table."
  },
  {
    "objectID": "posts/2024-08-01-seal_of_approval-nc/index.html#overview",
    "href": "posts/2024-08-01-seal_of_approval-nc/index.html#overview",
    "title": "Seal of Approval: nc",
    "section": "Overview",
    "text": "Overview\nnc is useful for extracting numeric data from text, for example consider the following strings, which indicate genomic positions, in bases on a chromosome:\n\nchr.pos.vec &lt;- c(\n  \"chr10:213,054,000-213,055,000\",\n  \"chrM:111,000\",              # no end.\n  \"chr1:110-111 chr2:220-222\") # two ranges.\n\nThe data above consist of a chromosome name (chr10), followed by a start position, and then optionally a dash and an end position. Using nc, we can extract these different pieces of information into a data table using the code below, which inputs the data to parse (first argument), along with a regular expression (subsequent arguments).\n\nnc::capture_first_vec(\n  chr.pos.vec,\n  chrom=\"chr.*?\",\n  \":\",\n  start=\"[0-9,]+\")\n\n    chrom       start\n   &lt;char&gt;      &lt;char&gt;\n1:  chr10 213,054,000\n2:   chrM     111,000\n3:   chr1         110\n\n\nThe code above uses chrom and start as argument names, which are therefore used for column names in the output data table (one row per input subject string, one column per named argument / capture group). However the code above only parses the start position (and not the optional end position). Below, we create a more complex regex to parse both the start and end, by first defining a common pattern to parse an integer,\n\nkeep.digits &lt;- function(x) as.integer(gsub(\"[^0-9]\", \"\", x))\n\nint.pattern &lt;- list(\"[0-9,]+\", keep.digits)\n\nIn the code above, we use a list to group the regex \"[0-9],]+\" with the function keep.digits which will be used for parsing the text that is extracted by that regex. We use that pattern twice in the code below,\n\nrange.pattern &lt;- list(\n  chrom=\"chr.*?\",\n  \":\",\n  start=int.pattern,\n  list( # un-named list becomes non-capturing group.\n    \"-\",\n    end=int.pattern\n  ), \"?\") # chromEnd is optional.\nnc::capture_first_vec(chr.pos.vec, range.pattern)\n\n    chrom     start       end\n   &lt;char&gt;     &lt;int&gt;     &lt;int&gt;\n1:  chr10 213054000 213055000\n2:   chrM    111000        NA\n3:   chr1       110       111\n\n\nThe result above is a data table containing the first match in each subject (three rows total). Note the second row has end=NA because that optional group did not match.\nBut the last subject has two potential matches (only the first is reported above). What if we wanted to get all matches in each subject? We can use another function, as in the code below.\n\nnc::capture_all_str(chr.pos.vec, range.pattern)\n\n    chrom     start       end\n   &lt;char&gt;     &lt;int&gt;     &lt;int&gt;\n1:  chr10 213054000 213055000\n2:   chrM    111000        NA\n3:   chr1       110       111\n4:   chr2       220       222\n\n\nThe output above includes all matches in each subject (four rows total), but does not include any information about which subject each row came from, because it treats the subject as a single string to parse. To get that info, we can use capture_all_str() for each row, using by=.I as in the code below.\n\nlibrary(data.table)\ndata.table(chr.pos.vec)[, nc::capture_all_str(\n  chr.pos.vec, range.pattern), by=.I]\n\n       I  chrom     start       end\n   &lt;int&gt; &lt;char&gt;     &lt;int&gt;     &lt;int&gt;\n1:     1  chr10 213054000 213055000\n2:     2   chrM    111000        NA\n3:     3   chr1       110       111\n4:     3   chr2       220       222\n\n\nThe output above includes the additional I column which is the index of the subject that each match came from (two rows with I=3 because there are two matches in the third subject).\nFinally, data.table::melt() is used to power the long-to-wide data reshaping functionality in nc. In data.table we could use measure() to specify a set of variables to reshape, as in the code below.\n\n(iris.wide &lt;- data.table(iris)[1])\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;  &lt;fctr&gt;\n1:          5.1         3.5          1.4         0.2  setosa\n\nmelt(iris.wide, measure.vars=measure(value.name, dim, pattern=\"(.*)[.](.*)\"))\n\n   Species    dim Sepal Petal\n    &lt;fctr&gt; &lt;char&gt; &lt;num&gt; &lt;num&gt;\n1:  setosa Length   5.1   1.4\n2:  setosa  Width   3.5   0.2\n\n\nThe result above has reshaped the four numeric input columns into two numeric output columns (value.name is the sentinel/keyword indicating that we want to make a new column for each unique value captured in that group). The equivalent nc code would be as below, with the regex defined using a named argument for each capture group (instead of one long pattern string with parentheses for each capture group).\n\nnc::capture_melt_multiple(\n  iris.wide,\n  column=\".*\",\n  \"[.]\",\n  dim=\".*\")\n\n   Species    dim Petal Sepal\n    &lt;fctr&gt; &lt;char&gt; &lt;num&gt; &lt;num&gt;\n1:  setosa Length   1.4   5.1\n2:  setosa  Width   0.2   3.5\n\n\nThe nc code above produces the same result, and in fact uses data.table::melt() internally.\nFor more info about the nc package, please read the vignettes on its CRAN page."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nAdvent of Code with data.table: Week One\n\n\n\n\n\n\ntutorials\n\n\ncommunity\n\n\n\n\n\n\n\n\n\nDec 7, 2024\n\n\n2 min\n\n\n\n\n\n\n\nComparing data.table reshape to duckdb and polars\n\n\n\n\n\n\ntips\n\n\ntutorials\n\n\ndeveloper\n\n\nbenchmarks\n\n\n\n\n\n\n\n\n\nOct 17, 2024\n\n\n21 min\n\n\n\n\n\n\n\nVisualizing performance regression of data.table with atime\n\n\n\n\n\n\nperformance\n\n\ntesting\n\n\ndeveloper\n\n\n\n\n\n\n\n\n\nOct 10, 2024\n\n\n12 min\n\n\n\n\n\n\n\nSeal of Approval: mlr3\n\n\n\n\n\n\nseal of approval\n\n\napplication package\n\n\n\n\n\n\n\n\n\nOct 1, 2024\n\n\n2 min\n\n\n\n\n\n\n\nSeal of Approval: collapse\n\n\n\n\n\n\nseal of approval\n\n\npartner package\n\n\n\n\n\n\n\n\n\nSep 21, 2024\n\n\n5 min\n\n\n\n\n\n\n\nNewly awarded translation projects\n\n\n\n\n\n\nannouncements\n\n\ngrant\n\n\ntranslation\n\n\n\n\n\n\n\n\n\nAug 20, 2024\n\n\n1 min\n\n\n\n\n\n\n\nSeal of Approval: dtplyr\n\n\n\n\n\n\nseal of approval\n\n\nbridge package\n\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\n2 min\n\n\n\n\n\n\n\nSeal of Approval: nc\n\n\n\n\n\n\nseal of approval\n\n\nextension package\n\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\n4 min\n\n\n\n\n\n\n\nSeal of Approval: tidyfast\n\n\n\n\n\n\nseal of approval\n\n\nbridge package\n\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\n4 min\n\n\n\n\n\n\n\nAnnouncement: The ‘Seal of Approval’\n\n\n\n\n\n\nannouncements\n\n\ngrant\n\n\ncommunity\n\n\nseal of approval\n\n\n\n\n\n\n\n\n\nJul 31, 2024\n\n\n2 min\n\n\n\n\n\n\n\nAnnouncement: Paola Corrales, data.table Ambassador\n\n\n\n\n\n\nannouncements\n\n\ngrant\n\n\nambassadors\n\n\ntravel\n\n\n\n\n\n\n\n\n\nJun 12, 2024\n\n\n2 min\n\n\n\n\n\n\n\nTwo Roads Diverged\n\n\n\n\n\n\nopinion\n\n\n\n\n\n\n\n\n\nJun 4, 2024\n\n\n8 min\n\n\n\n\n\n\n\nTesting infrastructure for data.table\n\n\n\n\n\n\ngrant\n\n\ntesting\n\n\ndeveloper\n\n\n\n\n\n\n\n\n\nMar 10, 2024\n\n\n8 min\n\n\n\n\n\n\n\nCommunity interviews about data.table\n\n\n\n\n\n\ncommunity\n\n\ngrant\n\n\n\n\n\n\n\n\n\nMar 6, 2024\n\n\n17 min\n\n\n\n\n\n\n\nResults of the 2023 survey\n\n\n\n\n\n\ncommunity\n\n\nguest post\n\n\ngovernance\n\n\n\n\n\n\n\n\n\nFeb 25, 2024\n\n\n6 min\n\n\n\n\n\n\n\nColumn assignment and reference semantics in data.table\n\n\n\n\n\n\ntips\n\n\ntutorials\n\n\ndeveloper\n\n\n\n\n\n\n\n\n\nFeb 18, 2024\n\n\n5 min\n\n\n\n\n\n\n\nThe Benefits of data.table Syntax\n\n\n\n\n\n\ntips\n\n\ntutorials\n\n\ndocumentation\n\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\n5 min\n\n\n\n\n\n\n\nNew governance, release with new features\n\n\n\n\n\n\ngovernance\n\n\nreleases\n\n\n\n\n\n\n\n\n\nJan 30, 2024\n\n\n4 min\n\n\n\n\n\n\n\nPiping data.tables\n\n\n\n\n\n\ntips\n\n\ntutorials\n\n\ndocumentation\n\n\nguest post\n\n\n\n\n\n\n\n\n\nJan 28, 2024\n\n\n5 min\n\n\n\n\n\n\n\nAnnouncement: Jan Gorecki, data.table Ambassador\n\n\n\n\n\n\nannouncements\n\n\ngrant\n\n\nambassadors\n\n\n\n\n\n\n\n\n\nJan 14, 2024\n\n\n2 min\n\n\n\n\n\n\n\nSummary of LatinR conference\n\n\n\n\n\n\nconferences\n\n\n\n\n\n\n\n\n\nNov 19, 2023\n\n\n3 min\n\n\n\n\n\n\n\nAnnouncement: The data.table Ambassadors Travel Grant\n\n\n\n\n\n\nannouncements\n\n\ngrant\n\n\nfunding opportunity\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\n3 min\n\n\n\n\n\n\n\nAnnouncement: data.table translation projects\n\n\n\n\n\n\nannouncements\n\n\ngrant\n\n\nfunding opportunity\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\n4 min\n\n\n\n\n\n\n\nWelcome to the data.table ecosystem project!\n\n\nAn NSF-POSE funded venture.\n\n\n\nannouncements\n\n\ngrant\n\n\n\n\n\n\n\n\n\nOct 15, 2023\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog is dedicated to building the community around the data.table R package. Its startup is funded by NSF-POSE Grant called “Expanding the data.table ecosystem for efficient big data manipulation in R”. Here you will find announcements related to the grant, as well as tips, updates, and tutorials for data.table and its sister packages.\nBlog managed by Kelly Bodwin, Tyson Barrett, and Toby Hocking. Email us at r.data.table@gmail.com.\nFind us in the R-Bloggers feed!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvent of Code with data.table: Week One\n\n\n\n\n\n\ntutorials\n\n\ncommunity\n\n\n\n\n\n\n\n\n\nDec 7, 2024\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nComparing data.table reshape to duckdb and polars\n\n\n\n\n\n\ntips\n\n\ntutorials\n\n\ndeveloper\n\n\nbenchmarks\n\n\n\n\n\n\n\n\n\nOct 17, 2024\n\n\nToby Dylan Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing performance regression of data.table with atime\n\n\n\n\n\n\nperformance\n\n\ntesting\n\n\ndeveloper\n\n\n\n\n\n\n\n\n\nOct 10, 2024\n\n\nDoris Afriyie Amoakohene\n\n\n\n\n\n\n\n\n\n\n\n\nSeal of Approval: mlr3\n\n\n\n\n\n\nseal of approval\n\n\napplication package\n\n\n\n\n\n\n\n\n\nOct 1, 2024\n\n\nMaximilian Mücke\n\n\n\n\n\n\n\n\n\n\n\n\nSeal of Approval: collapse\n\n\n\n\n\n\nseal of approval\n\n\npartner package\n\n\n\n\n\n\n\n\n\nSep 21, 2024\n\n\nSebastian Krantz\n\n\n\n\n\n\n\n\n\n\n\n\nNewly awarded translation projects\n\n\n\n\n\n\nannouncements\n\n\ngrant\n\n\ntranslation\n\n\n\n\n\n\n\n\n\nAug 20, 2024\n\n\nToby Dylan Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nSeal of Approval: dtplyr\n\n\n\n\n\n\nseal of approval\n\n\nbridge package\n\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nSeal of Approval: nc\n\n\n\n\n\n\nseal of approval\n\n\nextension package\n\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\nToby Dylan Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nSeal of Approval: tidyfast\n\n\n\n\n\n\nseal of approval\n\n\nbridge package\n\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\nTyson S. Barrett\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncement: The ‘Seal of Approval’\n\n\n\n\n\n\nannouncements\n\n\ngrant\n\n\ncommunity\n\n\nseal of approval\n\n\n\n\n\n\n\n\n\nJul 31, 2024\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncement: Paola Corrales, data.table Ambassador\n\n\n\n\n\n\nannouncements\n\n\ngrant\n\n\nambassadors\n\n\ntravel\n\n\n\n\n\n\n\n\n\nJun 12, 2024\n\n\nCommunity Team\n\n\n\n\n\n\n\n\n\n\n\n\nTwo Roads Diverged\n\n\n\n\n\n\nopinion\n\n\n\n\n\n\n\n\n\nJun 4, 2024\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nTesting infrastructure for data.table\n\n\n\n\n\n\ngrant\n\n\ntesting\n\n\ndeveloper\n\n\n\n\n\n\n\n\n\nMar 10, 2024\n\n\nToby Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nCommunity interviews about data.table\n\n\n\n\n\n\ncommunity\n\n\ngrant\n\n\n\n\n\n\n\n\n\nMar 6, 2024\n\n\nAnirban Chetia\n\n\n\n\n\n\n\n\n\n\n\n\nResults of the 2023 survey\n\n\n\n\n\n\ncommunity\n\n\nguest post\n\n\ngovernance\n\n\n\n\n\n\n\n\n\nFeb 25, 2024\n\n\nAljaž Sluga\n\n\n\n\n\n\n\n\n\n\n\n\nColumn assignment and reference semantics in data.table\n\n\n\n\n\n\ntips\n\n\ntutorials\n\n\ndeveloper\n\n\n\n\n\n\n\n\n\nFeb 18, 2024\n\n\nToby Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nThe Benefits of data.table Syntax\n\n\n\n\n\n\ntips\n\n\ntutorials\n\n\ndocumentation\n\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\nTyson Barrett\n\n\n\n\n\n\n\n\n\n\n\n\nNew governance, release with new features\n\n\n\n\n\n\ngovernance\n\n\nreleases\n\n\n\n\n\n\n\n\n\nJan 30, 2024\n\n\nToby Dylan Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nPiping data.tables\n\n\n\n\n\n\ntips\n\n\ntutorials\n\n\ndocumentation\n\n\nguest post\n\n\n\n\n\n\n\n\n\nJan 28, 2024\n\n\nElio Campitelli\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncement: Jan Gorecki, data.table Ambassador\n\n\n\n\n\n\nannouncements\n\n\ngrant\n\n\nambassadors\n\n\n\n\n\n\n\n\n\nJan 14, 2024\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nSummary of LatinR conference\n\n\n\n\n\n\nconferences\n\n\n\n\n\n\n\n\n\nNov 19, 2023\n\n\nToby Dylan Hocking\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncement: The data.table Ambassadors Travel Grant\n\n\n\n\n\n\nannouncements\n\n\ngrant\n\n\nfunding opportunity\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncement: data.table translation projects\n\n\n\n\n\n\nannouncements\n\n\ngrant\n\n\nfunding opportunity\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to the data.table ecosystem project!\n\n\nAn NSF-POSE funded venture.\n\n\n\nannouncements\n\n\ngrant\n\n\n\n\n\n\n\n\n\nOct 15, 2023\n\n\nToby Hocking\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-05-20-kelly_bodwin/index.html",
    "href": "posts/2024-05-20-kelly_bodwin/index.html",
    "title": "Two Roads Diverged",
    "section": "",
    "text": "A little-known historical tidbit is that Robert Frost’s The Road Less Traveled - so often cited as a celebration of individuality and difficult choices - was in fact meant as a joke to tease an indecisive friend. Frost’s intent was to be ironic; to make fun of someone who is overly dramatic looking back on their choices. Come on buddy, he says, just pick a path - they are both beautiful and will get you somewhere interesting.\nI bring this up because, like in the poem, I believe we in the R community often overdramatize moments of divergence between different R dialects, packages, and syntaxes.\nAnd I, like poor Robert Frost, also feel there may be some misunderstanding around my intent here.\nSo to make sure my opinions are loud and clear, we’ll get some help from this cartoon lady to shout out the things I most want y’all to hear from me:\nThis blog post is my attempt to dispel the myths surrounding the relationship between data.table and the tidyverse, and to explain why I believe deeply in both."
  },
  {
    "objectID": "posts/2024-05-20-kelly_bodwin/index.html#two-roads-diverged-in-a-yellow-wood-and-sorry-i-could-not-travel-both",
    "href": "posts/2024-05-20-kelly_bodwin/index.html#two-roads-diverged-in-a-yellow-wood-and-sorry-i-could-not-travel-both",
    "title": "Two Roads Diverged",
    "section": "“Two roads diverged in a yellow wood, and sorry I could not travel both…”",
    "text": "“Two roads diverged in a yellow wood, and sorry I could not travel both…”\nA bit of history to kick us off. (I know, I know, I’ll keep it short.)\nThe first official 1.0 version of R was released February 29, 2000, and if I had to guess, I’d bet the first add-on package was created the next day. The beating heart of R is the base language, that the R Core Team has lovingly and diligently maintained for over 4 decades - but its soul, if you will, is the incredible collection of packages that expand and adapt this core.\ndata.table was released in 2008 by Matt Dowle. (See this video for a very cool recap of the inspiration and process from Matt himself!) Since then, it has grown enormously in scope, contributors, user base, and dependencies.\nIn 2014, dplyr was released by Hadley Wickham, the birth of what we now know as the tidyverse.\nThis meant that users now had several options to pick from if they wanted to, say, calculate means by group:\n\n## Base R\naggregate(bill_length_mm ~ species, data = penguins, mean)\n\n\n## data.table\n\npenguins_dt &lt;- data.table(penguins)\npenguins_dt[, .(mean_bill=mean(bill_length_mm)), by=species]\n\n\n## dplyr\n\npenguins %&gt;%\n  group_by(species) %&gt;%\n  summarize(mean_bill = mean(bill_length_mm))\n\n\n\n\n\n\ndata.table is not technically Base R!\n\n\n\nIt is true that the data.table syntax most closely mimics that of Base R data frames, and deliberately so. However, data.table is an open-source package like any other. Nobody - and I mean nobody - uses only Base R in their work. What a silly culture that would be, if we have all the beautiful multiverse of an open-source language, and we limit ourselves only to the core functionality!\n\n\n\nI don’t even remember what this ad was for."
  },
  {
    "objectID": "posts/2024-05-20-kelly_bodwin/index.html#looked-down-one-as-far-as-i-could-then-took-the-other-just-as-fair",
    "href": "posts/2024-05-20-kelly_bodwin/index.html#looked-down-one-as-far-as-i-could-then-took-the-other-just-as-fair",
    "title": "Two Roads Diverged",
    "section": "“… looked down one as far as I could … then took the other, just as fair…”",
    "text": "“… looked down one as far as I could … then took the other, just as fair…”\nSo: We have multiple dialects. How to choose which one to use?\nThere is no single right answer to that question; it’s all a matter of individual preference and use case. What do you, as the programmer, value most? Brevity of code? Readability of code? Speed? Familiarity? Consistency with collaborators? Availability of learning resources?\nI could go on - there are infinitely many reasons, from the personal to the professional to the practical, to choose one path or the other. Sometimes, the answer is as simple as, “This is the way that I know how to do it.”\nI can’t tell you how to pick what works for you.\n\n\n\n\n\n“Dialect” or syntax choices in R are contextual and case-by-case, not lifetime commitments!\n\n\n\nThe idea of “loyalty” to a package is nonsense. A package is a tool. You might have admiration, respect, or even loyalty to a package developer; you might even therefore trust that it’s worth your time and energy to follow their recommendations.\nBut if you start feeling bad when you sprinkle a little Base R into your tidy workflow… if you are ashamed for piping a data.table object into ggplot… well that’s getting us nowhere, is it? We are blessed with an overabundance of useful tools and we shouldn’t be limiting ourselves!\n\n## Great news!  This is not illegal!\n\npenguins %&gt;%\n  data.table() %&gt;%\n  .[, .(mean_bill=mean(bill_length_mm)), by=species]\n\nWe all use our own favorite collection of packages, in the combinations that work for us. It might be fun to discuss and learn about new options or new preferences, but no more purity culture, please!\n\n\n\n\n\nIt’s okay to use different syntaxes and package styles all in one workflow!"
  },
  {
    "objectID": "posts/2024-05-20-kelly_bodwin/index.html#i-shall-be-telling-this-with-a-sigh-somewhere-ages-and-ages-hence",
    "href": "posts/2024-05-20-kelly_bodwin/index.html#i-shall-be-telling-this-with-a-sigh-somewhere-ages-and-ages-hence",
    "title": "Two Roads Diverged",
    "section": "“…I shall be telling this with a sigh, somewhere ages and ages hence…”",
    "text": "“…I shall be telling this with a sigh, somewhere ages and ages hence…”\nIt has come to the point where I can’t avoid mentioning what mainly motivated this blog post: The Great Twitter War of 2018. (Please read that sentence with every ounce of irony you have in you).\nBriefly for those who weren’t “lucky” enough to be in the tweetstorm: Sometime around 2018, the #rstats Twitter community exploded into a debate about the relative merits of the tidyverse, data.table, and Base R.\n\n\n\nIt was basically a lot of this. (Source: XKCD#386)\n\n\nIt’s sad to me that the community seems to remember this time as a fight, because so much of that conversation was productive and interesting. Educators shared their experiences teaching with different dialects. Developers talked about the speed trade-offs of the various options. New users were excited to be exposed to information about their options.\nBut - as seems to be the norm on the internet - a vocal subset of this conversation took the form of an “us vs. them” debate, and weird lines were drawn between data.table/Base R and the tidyverse.\n\n\n\nRabblerabblerabble\n\n\nIt’s important to note that the primary developers themselves - Hadley Wickham and Matt Dowle - were not the cause of the drama. In fact, this good conversations from this Twitter whirlwind lead to the creation of one of my favorite packages, dtplyr!\nSo why am I partially digging up a buried hatchet?\nBecause sadly, even today, I sometimes run into vitriol when I post on social media about data.table or the tidyverse, and I know I’m not alone in this.\nEven today, I have my college students asking me about the rift in the R world, and if they have to “choose a side” to learn R.\nAnd most relevant to this blog - I have gotten a lot of questions about why I am involved in a data.table project, since I’m “supposed” to be Team Tidyverse.\nTherefore, to be ultra clear:\n\n\n\n\n\nThis grant is NOT about helping data.table “beat” dplyr.\n\n\n\nThis could not be further from the truth! I’m a tidyverse girlie - from my dplyr earrings to my hex fabric shirts - and I am also a data.table girlie. I, personally, would not be working on this project if I thought anyone involved viewed it as anti-tidyverse in any way.\nWhat we want is the same thing any open-source fan wants:\n\nWe want users to be aware of the many fantastic tools, including data.table, that exist in the R world.\nWe want developers to be inspired to build new and exciting packages, that stand on the shoulders of giants like data.table, the tidyverse, and so many others.\nWe want beloved packages like data.table to stick around long term, and to grow and evolve with R and the R community."
  },
  {
    "objectID": "posts/2024-05-20-kelly_bodwin/index.html#and-that-has-made-all-the-difference.",
    "href": "posts/2024-05-20-kelly_bodwin/index.html#and-that-has-made-all-the-difference.",
    "title": "Two Roads Diverged",
    "section": "“… and that has made all the difference.”",
    "text": "“… and that has made all the difference.”\nSo, where are we going from here, as a community? Only good places, I think, no matter which path we take in the yellow wood!\nI am so excited about this project and about the NSF-POSE grant - both for the longevity of data.table, and for everything we are learning about open-source ecosystems and how to sustain them.\n\n\n\n\n\nI love the #rstats community!!! Let’s do cool stuff together."
  },
  {
    "objectID": "posts/2024-05-20-kelly_bodwin/index.html#addendum",
    "href": "posts/2024-05-20-kelly_bodwin/index.html#addendum",
    "title": "Two Roads Diverged",
    "section": "Addendum",
    "text": "Addendum\nWant to hear me rant more about the R community, multiple dialects/languages, and this grant project? I’ll be speaking on these topics at UseR!2024, JSM, and Posit::conf - or you can always find me on BlueSky or Fosstodon!"
  },
  {
    "objectID": "posts/2024-10-10-Performance-Doris_Amoakohene/index.html",
    "href": "posts/2024-10-10-Performance-Doris_Amoakohene/index.html",
    "title": "Visualizing performance regression of data.table with atime",
    "section": "",
    "text": "Since August 2023, I have been working on performance testing, which could be useful for expanding the open-source ecosystem around data.table package in R. This could increase confidence in code contributions by ensuring the sustained efficiency of the data.table package.\nIn data.table, the term “performance regression” refers to a change to the data.table source code, or to the core R build, that causes an increase in either time metrics and memory metrics.\nIt is important that we prevent significant performance regression from reaching the current release of the data.table package. Slowness or big memory usage can be frustrating; and in fact, are the issues data.table is most used to solve. Any performance regression that makes it into a version release will degrade user experience.\nIn this blog post, I will demonstrate the use of benchmarking techniques to verify whether reported issues on data.table have been successfully resolved."
  },
  {
    "objectID": "posts/2024-10-10-Performance-Doris_Amoakohene/index.html#overview",
    "href": "posts/2024-10-10-Performance-Doris_Amoakohene/index.html#overview",
    "title": "Visualizing performance regression of data.table with atime",
    "section": "Overview",
    "text": "Overview\n\nUnderstanding performance in data.table\ndata.table is an extension of R’s data.frame, designed to handle large datasets efficiently. It provides a syntax that is both concise and expressive, allowing users to perform complex data manipulations with ease. Its efficiency is particularly evident when dealing with tasks like filtering, grouping, aggregating, and joining data.\nThe development team behind data.table is committed to continuously improving its performance. Over the years, several major version changes have been introduced, aiming to enhance speed and efficiency. These changes include algorithmic optimizations, memory management improvements, and enhancements to parallel processing capabilities. Upgrading to the latest version ensures that users can leverage the most recent performance enhancements.\n\n\nWhy do we run performance tests on GitHub commits?\nRunning performance tests on GitHub commits helps maintain a high-performance standard for the package, detect and fix performance regressions, optimize code, validate performance improvements, ensure consistent performance over time and to encourage confidence in code contributions from new people.\nIt is an essential practice to deliver a performant and reliable package to end-users.\n\n\nBenchmarking for performance evaluation\nTo evaluate data.table performance, it is essential to employ benchmarking methodologies. The approach I used utilizes the atime_versions function from the atime package, which measures the actual execution time of specific operations. This function allows for accurate comparisons between different versions of the data.table package, by benchmarking against time and memory usage and giving a graphical visualization of the results."
  },
  {
    "objectID": "posts/2024-10-10-Performance-Doris_Amoakohene/index.html#details-of-the-performance-tests",
    "href": "posts/2024-10-10-Performance-Doris_Amoakohene/index.html#details-of-the-performance-tests",
    "title": "Visualizing performance regression of data.table with atime",
    "section": "Details of the performance tests",
    "text": "Details of the performance tests\nThe primary function atime_versions has six main arguments:\n\npkg.path: This argument specifies the location on your system where you have stored a git clone of the data.table package.\npkg.edit.fun: The default behavior of pkg.edit.fun is designed to work with Rcpp packages and involves replacing instances of “PKG” with “PKG.SHA” in the package code. Any occurrences of the string “PKG” within the package code will be replaced with “PKG.SHA”, where “SHA” represents the commit SHA/ids associated with the version being installed.\nN: This argument determines the number of iterations for the benchmarking process. It is a sequence of numbers that define different data sizes to test the performance of the operation.\nsetup: This section contains the setup code for generating the dataset used in the benchmarking process, the setup is determined by the value of N.\nexpr: This section contains the expression that represents the operation being benchmarked. It uses the data.table::[.data.table`` syntax to perform the operation on the dataset.\n\nIn the given syntax data.table::`[.data.table`, the first part data.table:: installs and loads different versions of the data.table package based on the specified commit ids. Hence, data.table:: will be translated to data.table.SHA1:: for some version hash SHA1. Following that, the expression specified within `[.data.table `` is executed on each installed version. This process is repeated for all the specified commit IDs in the code.\nFor example:\ndata.table.ec1259af1bf13fc0c96a1d3f9e84d55d8106a9a4:::`[.data.table`(DT, , .(v3=mean(v3, na.rm=TRUE)), by=id3, verbose=TRUE)\nIn this example, the expression [.data.table is executed on the DT dataset using the specified commit ID (ec1259af1bf13fc0c96a1d3f9e84d55d8106a9a4) of the data.table package. The expression calculates the mean of the v3 column (ignoring missing values) grouped by id3, and the verbose=TRUE argument enables verbose output during the operation. This process is typically repeated for all commit IDs in your code to compare the performance of different versions of the data.table package.\n\n... : This specifies the different versions of the data.table packages that will be tested. It includes three versions: “Before,” “Regression,” and “Fixed.” Each version is associated with a specific commit id.\n\n\nTest procedure\nWe run the full performance regression with atime:\n\nBefore the change causing performance regression is made (Before)\nWhen the change causing performance regression is first submitted (Regression)\nAfter the Pull Request (PR) which fixes the performance regression (Fixed)\n\n\n\nOverall workflow\nWhen a fixing Pull Request is submitted, our procedure automatically takes the following steps:\n\nPass the hashes for different branches (Before, Regression, Fix) to atime_versions; along with various parameters for the test (number of simulations, code expression to run, etc.).\nUse the atime_versions function to measure time and memory usage across different versions.\nGenerate a plot to showcase the test results, using the atime package built in plotting functions.\nDisplay the plot and test results as a comment on the submitted Pull Request.\n\nHere is an example of how to perform the atime test. More documentation of the atime package can be found here."
  },
  {
    "objectID": "posts/2024-10-10-Performance-Doris_Amoakohene/index.html#example",
    "href": "posts/2024-10-10-Performance-Doris_Amoakohene/index.html#example",
    "title": "Visualizing performance regression of data.table with atime",
    "section": "Example",
    "text": "Example\nThe first example we will show is an issue reported on performing group computations, specifically when running R’s C eval: link to GitHub Issue that reported regression. This regression was caused by the inclusion of the certain code within the #if block. This PR discusses the specific C code in q7 and q8 in the “db-benchmark” which causes the regression.\nThis PR fixed the regression problem.\nThe details of the code problems and solutions are not required for the example; we link them only to share a map of the regression-and-fix process.\nTo produce performance test results, we first load package dependencies, as well as the current GitHub snapshot of data.table in development:\n\nlibrary(atime)\nlibrary(ggplot2)\nlibrary(data.table)\n\ntdir &lt;- tempfile()\ndir.create(tdir)\ngit2r::clone(\"https://github.com/Rdatatable/data.table\", tdir ) \n\nNext, we establish our performance test. Here, we will create a data.table object and then compute the range by group. We vary the size of the object by varying values of N across tests.\n\nd &lt;- data.table(\n      id3 = sample(c(seq.int(N*0.9), sample(N*0.9, N*0.1, TRUE))),\n      v1 = sample(5L, N, TRUE),\n      v2 = sample(5L, N, TRUE)\n      )\n\ndata.table:::`[.data.table`(d, , (max(v1)-min(v2)), by = id3)\n\nThis setup and expression is then passed to atime_versions, along with a bit of package management information, and hashes (a.k.a. “Commit ID” or “SHA”) for the commits before, during, and after the performance regression.\n\natime.list.4200 &lt;- atime::atime_versions(\n  pkg.path = tdir,\n  pkg.edit.fun = pkg.edit.fun,\n  N = 10^seq(1,20),\n  setup = { \n    set.seed(108)\n    d &lt;- data.table(\n      id3 = sample(c(seq.int(N*0.9), sample(N*0.9, N*0.1, TRUE))),\n      v1 = sample(5L, N, TRUE),\n      v2 = sample(5L, N, TRUE))\n  },\n  expr = data.table:::`[.data.table`(d, , (max(v1)-min(v2)), by = id3),\n  \"Before\" = \"793f8545c363d222de18ac892bc7abb80154e724\", # commit hash in PR prior to regression\n  \"Regression\" = \"c152ced0e5799acee1589910c69c1a2c6586b95d\", # commit hash in PR causing regression\n  \"Fixed\" = \"f750448a2efcd258b3aba57136ee6a95ce56b302\" # commit hash in PR that fixes the regression\n)\n\n\n\n\n\n\n\nNote\n\n\n\nThe function pkg.edit.fun that is passed to atime_versions above is a custom function written to manage the packages and paths on the server running this test.\nYou can see the code below if you wish.\n\n\n\n\nCode\npkg.edit.fun=function(old.Package, new.Package, sha, new.pkg.path){\n      pkg_find_replace &lt;- function(glob, FIND, REPLACE){\n        atime::glob_find_replace(file.path(new.pkg.path, glob), FIND, REPLACE)\n      }\n      Package_regex &lt;- gsub(\".\", \"_?\", old.Package, fixed=TRUE)\n      Package_ &lt;- gsub(\".\", \"_\", old.Package, fixed=TRUE)\n      new.Package_ &lt;- paste0(Package_, \"_\", sha)\n      pkg_find_replace(\n        \"DESCRIPTION\", \n        paste0(\"Package:\\\\s+\", old.Package),\n        paste(\"Package:\", new.Package))\n      pkg_find_replace(\n        file.path(\"src\",\"Makevars.*in\"),\n        Package_regex,\n        new.Package_)\n      pkg_find_replace(\n        file.path(\"R\", \"onLoad.R\"),\n        Package_regex,\n        new.Package_)\n      pkg_find_replace(\n        file.path(\"R\", \"onLoad.R\"),\n        sprintf('packageVersion\\\\(\"%s\"\\\\)', old.Package),\n        sprintf('packageVersion\\\\(\"%s\"\\\\)', new.Package))\n      pkg_find_replace(\n        file.path(\"src\", \"init.c\"),\n        paste0(\"R_init_\", Package_regex),\n        paste0(\"R_init_\", gsub(\"[.]\", \"_\", new.Package_)))\n      pkg_find_replace(\n        \"NAMESPACE\",\n        sprintf('useDynLib\\\\(\"?%s\"?', Package_regex),\n        paste0('useDynLib(', new.Package_))\n    }\n\n\n\nResults\nThe atime package uses the results of the performance test to create the following plot:\n\n\n\nPlot showing the 3 branches (Regression, Fixed and Before) of the issues in #4200\n\n\nThe graph compares the time required to execute the operation before, during, and after fixing a regression issue. The x-axis (N) represents the size of the data on a logarithmic scale. The y-axis represents the median time in milliseconds (logarithmic scale).\nLines:\n“Before”: Indicates performance before fixing the regression; we hope to achieve this performance after fixing.\n“Regression”: Represents an ideal or target performance level.\n“Fixed”: Shows improved performance after fixing.\nIn the graph, as data size (N) increases, there’s an initial increase in median time, but after addressing the regression issue, there is a significant reduction in the median time, indicating improved performance (fix). The regression issue was successfully addressed."
  },
  {
    "objectID": "posts/2024-10-10-Performance-Doris_Amoakohene/index.html#automated-testing-with-github-actions",
    "href": "posts/2024-10-10-Performance-Doris_Amoakohene/index.html#automated-testing-with-github-actions",
    "title": "Visualizing performance regression of data.table with atime",
    "section": "Automated testing with Github Actions",
    "text": "Automated testing with Github Actions\nAs part of the data.table ecosystem project, Anirban Chetia has implemented a GitHub Action to automatically run performance tests any time the data.table repository is Pull Requested. This action runs the atime performance test and generates plots of the results in a comment within the pull request. See an example in this pull request.\nThis action allows the package maintainers to easily determine if a Pull Request has any impact on the time or memory usage of the build for the data.table package. To learn more you can visit Anirban’s documentation or this ReadMe about the atime package"
  },
  {
    "objectID": "posts/2024-10-10-Performance-Doris_Amoakohene/index.html#conclusion",
    "href": "posts/2024-10-10-Performance-Doris_Amoakohene/index.html#conclusion",
    "title": "Visualizing performance regression of data.table with atime",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, we have delved into the use of the atime package to compare the asymptotic time and memory usage of different development versions of the data.table package. Specifically, we visualized the comparisons between the “Before,” “Regression,” and “Fixed” versions for a specific performance regression issue.\nBy employing benchmarking methodologies like atime, we gain valuable insights into the performance characteristics of proposed updates to the data.table package. This allowed us to identify and address performance regressions, ensuring that each new version of the package has indeed solved the particular issue reported.\nFor more examples or practice with atime and regression, you can visit this link and the corresponding fix PR here."
  },
  {
    "objectID": "posts/2024-01-14-ambassador_gorecki-community_team/index.html",
    "href": "posts/2024-01-14-ambassador_gorecki-community_team/index.html",
    "title": "Announcement: Jan Gorecki, data.table Ambassador",
    "section": "",
    "text": "A round of applause!\n\n\nJan is a natural choice for an Ambassador, due to his many years of fantastic contribution to the data.table package. You can find his great work in open-source development at github.com/jangorecki. Find him on Mastodon at fosstodon.org/@jangorecki to say congratulations, or chat about all things data.table."
  },
  {
    "objectID": "posts/2024-01-14-ambassador_gorecki-community_team/index.html#please-join-me-in-congratulting-our-first-ever-data.table-ambassador-jan-gorecki",
    "href": "posts/2024-01-14-ambassador_gorecki-community_team/index.html#please-join-me-in-congratulting-our-first-ever-data.table-ambassador-jan-gorecki",
    "title": "Announcement: Jan Gorecki, data.table Ambassador",
    "section": "",
    "text": "A round of applause!\n\n\nJan is a natural choice for an Ambassador, due to his many years of fantastic contribution to the data.table package. You can find his great work in open-source development at github.com/jangorecki. Find him on Mastodon at fosstodon.org/@jangorecki to say congratulations, or chat about all things data.table."
  },
  {
    "objectID": "posts/2024-01-14-ambassador_gorecki-community_team/index.html#talks",
    "href": "posts/2024-01-14-ambassador_gorecki-community_team/index.html#talks",
    "title": "Announcement: Jan Gorecki, data.table Ambassador",
    "section": "Talks",
    "text": "Talks\nAs part of his Ambassador role, Jan will be giving three exciting talks this year:\n\nRolling statistics and moving windows in Edinburgh on January 26th, with the Edinburgh R Users group.\n\nRolling statistics are an interesting topic for optimizations, therefore in my talk I will use R language to present naive implementation, and the optimized implementation, on a simple case of rolling mean. Then I will move to data.table implementations of rolling statistics explaining possible optimizations in other functions, which are not that straightforward anymore, like min/max and, actually very complex, median. Finally benchmarks will be presented comparing data.table implementations to base R, pandas, polars, slider/dplyr, duckdb and spark.\n\n\n\nHigh-productivity data frame operations with data.table on February 8th in Sevilla, Spain with the Sevilla R Users group\n\n\n\nTalk with SevillaR\n\n\nRegister to watch online here\n\n\nThe Spanish R Conference in Sevilla, Spain. (Date and topic TBD)\nWe look forward to Jan’s excellent talks, and a forthcoming blog post right here on The Raft to share his experiences - and of course, his continued wonderful contributions to the data.table community. Thank you, Jan!"
  },
  {
    "objectID": "posts/2024-01-14-ambassador_gorecki-community_team/index.html#become-an-ambassador",
    "href": "posts/2024-01-14-ambassador_gorecki-community_team/index.html#become-an-ambassador",
    "title": "Announcement: Jan Gorecki, data.table Ambassador",
    "section": "Become an Ambassador",
    "text": "Become an Ambassador\nDo you have ideas for talks about data.table? Do you want to be part of this new community movement? Apply now for the data.table Ambassadors Grant to fund conference travel for presentations related to data.table. More details on the Ambassadors program at this blog post, and more details on the grant project itself are here.\nQuestions? Email r.data.table@gmail.com with any and all questions!"
  },
  {
    "objectID": "posts/2024-08-20-translation_projects-community_team/index.html",
    "href": "posts/2024-08-20-translation_projects-community_team/index.html",
    "title": "Newly awarded translation projects",
    "section": "",
    "text": "We are pleased to fund a French translation project, led by Philippe Grosjean, who is also the leader of the base R French translation. Co-authors include Christian Wia, Vincent Rocher, Vincent Runge, and Elise Maigné. The project has been awarded US$2000 to create and maintain the following translations:\n\nall messages from the R-data.table.pot and data.table.pot files (approximately 1300 errors, warnings, etc).\nall 12 vignettes.\nthe cheatsheet, which was recently updated!"
  },
  {
    "objectID": "posts/2024-08-20-translation_projects-community_team/index.html#french-translation-project",
    "href": "posts/2024-08-20-translation_projects-community_team/index.html#french-translation-project",
    "title": "Newly awarded translation projects",
    "section": "",
    "text": "We are pleased to fund a French translation project, led by Philippe Grosjean, who is also the leader of the base R French translation. Co-authors include Christian Wia, Vincent Rocher, Vincent Runge, and Elise Maigné. The project has been awarded US$2000 to create and maintain the following translations:\n\nall messages from the R-data.table.pot and data.table.pot files (approximately 1300 errors, warnings, etc).\nall 12 vignettes.\nthe cheatsheet, which was recently updated!"
  },
  {
    "objectID": "posts/2024-08-20-translation_projects-community_team/index.html#portuguese-translation-project",
    "href": "posts/2024-08-20-translation_projects-community_team/index.html#portuguese-translation-project",
    "title": "Newly awarded translation projects",
    "section": "Portuguese translation project",
    "text": "Portuguese translation project\nWe are please to fund a Portuguese translation project, led by Leonardo Ferreira Fontenelle. Co-authors include Rafael Ferreira Fontenelle and Italo de Oliveira Santos. The aim is to provide a pt_BR translation which would make data.table more accessible to the 200 million people in Brazil. This team has been awarded US$2000 to create and maintain translations of the messages (errors, warnings, etc)."
  },
  {
    "objectID": "posts/2023-11-01-travel_grant_announcement-community_team/index.html",
    "href": "posts/2023-11-01-travel_grant_announcement-community_team/index.html",
    "title": "Announcement: The data.table Ambassadors Travel Grant",
    "section": "",
    "text": "We on the community team are very excited to announce another major funding opportunity!\nAs you may know, the National Science Foundation (NSF) has provided funds to support the project “Expanding the data.table ecosystem for efficient big data manipulation in R.” One goal of this project is to better disseminate information about the development and progress of the data.table package.\nTo this end, applications are now open for the data.table Ambassadors Grant to fund conference travel for presentations related to data.table."
  },
  {
    "objectID": "posts/2023-11-01-travel_grant_announcement-community_team/index.html#ambassadors",
    "href": "posts/2023-11-01-travel_grant_announcement-community_team/index.html#ambassadors",
    "title": "Announcement: The data.table Ambassadors Travel Grant",
    "section": "Ambassadors",
    "text": "Ambassadors\nIn order to become a funded ambassador, applicants must meet the following requirements:\n\nPresent a talk, poster, or tutorial at a relevant conference or meeting that is in some way related to the data.table package.\nWrite a blog post about the talk/presentation for The Raft.\n\nFunding is in the form of a flat $2700 stipend towards travel and registration expenses, to be paid upon completion of the presentation and blog post.\nThis program will fund up to 4 Ambassadors per year for the next three years. We are particularly interested in facilitating travel for applicants who may not have as much opportunity; such as early-career researchers, or those from historically marginalized communities.\n\n\n\nThe data.table community is everywhere!"
  },
  {
    "objectID": "posts/2023-11-01-travel_grant_announcement-community_team/index.html#applying",
    "href": "posts/2023-11-01-travel_grant_announcement-community_team/index.html#applying",
    "title": "Announcement: The data.table Ambassadors Travel Grant",
    "section": "Applying",
    "text": "Applying\n\nFill out the Google Form here to apply for the Ambassador Grant.\n\nYou will be asked to provide:\n\nInformation about your presentation:\n\nA max 1-page abstract for your proposed talk or workshop.\nA categorization of the talk: Is it a technical contribution to data.table or related packages? An applied use case for data.table in industry or education? A tutorial or workshop teaching data.table skills to attendees?\nThe conference or meeting at which your presentation has been accepted. (Presentations that are submitted, but not yet accepted, may apply; in exceptional cases, Grant funding may be offered conditional on acceptance.)\nA max 1-page statement addressing the following prompt: How does your proposed talk or presentation further the goals of the NSF-POSE Grant Program? How will it enhance the data.table community and ecosystem?\n\n\n\nPersonal Statements:\n\nA brief statement regarding your personal connection to the data.table community, and why you are motivated to become an Ambassador.\nOptionally, materials or links showing your prior work and connection to data.table.\nOptionally, your self-identification as a member of a historically marginalized community or an early-career researcher.\n\n\n\nWe can’t wait to hear from you!\n\n\n\n“Water” you waiting for? Apply now!"
  },
  {
    "objectID": "posts/2023-11-01-travel_grant_announcement-community_team/index.html#questions",
    "href": "posts/2023-11-01-travel_grant_announcement-community_team/index.html#questions",
    "title": "Announcement: The data.table Ambassadors Travel Grant",
    "section": "Questions",
    "text": "Questions\nNot sure if you should apply? Wondering if there are still Ambassador slots left in the year? Email r.data.table@gmail.com with any and all questions!"
  },
  {
    "objectID": "posts/2024-10-01-seal_of_approval-mlr3/index.html",
    "href": "posts/2024-10-01-seal_of_approval-mlr3/index.html",
    "title": "Seal of Approval: mlr3",
    "section": "",
    "text": "Author(s): Michel Lang, Bernd Bischl, Jakob Richter, Patrick Schratz, Martin Binder, Florian Pfisterer, Raphael Sonabend, Marc Becker, Sebastian Fischer\nMaintainer: Marc Becker (marcbecker@posteo.de)\nSeal of Approval\n\n\n\nmlr3 hex sticker\n\n\nA modern object-oriented machine learning framework. Successor of mlr."
  },
  {
    "objectID": "posts/2024-10-01-seal_of_approval-mlr3/index.html#mlr3",
    "href": "posts/2024-10-01-seal_of_approval-mlr3/index.html#mlr3",
    "title": "Seal of Approval: mlr3",
    "section": "",
    "text": "Author(s): Michel Lang, Bernd Bischl, Jakob Richter, Patrick Schratz, Martin Binder, Florian Pfisterer, Raphael Sonabend, Marc Becker, Sebastian Fischer\nMaintainer: Marc Becker (marcbecker@posteo.de)\nSeal of Approval\n\n\n\nmlr3 hex sticker\n\n\nA modern object-oriented machine learning framework. Successor of mlr."
  },
  {
    "objectID": "posts/2024-10-01-seal_of_approval-mlr3/index.html#relationship-with-data.table",
    "href": "posts/2024-10-01-seal_of_approval-mlr3/index.html#relationship-with-data.table",
    "title": "Seal of Approval: mlr3",
    "section": "Relationship with data.table",
    "text": "Relationship with data.table\nmlr3 was designed to integrate closely with data.table for efficient data handling in machine learning workflows. There are two main ways mlr3 is related to data.table:\n\nData Backend: mlr3 uses data.table as the core data backend for all Task objects. This means that when you work with tasks in mlr3, the underlying data is stored and managed using data.table. Moreover, users can leverage data.table syntax directly within mlr3 workflows. Accessing task data via task$data() returns a data.table, enabling you to apply data.table operations for data preprocessing, feature engineering, and subsetting without any additional conversion or overhead.\nResult Storage: mlr3 stores various results such as predictions, resampling outcomes, and benchmarking results as data.table objects."
  },
  {
    "objectID": "posts/2024-10-01-seal_of_approval-mlr3/index.html#overview",
    "href": "posts/2024-10-01-seal_of_approval-mlr3/index.html#overview",
    "title": "Seal of Approval: mlr3",
    "section": "Overview",
    "text": "Overview\nExcerpted from the mlr3 book\nThe mlr3 universe includes a wide range of tools taking you from basic ML to complex experiments. To get started, here is an example of the simplest functionality – training a model and making predictions.\n\nlibrary(mlr3)\n\ntask = tsk(\"penguins\")\nsplit = partition(task)\nlearner = lrn(\"classif.rpart\")\n\nlearner$train(task, row_ids = split$train)\nlearner$model\n\nn= 230 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 230 129 Adelie (0.439130435 0.186956522 0.373913043)  \n  2) flipper_length&lt; 206.5 140  41 Adelie (0.707142857 0.285714286 0.007142857)  \n    4) bill_length&lt; 43.05 98   3 Adelie (0.969387755 0.030612245 0.000000000) *\n    5) bill_length&gt;=43.05 42   5 Chinstrap (0.095238095 0.880952381 0.023809524) *\n  3) flipper_length&gt;=206.5 90   5 Gentoo (0.022222222 0.033333333 0.944444444) *\n\n\n\nprediction = learner$predict(task, row_ids = split$test)\nprediction\n\n&lt;PredictionClassif&gt; for 114 observations:\n row_ids     truth  response\n       3    Adelie    Adelie\n       5    Adelie    Adelie\n       6    Adelie    Adelie\n     ---       ---       ---\n     342 Chinstrap Chinstrap\n     343 Chinstrap    Gentoo\n     344 Chinstrap Chinstrap\n\n\n\nprediction$score(msr(\"classif.acc\"))\n\nclassif.acc \n  0.9473684 \n\n\nIn this example, we trained a decision tree on a subset of the penguins dataset, made predictions on the rest of the data and then evaluated these with the accuracy measure.\nThe mlr3 interface also lets you run more complicated experiments in just a few lines of code:\n\nlibrary(mlr3verse)\n\ntasks = tsks(c(\"german_credit\", \"sonar\"))\n\nglrn_rf_tuned = as_learner(ppl(\"robustify\") %&gt;&gt;% auto_tuner(\n    tnr(\"grid_search\", resolution = 5),\n    lrn(\"classif.ranger\", num.trees = to_tune(200, 500)),\n    rsmp(\"holdout\")\n))\nglrn_rf_tuned$id = \"RF\"\n\nglrn_stack = as_learner(ppl(\"robustify\") %&gt;&gt;% ppl(\"stacking\",\n    lrns(c(\"classif.rpart\", \"classif.kknn\")),\n    lrn(\"classif.log_reg\")\n))\nglrn_stack$id = \"Stack\"\n\nlearners = c(glrn_rf_tuned, glrn_stack)\nbmr = benchmark(benchmark_grid(tasks, learners, rsmp(\"cv\", folds = 3)))\n\nbmr$aggregate(msr(\"classif.acc\"))\n\n      nr       task_id learner_id resampling_id iters classif.acc\n   &lt;int&gt;        &lt;char&gt;     &lt;char&gt;        &lt;char&gt; &lt;int&gt;       &lt;num&gt;\n1:     1 german_credit         RF            cv     3   0.7749966\n2:     2 german_credit      Stack            cv     3   0.7450175\n3:     3         sonar         RF            cv     3   0.8077295\n4:     4         sonar      Stack            cv     3   0.7121463\nHidden columns: resample_result\n\n\nIn this more complex example, we selected two tasks and two learners, used automated tuning to optimize the number of trees in the random forest learner, and employed a machine learning pipeline that imputes missing data, consolidates factor levels, and stacks models. We also showed basic features like loading learners and choosing resampling strategies for benchmarking. Finally, we compared the performance of the models using the mean accuracy with three-fold cross-validation."
  },
  {
    "objectID": "posts/2024-03-06-interviews-anirban_chetia/index.html",
    "href": "posts/2024-03-06-interviews-anirban_chetia/index.html",
    "title": "Community interviews about data.table",
    "section": "",
    "text": "One stipulation of NSF POSE funded projects like this one was to conduct several interviews under NSF’s I-CORPS program (Winter 2024 Cohort), to gather information as to how data.table as an open-source project can improve and remain sustainable. For four weeks starting on the 17th of January, I conducted a total of 60 interviews with R Users and data.table contributors. Issue#5880 on the data.table GitHub mentions this, and has a link to the Google Doc that contains the list of people interviewed.\n\nProject PI Toby Hocking assigned me to do these interviews and serve as the EL (Entrepreneurial Lead) for the data.table team. In addition to the interviews, this position involves tasks such as making and giving various presentations. Having successfully completed the program and conducted the interviews, it’s time to share the insights I gathered from them as a source of open-ended knowledge for the community.\nBut before you head below to read those parts, I would like to convey a big Thank you! to everyone who took part in this; from making availability and scheduling, to providing comprehensive feedback, and all the while being extremely communicative. I sincerely appreciate it. Not just for the value of your insights brought to the table, but also for being great people to talk with in general!"
  },
  {
    "objectID": "posts/2024-03-06-interviews-anirban_chetia/index.html#whats-it-all-about",
    "href": "posts/2024-03-06-interviews-anirban_chetia/index.html#whats-it-all-about",
    "title": "Community interviews about data.table",
    "section": "",
    "text": "One stipulation of NSF POSE funded projects like this one was to conduct several interviews under NSF’s I-CORPS program (Winter 2024 Cohort), to gather information as to how data.table as an open-source project can improve and remain sustainable. For four weeks starting on the 17th of January, I conducted a total of 60 interviews with R Users and data.table contributors. Issue#5880 on the data.table GitHub mentions this, and has a link to the Google Doc that contains the list of people interviewed.\n\nProject PI Toby Hocking assigned me to do these interviews and serve as the EL (Entrepreneurial Lead) for the data.table team. In addition to the interviews, this position involves tasks such as making and giving various presentations. Having successfully completed the program and conducted the interviews, it’s time to share the insights I gathered from them as a source of open-ended knowledge for the community.\nBut before you head below to read those parts, I would like to convey a big Thank you! to everyone who took part in this; from making availability and scheduling, to providing comprehensive feedback, and all the while being extremely communicative. I sincerely appreciate it. Not just for the value of your insights brought to the table, but also for being great people to talk with in general!"
  },
  {
    "objectID": "posts/2024-03-06-interviews-anirban_chetia/index.html#selected-quotes",
    "href": "posts/2024-03-06-interviews-anirban_chetia/index.html#selected-quotes",
    "title": "Community interviews about data.table",
    "section": "Selected Quotes",
    "text": "Selected Quotes\nWe begin with some direct (anonymous) quotes from the interviews giving positive feedback and general statements about data.table.\n\n“data.table would be one of the few arguments (in addition to Shiny) that I could bring forward to make people use R instead of Python”\n\n\n“Using data.table, it becomes easier to read, manipulate, and represent data in a more appropriate way for my needs”\n\n\n“If I give my script to somebody, knowing that only data.table needs to be installed is reassuring”\n\n\n“I like it for minimalism and since it’s backward compatible with data.frame”\n\n\n“data.table was small enough to put my data into RAM (noticeable copy reduction compared to dplyr) and do analyses on my old laptop”\n\n\n“Very convenient to operate on lists as columns, wherein the base structure in which mlr3 is programmed around is in essence, a data.table”\n\n\n“Not something I’d recommend to everyone because of its peculiar syntax, but once you are used to it, I believe it can be very expressive while reducing lines of code dramatically”\n\n\n“I’m happy to see the community mobilization around this package since it brings such a valuable contribution to base R and is used by so many other packages.”\n\n\n“All the f-xyzfunctions are super useful. For example, it takes ages to read a big CSV with read_csv, while using fread sometimes doesn’t even allow me to grab a cup of coffee :)”\n\nNext we will summarize some consistent themes that will help guide the grant moving forward."
  },
  {
    "objectID": "posts/2024-03-06-interviews-anirban_chetia/index.html#theme-1-contribution-of-package-development",
    "href": "posts/2024-03-06-interviews-anirban_chetia/index.html#theme-1-contribution-of-package-development",
    "title": "Community interviews about data.table",
    "section": "Theme 1: Contribution of package development",
    "text": "Theme 1: Contribution of package development\nThe first theme of the interviews was what values motivate people or prevent people in contributing to the data.table project. These varied from person to person, but I summarize a few common answers below.\n\nPositive motivations\n\nFor work reasons, wherein their core goal is to help maintain their own software/package (downstream dependencies) by contributing back.\nFor visibility or for employment where data.table is a required/preferred tool of trade, and adds to their CV.\nUsing the software for not just work but for a personal project of interest where they derive usefulness from functions exported by data.table.\nMaking one a better programmer. This comes to be not just by contributing directly, but also by learning from others’ contributions.\nTrying to be a part of the data.table community, wherein the feeling of being a part of something big and crucial tends to be in play here. Making connections in the open-source community also tends to be an attraction.\nFinancial gain or incentives from working on the project. None of the people interviewed were paid to do so, but nearly all of them agreed that a paid position could be a win-win for the interested ones. For example, the author of data.table, Matt Dowle, was able to work on the package as part of his previous paid position at H20.ai.\n\n\n\nBarriers\n\nNot being a heavy user of the package. To both be reliant on active maintenance of the currently offered functionality and to see the value of time invested in contributing, one has to use data.table often enough - something not everyone does. The pressing need to rely on the tool needs to be existent for some to contribute.\nIt is not something they use in their current toolchain. This specifically applies to the people who were former contributors or users - For them data.table is a technology they used in the past for their former work/interests, and not something they require to or would consider using for their job at present. For some, they might need an entire career switch to even use R as well!\nLack of professional motivation and brevity. Some users are tightly occupied with work and do not have the time to contribute to open-source projects, let alone data.table (would be an addition to their list of competing priorities). For the few who do, they choose not to contribute either due to the lack of incentive, or due to the lack of motivation, primarily fueled by them thinking their contributions would not be satisfactory in comparison to what they would usually contribute to for the products they develop for their job.\nThe feeling of not having adequate knowledge to make meaningful contributions. For a handful, the codebase is overwhelming, and/or they are new to GitHub itself and not sure where to start digging, and/or they are not used to working with that type or level of code in R.\nLack of C programming knowledge. Although at the surface there is R, the core still has C and thus a fair proportion of the people interviewed mentioned that their inexperience with the language is one reason that holds them back from making contributions (especially ones that involve diving a bit deeper).\nTime in between submission and merging of a PR being rather long. This resonated with a few who contributed in the past, and some were just concerned for the ones who have or are contributing, as they may opt out of future contributions if their pull requests are left hanging for a long time. Given that it takes time to be thorough with changes introduced in pull requests (PRs), there is room for understanding if they are not merged timely or fast enough since in the long run, unintended consequences are always a possibility after the foreign code has been integrated (might break stuff and make it harder to debug later on). People do feel that taking the quick and easy route in merging PRs (especially big ones) can be challenging or that contributors and reviewing volunteers have limited time, however, they also think it can be faster (especially if more people are involved) and massive delays (ranging from several weeks to months) can be avoided. (especially since people would also lose context regarding their contributions and would need to revisit the discussion as a whole)\nSome users find the tone in the wording used to bring in people to be close-ended. They would be inclined and interested to contribute if the available reading material (such as the FAQ and Readme) is more inviting and friendly.\nDiversity and inclusion: For many, English is not their primary language, and a fair amount of jargon exists while going in the vignettes and documentation. The inclusion of more people through translations would be something to look forward to. A few also referenced wanting a welcoming culture in contribution. A broader approach might be required for newcomers to open issues and pull requests. Additionally, people tend to adopt the ‘contributor mode’ when sought upon - simple things like making it explicit on the GitHub repository that contributors are needed can pull them towards deciding to push a change.\n\n\n\nSummary and Takeaways\nSection added by Kelly Bodwin\nBased on these findings, we on the grant team see three major directions for encouraging more contribution to data.table:\n\nUse in projects: Interviewees reported adding their own functionality to data.table based on needs in personal or work projects. Others cited their own lack of data.table use as a reason not to be more involved. The more we can encourage practical adoption of data.table, where it can be useful to users, the more contribution we will see from users.\nFeeling of community and culture of inclusion: This is already a focus of the grant project, and it is great to hear that this is already valued by the users and community members! We hope to vastly expand the beginner-friendliness and language diversity of documentation.\nBeginner-friendliness and support: Interviewees reported not having the programming skills to add to data.table. Going forward, we hope to better denote and emphasize the areas of contribution for less experienced programmers, and to provide more supporting resources for new community members to learn about the structure of the package.\nFinancial and professional benefits: Contributors report that developing for data.table has positive impact on professional development and hireability, and that they would welcome financial incentives as well. I believe we should experiment with structure that help support our developers in concrete ways.\nPull Request process and timeline: We believe that the newly established Governance Document for the package will help clarify and streamline the contributor process for the future."
  },
  {
    "objectID": "posts/2024-03-06-interviews-anirban_chetia/index.html#theme-2-adoption-of-data.table",
    "href": "posts/2024-03-06-interviews-anirban_chetia/index.html#theme-2-adoption-of-data.table",
    "title": "Community interviews about data.table",
    "section": "Theme 2: Adoption of data.table",
    "text": "Theme 2: Adoption of data.table\nThe second theme is what drives people to be regular users of the data.table package. We mostly focused on barriers to adpotion.\n\nIndividual reasons to not adopt\nPeople cited various reasons for not utilizing or transitioning to data.table:\n\nLow rationale to switch to data.table when dealing with small datasets. In comparison to other R packages that achieve the same functionality, notable efficiency is mostly observed when the data being dealt with is considerably large (wherein the operations performed on them scale well to see a visible difference).\nOn the flip side, there are rare cases where people found data.table to be not scalable enough, and instead use database tools like DuckDB for their datasets which are increasingly larger than memory. (Some mentioned that making operations work on-disk would be something to look out for and implement in the future). An intermediate solution for them here would be to have a syntax translator tool that would translate data.table syntax to SQL queries, similar to what dbplyr is for dplyr commands, as they prefer the data.table way to write code and only require things to be more scalable.\nNot enough resources online to learn data.table in an easy yet detailed manner. This comes in stark contrast to abundant resources available for topics such as data.frame and dplyr for instance.\nLack of an integration with tidyverse.\nNot working with a group of collaborators/coworkers who primarily use data.table.\nFor a handful, the syntax does not come to be natural although they potentially benefit from the speed. They would stick to tidyverse or Base R in terms of being easier to use, unless they are running big computations where time is key.\nSome feel that data.table requires a certain level of understanding and experience in R prior to using it. They believe that it isn’t easy for newbies to adapt reasonably quickly, with regards to their own experiences in learning it. For reasons discussed above again, people with beginner-level experience in R or not enough reason to have code be the most efficient tend to stick with easier-to-use packages and functions, which is especially common in entry-level data science courses.\n\n\n\nAreas of improvement\nSpecific areas were identified by the interviewed population, including regular users, that they would like to see improvement in or be worked upon.\nIn terms of technical improvements:\n\nPeople miss fread being able to read fixed-width files, although there is iotools now.\nAn R core member gingerly pointed out that after several years of abundant reports of installation problems on MacOS, data.table still shows as being unable to detect OpenMP support and use multiple threads on the platform (while the same is not prevalent on Windows or Linux), even after including OpenMP run-time in CRAN R releases specifically for data.table. I noticed the same issue too on OS X being prevalent till date (and it has been there for a while as it appears, as I first encountered it more than two years ago).\nFew desired additional functionality for working with spatial geometries and mixed-model packages (such as glmmTMB and lme4).\nSome people found using the Walrus operator to be weird, especially given that not every data.table operation requires that. They don’t like keeping in mind the names of columns (they tend to move towards the set function for explicitly assigning) as well. The in-place assignment using := is acceptable as they feel, but they would ideally want to be able to do DT$x &lt;- y meaning DT[, x := y].\ndataset[get(\"categoricalColumn\")] could be optimized further (for reference, please check getDTeval or this paper).\nA few people find the syntax of dcast and melt to be confusing, and often end up making mistakes since those reshaping operations complement each other (long to wide and vice versa respectively) or are the reverse. Probably not best to change this given it would break things and is just something to be learned over time, but more examples might help.\nMore often a mild inconvenience than a common source of error, but to a few, the masking of functions from other packages (such as between, first, and last from dplyr, or transpose from purrr) is something they do not like.\n\nIn terms of the community revolving around data.table:\n\nDocumentation tends to be lacking, i.e. not enough well-documented data.table resources or online materials (blogs/articles, videos) exist. Even experienced developers feel that it isn’t entirely straightforward to find out how to do more complex things, so more extensive documentation and examples would be great, if not a necessity. Things that read as friendly and expressive while maintaining details are a go-to. Making the existing documentation more lucid and navigable is another point mentioned by a few.\nMore involvement is required on Stack Exchange or QA-oriented platforms that programmers and alike frequent. People mentioned that when they are looking for answers on Stack Overflow for questions that they come across in R, they find that data.table-based answers are lacking compared to dplyr.\nMore edu-centric approaches need to be undertaken. Most educational institutions do not resort to having data.table as part of their curriculum in R-based courses. While the coursework tends to be easier for newcomers, having more resources accessible can help people learn the more optimized version right from scratch and avoid learning it in the long run (almost all of the interviewees had to explore data.table on their own!) when efficiency or just the concise way of writing things is found to be better for some. Thus, it might help if instructors can start incorporating lessons using the package.\n\n\n\nSummary and Takeaways\nSection added by Kelly Bodwin\n\nEducation and Resources: It has been clear from the outset of this project that data.table could benefit from a lot more documentation, guides, tutorials, etc. This is always a tough issue, because creating such materials can be a thankless task with not a lot of concrete payoff. However, thanks to the grant, we are able to fund time for this project! Expect good things on the horizon in this category.\nSyntax and the R sub-languages: The diversity of R syntax is a blessing and a curse, and everyone has their favorite sytax style, from tidyverse to formula style to Base R to data.table, and every combination in between. Ultimately, our goal should be to be as flexible and possible and offer ways for data.table to interface smoothly with other styles, without losing it’s core syntax structure and personality! (dtplyr and tidyfast are lovely examples of such interfacing.)\nApplicability to the problem at hand: This is an interesting one. Can we do better at defining what dataset sizes and types are the best use cases for data.table? Can we provide more options for interfacing with databases, so that users can perhaps pull data using database tools, but analyze on-disk with data.table?"
  },
  {
    "objectID": "posts/2024-03-06-interviews-anirban_chetia/index.html#theme-3-open-source-sustainability",
    "href": "posts/2024-03-06-interviews-anirban_chetia/index.html#theme-3-open-source-sustainability",
    "title": "Community interviews about data.table",
    "section": "Theme 3: Open-source sustainability",
    "text": "Theme 3: Open-source sustainability\nFinally, we asked interviewees what might be necessary to give data.table long term sustainability.\n\nDevelopers and maintainers of packages dependent on data.table said they would be concerned if it was no longer maintained. Most of them said they don’t have the resources to maintain a fork and can’t depend on something that’s not developed as much. They would be happy to sponsor and help sustain the project.\nEven if data.table were to go into maintenance mode, people would continue using it as long as the existing functionality isn’t broken.\nOccasional users are not keen to become contributors. Heavy users are but would likely step back at one point. Any external contributors (non-users with generic contributions) are not sustainable. Most people were of the opinion that there will always be an influx of contributors given that data.table is a prominent package in the R ecosystem. Thus, stability can be achieved by a set of core (active) members plus a constant inflow of newcomers or periphery (drive-by) contributors.\nCommunity growth tends to be crucial in the long run, as people recognize that motivations to contribute are fluid and subject to change in the long run (career changes or simply switching to a different focus at some point in time). It becomes essential for maintainers to share knowledge to help onboard new active contributors, and for others to connect with them (a peer network is mutually beneficial).\nAcademia is one source for bringing in contributors, as professors or researchers can pay people to do services or research work. People believe there might be ample spare money from grants or educational funding.\nBecoming part of a foundation or community can help to share struggles and grant opportunities.\nBig companies look for people who are experts in a tool they use. Since data.table is in demand for data science and related roles in the job market, this might potentially bring in more contributors to learn and be good at the software.\nGitHub Sponsors is a convenient way for people on GitHub to fund the project. Individual developers and maintainers of data.table can have their pages as they deem necessary, or some other source of sponsorship (such as Patreon or Buy Me a Coffee).\nA fair proportion is willing to contribute to such a central fund. As for how and to whom will these funds be dispersed, or what would be a good way to distribute them proportionately, remains a question.\n\n\nSummary and Takeaways\nSection added by Kelly Bodwin\nI don’t have much to add to this one - it’s clear that we need more support structure for open-source maintenance, whether from private sources or public grants or community sponsorship."
  },
  {
    "objectID": "posts/2024-03-06-interviews-anirban_chetia/index.html#anis-roadmap",
    "href": "posts/2024-03-06-interviews-anirban_chetia/index.html#anis-roadmap",
    "title": "Community interviews about data.table",
    "section": "Ani’s Roadmap",
    "text": "Ani’s Roadmap\n\nHere is my list of ideas to potentially do or keep in mind for the agenda going forward:\n\nCreating a GitHub Sponsors page for the Rdatatable organization.\nApplying for funding from organizations such as NumFocus (5675) and being a part of communities such as rOpenSci.\nApplying for R Consortium grants, and using Kickstarter crowdfunding if required (for project maintenance and implementation of complex features) in the long run.\nGetting people involved in data.table-based projects for Summer/Winter of Code programs, such as Google Summer of Code (Interested? Check our page for this year if you would like to apply or take part!).\nBrainstorming and organizing Hackathons to spur interest, or some form of data.table-based events (like how tidy-dev-day exists for tidyverse).\nPromoting data.table via conferences and open-ended blogs/articles, showcasing its features and benefits of adoption.\nCreating videos and interactive tutorials.\nClassifying more of issues (if applicable) that fall under or are labelled as ‘beginner-task’."
  },
  {
    "objectID": "posts/2023-10-17-translation_announcement-toby_hocking/index.html",
    "href": "posts/2023-10-17-translation_announcement-toby_hocking/index.html",
    "title": "Announcement: data.table translation projects",
    "section": "",
    "text": "In 2023-2025, National Science Foundation (NSF) has provided funds to support the project “Expanding the data.table ecosystem for efficient big data manipulation in R.” One of the goals of this project is to create translations from English to other languages, in order to make data.table more accessible."
  },
  {
    "objectID": "posts/2023-10-17-translation_announcement-toby_hocking/index.html#motivation-make-data.table-more-accessible",
    "href": "posts/2023-10-17-translation_announcement-toby_hocking/index.html#motivation-make-data.table-more-accessible",
    "title": "Announcement: data.table translation projects",
    "section": "Motivation: make data.table more accessible",
    "text": "Motivation: make data.table more accessible\ndata.table is widely used (1400+ other R packages depend on it), so it is an essential part of the R data analysis ecosystem. One of the three goals of the NSF project is to create new documentation materials, to encourage a wider diversity of users and contributors (see the other post for an overview of project goals). In this project, an important part of the documentation efforts will be creating translations of data.table documentation, in languages other than English. The goal of this translation project is to make data.table easier and more accessible, for people who do not natively speak English.\nIn fact, data.table already has its source code strings (errors, warnings, etc) translated into Chinese, which is a good start. The goal of this project will be expanding the translations to other written materials (vignettes, slides, etc), as well as other languages with a substantial R user base. Based on a prior analysis by Michael Chirico, priority languages will include Chinese, Portuguese, Spanish, French, Russian, Arabic, Hindi."
  },
  {
    "objectID": "posts/2023-10-17-translation_announcement-toby_hocking/index.html#grant-supported-translations",
    "href": "posts/2023-10-17-translation_announcement-toby_hocking/index.html#grant-supported-translations",
    "title": "Announcement: data.table translation projects",
    "section": "Grant-supported translations",
    "text": "Grant-supported translations\nThe types of materials that we would like to translate are:\n\nerrors/warnings/messages defined in strings in R and C code. These are referred to as “messages” in the terminology of gettext. The potools package by Michael Chirico can help create the files in the required format. Some method of continuous updating/maintenance will be encouraged, such as github, or the R project weblate.\nmost important vignettes (intro, import, reshape)\nother documentation (cheat sheets, slides, etc)\n\nOver the course of the NSF project (Sep 2023 to Aug 2025), there will be 20 translation awards, each US$500."
  },
  {
    "objectID": "posts/2023-10-17-translation_announcement-toby_hocking/index.html#how-to-apply",
    "href": "posts/2023-10-17-translation_announcement-toby_hocking/index.html#how-to-apply",
    "title": "Announcement: data.table translation projects",
    "section": "How to apply",
    "text": "How to apply\nApplications should be submitted to toby.hocking@r-project.org, and should include the following information:\n\nwho is the project leader? Projects which include two or more people are encouraged, in order to proof-read translations, and ensure quality. The project leader should be a trusted member of the R community, who will be responsible for proof-reading and approving other project members’ translations.\nhow many people will be translating, and how many US$500 awards are you requesting?\nwhich non-English language?\nhow many regional dialects are represented on your team? For example, Spanish dialects: Mexican, Chilean, etc.\nfor each person in your project, how many years using R and data.table?\nwhat documents/materials do you propose to translate? (messages/vignettes/other)\nwhat is your proposed timeline for completing the translation of these documents/materials?\nwhat are your plans for ensuring that your translation is consistent with other R translations? (for example, base R messages, vignettes in other packages) In detail, there are several possible ways to translate any given text from English. For example, “computer” could be translated to Spanish as “ordinator” or “computador” or “computadora” but for consistency, only one of these should be used across all R documentation.\nafter the initial translation is complete, what is your plan for continued maintenance? In other words, when the corresponding English source files are updated on GitHub, what is your plan for being aware of those updates, and making corresponding updates of your translation, for a future release of data.table?\nhow will your translation project help create an expanded data.table ecosystem of users and contributors? You should make an argument that there are a large number of native speakers of your target language, using data such as in a prior analysis by Michael Chirico.\n\nApplications will be reviewed on a monthly basis, using the following criteria:\n\nWhat is the probability of success of this translation project, and its continued maintenace?\nWill this translation project help create an expanded ecosystem of users and contributors?"
  },
  {
    "objectID": "posts/2024-08-01-seal_of_approval-tidyfast/index.html",
    "href": "posts/2024-08-01-seal_of_approval-tidyfast/index.html",
    "title": "Seal of Approval: tidyfast",
    "section": "",
    "text": "tidyfast hex sticker\n\n\n\nAuthor(s): Tyson S. Barrett, Mark Fairbanks, Ivan Leung, Indrajeet Patil\nMaintainer: Tyson S. Barrett (t.barrett88@gmail.com)\nThe goal of tidyfast is to provide fast and efficient alternatives to some tidyr (and a few dplyr) functions using data.table under the hood. Each have the prefix of dt_ to allow for autocomplete in IDEs such as RStudio. These should compliment some of the current functionality in dtplyr (but notably does not use the lazy_dt() framework of dtplyr). This package imports data.table and cpp11 (no other dependencies). These are, in essence, translations from a more tidyverse grammar to data.table. Most functions herein are in places where, in my opinion, the data.table syntax is not obvious or clear. As such, these functions can translate a simple function call into the fast, efficient, and concise syntax of data.table."
  },
  {
    "objectID": "posts/2024-08-01-seal_of_approval-tidyfast/index.html#tidyfast",
    "href": "posts/2024-08-01-seal_of_approval-tidyfast/index.html#tidyfast",
    "title": "Seal of Approval: tidyfast",
    "section": "",
    "text": "tidyfast hex sticker\n\n\n\nAuthor(s): Tyson S. Barrett, Mark Fairbanks, Ivan Leung, Indrajeet Patil\nMaintainer: Tyson S. Barrett (t.barrett88@gmail.com)\nThe goal of tidyfast is to provide fast and efficient alternatives to some tidyr (and a few dplyr) functions using data.table under the hood. Each have the prefix of dt_ to allow for autocomplete in IDEs such as RStudio. These should compliment some of the current functionality in dtplyr (but notably does not use the lazy_dt() framework of dtplyr). This package imports data.table and cpp11 (no other dependencies). These are, in essence, translations from a more tidyverse grammar to data.table. Most functions herein are in places where, in my opinion, the data.table syntax is not obvious or clear. As such, these functions can translate a simple function call into the fast, efficient, and concise syntax of data.table."
  },
  {
    "objectID": "posts/2024-08-01-seal_of_approval-tidyfast/index.html#relationship-with-data.table",
    "href": "posts/2024-08-01-seal_of_approval-tidyfast/index.html#relationship-with-data.table",
    "title": "Seal of Approval: tidyfast",
    "section": "Relationship with data.table",
    "text": "Relationship with data.table\ntidyfast was designed to be an extension to and translation of data.table. As such, there are three main ways tidyfast is related to data.table.\n\nThis package is built directly on data.table using direct calls to [.data.table and other functions under the hood.\nIt only relies on two packages, cpp11 and data.table both stable packages that are unlikely to have breaking changes often. This follows the data.table principle of few dependencies.\nIt was designed to also show how others can use data.table within their own package to create functions that flexibly call data.table in complex ways."
  },
  {
    "objectID": "posts/2024-08-01-seal_of_approval-tidyfast/index.html#overview",
    "href": "posts/2024-08-01-seal_of_approval-tidyfast/index.html#overview",
    "title": "Seal of Approval: tidyfast",
    "section": "Overview",
    "text": "Overview\nAs shown on the tidyfast GitHub page, tidyfast has several functions that have the prefix dt_. A few notable functions from the package are shown below.\n\nlibrary(tidyfast)\nlibrary(data.table)\nlibrary(magrittr)\n\n\ndt_fill\nFilling NAs is a useful function but tidyr::fill(), especially when done by many, many groups can become too slow. dt_fill() is useful for this and can be used a few different ways.\n\nx = 1:10\ndt_with_nas &lt;- data.table(\n  x = x,\n  y = shift(x, 2L),\n  z = shift(x, -2L),\n  a = sample(c(rep(NA, 10), x), 10),\n  id = sample(1:3, 10, replace = TRUE)\n)\n\n# Original\ndt_with_nas\n\n        x     y     z     a    id\n    &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1:     1    NA     3     4     1\n 2:     2    NA     4    NA     1\n 3:     3     1     5    NA     2\n 4:     4     2     6     2     2\n 5:     5     3     7     3     3\n 6:     6     4     8     1     3\n 7:     7     5     9     9     3\n 8:     8     6    10    NA     1\n 9:     9     7    NA    10     2\n10:    10     8    NA     6     1\n\n# All defaults\ndt_fill(dt_with_nas, y, z, a, immutable = FALSE)\n\n        x     y     z     a    id\n    &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1:     1    NA     3     4     1\n 2:     2    NA     4     4     1\n 3:     3     1     5     4     2\n 4:     4     2     6     2     2\n 5:     5     3     7     3     3\n 6:     6     4     8     1     3\n 7:     7     5     9     9     3\n 8:     8     6    10     9     1\n 9:     9     7    10    10     2\n10:    10     8    10     6     1\n\n# by id variable called `grp`\ndt_fill(dt_with_nas, \n        y, z, a, \n        id = list(id))\n\n        x     y     z     a    id\n    &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1:     1    NA     3     4     1\n 2:     2    NA     4     4     1\n 3:     3     1     5     4     2\n 4:     4     2     6     2     2\n 5:     5     3     7     3     3\n 6:     6     4     8     1     3\n 7:     7     5     9     9     3\n 8:     8     6    10     9     1\n 9:     9     7    10    10     2\n10:    10     8    10     6     1\n\n# both down and then up filling by group\ndt_fill(dt_with_nas, \n        y, z, a, \n        id = list(id), \n        .direction = \"downup\")\n\n        x     y     z     a    id\n    &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1:     1     6     3     4     1\n 2:     2     6     4     4     1\n 3:     3     1     5     4     2\n 4:     4     2     6     2     2\n 5:     5     3     7     3     3\n 6:     6     4     8     1     3\n 7:     7     5     9     9     3\n 8:     8     6    10     9     1\n 9:     9     7    10    10     2\n10:    10     8    10     6     1\n\n\n\n\ndt_nest\nNesting data can be useful for a number of reasons, including running multiple statistical models in a structured way, storing non-standard data types (e.g., graphics), easing the cognitive burden of joining data sets, storing information that is only useful as a group (e.g., boundaries of polygons), among others. The dt_nest() function takes a data.table and ID variables and nests the remaining columns into a list column of data.tables as shown below.\n\ndt &lt;- data.table(\n   x = rnorm(1e5),\n   y = runif(1e5),\n   grp = sample(1L:5L, 1e5, replace = TRUE),\n   nested1 = lapply(1:10, sample, 10, replace = TRUE),\n   nested2 = lapply(c(\"thing1\", \"thing2\"), sample, 10, replace = TRUE),\n   id = 1:1e5\n)\n\nnested &lt;- dt_nest(dt, grp)\nnested\n\nKey: &lt;grp&gt;\n     grp                  data\n   &lt;int&gt;                &lt;list&gt;\n1:     1 &lt;data.table[20074x5]&gt;\n2:     2 &lt;data.table[19792x5]&gt;\n3:     3 &lt;data.table[20113x5]&gt;\n4:     4 &lt;data.table[19991x5]&gt;\n5:     5 &lt;data.table[20030x5]&gt;\n\n\n\n\ndt_pivot_longer and dt_pivot_wider\nThe last example for this brief post is pivoting. In my opinion, the pivot syntax is easy to remember and use and as such, is nice to have that syntax with the performance of melt() and dcast(). The syntax, although it doesn’t have the full functionality of tidyr’s pivot functions, can do most things you need to do with reshaping data.\n\nbillboard &lt;- tidyr::billboard \n\nlonger &lt;- billboard %&gt;%\n  dt_pivot_longer(\n     cols = c(-artist, -track, -date.entered),\n     names_to = \"week\",\n     values_to = \"rank\"\n  )\n\nWarning in melt.data.table(data = dt_, id.vars = id_vars, measure.vars = cols,\n: 'measure.vars' [wk1, wk2, wk3, wk4, ...] are not all of the same type. By\norder of hierarchy, the molten data value column will be of type 'double'. All\nmeasure variables not of type 'double' will be coerced too. Check DETAILS in\n?melt.data.table for more on coercion.\n\nlonger\n\n                 artist                   track date.entered   week  rank\n                 &lt;char&gt;                  &lt;char&gt;       &lt;Date&gt; &lt;char&gt; &lt;num&gt;\n    1:            2 Pac Baby Don't Cry (Keep...   2000-02-26    wk1    87\n    2:          2Ge+her The Hardest Part Of ...   2000-09-02    wk1    91\n    3:     3 Doors Down              Kryptonite   2000-04-08    wk1    81\n    4:     3 Doors Down                   Loser   2000-10-21    wk1    76\n    5:         504 Boyz           Wobble Wobble   2000-04-15    wk1    57\n   ---                                                                   \n24088:      Yankee Grey    Another Nine Minutes   2000-04-29   wk76    NA\n24089: Yearwood, Trisha         Real Live Woman   2000-04-01   wk76    NA\n24090:  Ying Yang Twins Whistle While You Tw...   2000-03-18   wk76    NA\n24091:    Zombie Nation           Kernkraft 400   2000-09-02   wk76    NA\n24092:  matchbox twenty                    Bent   2000-04-29   wk76    NA\n\n\nCan also take that long data set and turn it wide again.\n\nwider &lt;- longer %&gt;% \n  dt_pivot_wider(\n    names_from = week,\n    values_from = rank\n  )\nwider[, .(artist, track, wk1, wk2)]\n\nKey: &lt;artist, track&gt;\n               artist                   track   wk1   wk2\n               &lt;char&gt;                  &lt;char&gt; &lt;num&gt; &lt;num&gt;\n  1:            2 Pac Baby Don't Cry (Keep...    87    82\n  2:          2Ge+her The Hardest Part Of ...    91    87\n  3:     3 Doors Down              Kryptonite    81    70\n  4:     3 Doors Down                   Loser    76    76\n  5:         504 Boyz           Wobble Wobble    57    34\n ---                                                     \n313:      Yankee Grey    Another Nine Minutes    86    83\n314: Yearwood, Trisha         Real Live Woman    85    83\n315:  Ying Yang Twins Whistle While You Tw...    95    94\n316:    Zombie Nation           Kernkraft 400    99    99\n317:  matchbox twenty                    Bent    60    37"
  },
  {
    "objectID": "posts/2024-02-25-survey_2023-aljaz_sluga/index.html",
    "href": "posts/2024-02-25-survey_2023-aljaz_sluga/index.html",
    "title": "Results of the 2023 survey",
    "section": "",
    "text": "Thanks to everyone who helped create, shared, or filled out the first data.table survey! The survey was officially open between October 17 and December 1 and it received 391 responses during this time.\nThis post provides a partial summary of the results. It covers all close-ended questions & includes short, informal summaries of the answers to some of the open-ended questions.\nI encourage you to explore the data yourself - you can find it here."
  },
  {
    "objectID": "posts/2024-02-25-survey_2023-aljaz_sluga/index.html#respondents",
    "href": "posts/2024-02-25-survey_2023-aljaz_sluga/index.html#respondents",
    "title": "Results of the 2023 survey",
    "section": "Respondents",
    "text": "Respondents\nA typical respondent was:\n\nan experienced user of R (87.2% having four or more years under their belt) and data.table (-||- 59.5%),\nusing the package for data manipulation (95.9%) and statistical analysis (65.3%),\nin a professional context (80%),\non a daily (46.3%) or at least weekly (28.1%) basis.\n\nFor a richer summary, here are the corresponding bar charts:"
  },
  {
    "objectID": "posts/2024-02-25-survey_2023-aljaz_sluga/index.html#the-good-and-the-bad",
    "href": "posts/2024-02-25-survey_2023-aljaz_sluga/index.html#the-good-and-the-bad",
    "title": "Results of the 2023 survey",
    "section": "The good and the bad",
    "text": "The good and the bad\nWhat do users appreciate most about data.table? A scan of the answers to this open-ended question quickly reveals a clear winner: performance. Nearly every answer brings up speed or memory efficiency.\n\n“Speed! When I need speed, I turn directly to data.table.”\n\nThe runner-up is syntax, with users praising its concision and expressiveness. At the same time, however, syntax often appears in the answers to the question about the biggest challenges in using data.table. For some users it is too concise or difficult to remember. Some users highlighted specific functionality that they find difficult to use: reshaping (dcast/melt) is brought up most often, followed by joining.\n\n“Some queries are so surprisingly simple for complex operations”\n\n\n“Still can’t get used to the syntax, have to look it up every time”\n\nWe explored this topic in a more structured way as well, by asking about the following areas:\n\nPerformance (speed & ability to handle large datasets)\nCode readability\nConcise syntax\nFew changes that break old code\nMinimal dependencies\nError messages\nDocumentation\n\nThe possible answers were Very dissatisfied, Somewhat dissatisfied, Neither satisfied nor dissatisfied, Somewhat satisfied, Very satisfied, which I mapped to -2:2 below. The majority of users are Very satisfied with performance (86.2%), minimal dependencies (77.8%), backward compatibility (60.8%), and syntax concision (57.1%). Syntax readability (35.5%), error messages (29.0%), and documentation (30.2%) lag behind.\n Does this pattern hold across all levels of data.table experience? The following plot shows the average (vertical red line) in addition to the distribution of answers across the different levels of experience.\n\nA way to contextualize these results is to consider how important the different areas are. Another grid question featured this same set of areas, but asked about their importance to the user. I standardized the satisfaction & importance scores and plot the averages below. The two areas that score relatively high in importance but relatively low in satisfaction are syntax readability and quality of documentation.\n\nAnother grid question asked about users’ satisfaction with:\n\nimporting & exporting\nfiltering\nmanipulation & aggregation\nreshaping\njoining/merging\n\nWhile Very satisfied was the dominant response for every area, the results are consistent with earlier qualitative observations in that the share of users selecting this response is substantially lower for reshaping (49.2%) and joining (45.9%) than the other areas (manipulation & aggregation 59.9%, import/export 62.7%, filtering 71.3%).\n The next plot considers variation across levels of data.table experience. One area where beginners (less than 2 years of experience) are less satisfied compared to other users is importing & exporting."
  },
  {
    "objectID": "posts/2024-02-25-survey_2023-aljaz_sluga/index.html#desired-functionality",
    "href": "posts/2024-02-25-survey_2023-aljaz_sluga/index.html#desired-functionality",
    "title": "Results of the 2023 survey",
    "section": "Desired functionality",
    "text": "Desired functionality\nWhat extra functionality would users like to see? The answers to this question covered a lot of different ground, but the three clear winners (with at least 10 mentions each) were:\n\nsupport for out-of-memory processing,\nricher import/export functionality (parquet was mentioned most often, followed by xlsx), and\nintegration with the pipe operator.\n\nPipe integration was also the subject of a later question in the survey, with the majority of users (69.4%) indicating they would find a helper function for working with the pipe useful.\n Another specific question asked about the alias for the walrus operator (:=). Interestingly, set() (47.3%) outperformed let() (39.2%), with setj() (13.6%) far behind."
  },
  {
    "objectID": "posts/2024-02-25-survey_2023-aljaz_sluga/index.html#contributing-to-data.table",
    "href": "posts/2024-02-25-survey_2023-aljaz_sluga/index.html#contributing-to-data.table",
    "title": "Results of the 2023 survey",
    "section": "Contributing to data.table",
    "text": "Contributing to data.table\nGood news for data.table is that many users indicated interest in contributing to the project. In particular, 80 respondents (20.8%) said Yes, and a further 191 (49.6%) respondents answered Maybe.\nWe followed up on this question by asking for interest in specific activities. The orange bars in the following plot represent interest in contributing, whereas the darker parts indicate actual contribution in the past.\n\nWhat would make contributing to data.table easier or more appealing? Setting aside personal reasons, such as lack of time or skill, the following areas were mentioned at least a few times each:\n\nDeveloper documentation. Probably the most common suggestion was documentation that would make it easier for new contributors to understand the codebase.\n\n“Documentation explaining the code of data.table. I mean the big picture, choices made but also some details.”\n\nGitHub issue & PR backlog. Shrinking the number of open GitHub issues and pull requests was another common suggestion.\n\n“data.table has to many open issues to efficiently search for existing issues.”\n\n\n“Getting rid of the current PR backlog would be a big step forward. I feel that a number of good PRs have died on the vine without good reason.”\n\nFast turnaround. A related suggestion was quicker reviews and evaluation of pull requests.\n\n“Some PRs take forever to be approved so this is disheartening for someone to get involved”\n\nSource code. A couple of users suggested that the source code could be restructured to make it easier to work with.\n\n“Better structured code (right now the [.data.table function is more than 2000 LOC; very hard to read and modify!). More structured test suite (right now it is a single long file with numbered tests; adding more tests can be cumbersome).”"
  },
  {
    "objectID": "posts/2024-02-25-survey_2023-aljaz_sluga/index.html#conclusion",
    "href": "posts/2024-02-25-survey_2023-aljaz_sluga/index.html#conclusion",
    "title": "Results of the 2023 survey",
    "section": "Conclusion",
    "text": "Conclusion\nThe responses to this survey make the value of data.table clear, but some users fear that the package may be abandoned or stagnating. Fear not, the project is again picking up steam! An important release with many new features and bug fixes recently landed on CRAN, and the project now has a governance document, which includes information on the different roles you can take. New contributions are very welcome, so check out the guidelines and take a look at the open issues - those labeled beginner-task are a particularly great place to start!"
  },
  {
    "objectID": "posts/2024-02-25-survey_2023-aljaz_sluga/index.html#what-can-you-do",
    "href": "posts/2024-02-25-survey_2023-aljaz_sluga/index.html#what-can-you-do",
    "title": "Results of the 2023 survey",
    "section": "What can you do?",
    "text": "What can you do?\nAre you interested in learning more, or helping grow the data.table community and infrastructure? Here are some places to start:\n\nSubmit a blog post to The Raft with your ideas, insights, or use cases of data.table.\nWeigh in on issues and community discussions at the data.table github page.\nContribute to beginner task updates or documentation needs for the package.\nApply for the data.table travel award to give a talk at a conference.\nEmail r.data.table@gmail.com to reach the NSF Grant steering committee with your thoughts and ideas."
  },
  {
    "objectID": "posts/2024-08-01-seal_of_approval-dtplyr/index.html",
    "href": "posts/2024-08-01-seal_of_approval-dtplyr/index.html",
    "title": "Seal of Approval: dtplyr",
    "section": "",
    "text": "dtplyr hex sticker\n\n\n\nAuthor(s): Hadley Wickham, Maximilian Girlich, Mark Fairbanks, Ryan Dickerson, Posit Software PBC\nMaintainer: Hadley Wickham (hadley@posit.co)\nProvides a data.table backend for dplyr. The goal of dtplyr is to allow you to write dplyr code that is automatically translated to the equivalent, but usually much faster, data.table code."
  },
  {
    "objectID": "posts/2024-08-01-seal_of_approval-dtplyr/index.html#dtplyr",
    "href": "posts/2024-08-01-seal_of_approval-dtplyr/index.html#dtplyr",
    "title": "Seal of Approval: dtplyr",
    "section": "",
    "text": "dtplyr hex sticker\n\n\n\nAuthor(s): Hadley Wickham, Maximilian Girlich, Mark Fairbanks, Ryan Dickerson, Posit Software PBC\nMaintainer: Hadley Wickham (hadley@posit.co)\nProvides a data.table backend for dplyr. The goal of dtplyr is to allow you to write dplyr code that is automatically translated to the equivalent, but usually much faster, data.table code."
  },
  {
    "objectID": "posts/2024-08-01-seal_of_approval-dtplyr/index.html#relationship-with-data.table",
    "href": "posts/2024-08-01-seal_of_approval-dtplyr/index.html#relationship-with-data.table",
    "title": "Seal of Approval: dtplyr",
    "section": "Relationship with data.table",
    "text": "Relationship with data.table\ndtplyr is a bridge for users who are more comfortable with the dplyr syntax, but who want to take advantage of the speed and efficiency benefits of data.table. This package exactly duplicates the core functions of dplyr, but replaces the back-end source code (originally in Base R) with data.table operations."
  },
  {
    "objectID": "posts/2024-08-01-seal_of_approval-dtplyr/index.html#overview",
    "href": "posts/2024-08-01-seal_of_approval-dtplyr/index.html#overview",
    "title": "Seal of Approval: dtplyr",
    "section": "Overview",
    "text": "Overview\nExcerpted from the dtplyr vignette\nTo use dtplyr, you must at least load dtplyr and dplyr. You may also want to load data.table so you can access the other goodies that it provides:\n\nlibrary(data.table)\nlibrary(dtplyr)\nlibrary(dplyr, warn.conflicts = FALSE)\n\nThen use lazy_dt() to create a “lazy” data.table object that tracks the operations performed on it.\n\nmtcars2 &lt;- lazy_dt(mtcars)\n\nYou can preview the transformation (including the generated data.table code) by printing the result:\n\nmtcars2 %&gt;% \n  filter(wt &lt; 5) %&gt;% \n  mutate(l100k = 235.21 / mpg) %&gt;% # liters / 100 km\n  group_by(cyl) %&gt;% \n  summarise(l100k = mean(l100k))\n\nSource: local data table [3 x 2]\nCall:   `_DT1`[wt &lt; 5][, `:=`(l100k = 235.21/mpg)][, .(l100k = mean(l100k)), \n    keyby = .(cyl)]\n\n    cyl l100k\n  &lt;dbl&gt; &lt;dbl&gt;\n1     4  9.05\n2     6 12.0 \n3     8 14.9 \n\n# Use as.data.table()/as.data.frame()/as_tibble() to access results\n\n\nBut generally you should reserve this only for debugging, and use as.data.table(), as.data.frame(), or as_tibble() to indicate that you’re done with the transformation and want to access the results:\n\nmtcars2 %&gt;% \n  filter(wt &lt; 5) %&gt;% \n  mutate(l100k = 235.21 / mpg) %&gt;% # liters / 100 km\n  group_by(cyl) %&gt;% \n  summarise(l100k = mean(l100k)) %&gt;% \n  as_tibble()\n\n# A tibble: 3 × 2\n    cyl l100k\n  &lt;dbl&gt; &lt;dbl&gt;\n1     4  9.05\n2     6 12.0 \n3     8 14.9"
  }
]