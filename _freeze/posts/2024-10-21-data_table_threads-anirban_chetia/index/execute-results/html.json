{
  "hash": "6f7add7e7c1d63caefb46a16e4df540d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"`data.table.threads` - find the best thread count!\"\nauthor: \"Anirban Chetia\"\ndate: \"2025-06-21\"\ncategories: [developer, benchmarks, performance]\nimage: \"threads.jpeg\"\ndraft: true\n---\n\n\n\n\n## [`data.table.threads`](https://github.com/Anirban166/data.table.threads)\n\nWhen working with large datasets, leveraging system resources like multiple CPU threads for shared-memory parallelization can significantly reduce computation time. However, it isn’t straightforward to determine the optimal thread count to obtain the lowest runtime for an operation, or on the other hand, the number of threads required for efficient speedup scaling.\n\nIn an R package like `data.table` where parallelization is extensively utilized (or by most operations), it can be convenient to automatically figure out the number of threads to use for achieving the fastest execution time in the case of a particular routine, without the user needing to rely on ad-hoc experiments. Likewise, it would be handy if the user could set the thread count keeping in mind maximum (or a user-defined ratio) scalability in terms of the speedup obtained, which otherwise can be tricky or time-consuming to figure out manually.\n\nThis is where `data.table.threads` comes in - A package designed to assist in finding the most suitable thread count for the various parallelized operations within `data.table`.\n\n## Key features\n\n`findOptimalThreadCount(rowCount, columnCount, ...)`, the main user-facing function, runs a set of predefined benchmarks for each applicable function across varying thread counts (iteratively from one to the highest number available as per the user's system) and involves computation to find the optimal/ideal speedup and thread count for each function. It returns a `data.table` object of a custom class (`print` and `plot` methods have been provided), which contains the optimal thread count for each function. It also provides plot data (consisting of speedup trends and key points) as attributes.\n\nPrinting this object would enlist the fastest median runtime (in milliseconds) along with the thread count that achieved it for each function. Plotting it would generate a ggplot that additionally shows the ideal and measured speedup trends for each benchmarked `data.table` function.\n\nIf the user wants to factor in a specified speedup efficiency, they can use the function `addRecommendedEfficiency` to add a speedup line (with a slope configured by input argument `efficiencyFactor`; default value is 0.5, or 50% efficiency) along with a point representing the recommended thread count which stems from the highest intersection between this line (of specified thread-use efficiency) and measured speedup data for each function.\n\nHere is an example:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(data.table.threads)\nlibrary(data.table)\n\n(benchmarks <- findOptimalThreadCount(1e7, 10, verbose = FALSE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ndata.table function  Thread count Fastest median runtime (ms)\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \nforder               1            420776.126000          \nGForce_sum           1            155222.146500          \nsubsetting           1            91177.583500           \nfrollmean            1            1336186.542500         \nfcoalesce            1            11525.876000           \nbetween              1            50793.730000           \nfifelse              1            20890.125500           \nnafill               1            13781.188000           \nCJ                   1            5713.855000            \n```\n\n\n:::\n\n```{.r .cell-code}\nplot(benchmarks)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\nbenchmarks_r <- addRecommendedEfficiency(benchmarks, recommendedEfficiency = 0.6)\n\nplot(benchmarks_r)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-2.png){width=672}\n:::\n:::\n\n\n\n\nThe plots are designed to visually interpret the relationship between the number of CPU threads used and the corresponding speedup achieved. Each panel in the grid represents a distinct parallelizable `data.table` function, showing how its performance scales as thread count increases.\n\nThe red lines represent the theoretical linear speedup, which refers to the 'ideal' scaling scenario (e.g., doubling the thread count results in half the runtime). The black and blue ones represent the actual speedup observed from running the benchmarks for each function and the speedup trend based on the user-specified efficiency factor, respectively.\n\nLikewise, in terms of the plotted points, the red ones denote the thread count that achieved the fastest execution time, while the blue ones depict the recommended thread count that balances scalability and thread efficiency based on the value specified for `efficiencyFactor`. Depending on the application, users are expected to prioritize either the fastest runtime (red) or efficient resource usage (blue). The recommended thread count can be particularly useful for multi-user environments or when running multiple parallel tasks, as it avoids saturating resources without significant performance gains.\n\nFrom the example above, we can infer that the functions all exhibit diverse scaling behaviors that reflect their computational characteristics and parallelization overheads. Most notably, `forder` shows near-linear scaling (expected considering factors such as the input containing more rows than columns) initially but tapers off as thread count increases.\n\nIn order to set the thread count based on observed results for a user-specified function and speedup efficiency value, the `setThreadCount(benchmarkData, functionName, efficiencyFactor)` function can then be used:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsetThreadCount(benchmarks, \n               functionName = \"forder\",\n               efficiencyFactor = 1)\n```\n:::\n\n\n\n\nWhen using `findOptimalThreadCount()`, users can also replace the predefined benchmarks with their own expressions by providing a list of custom functions as the `benchmarksList` argument, enabling evaluation tailored to their specific use cases. On top of that, they can also specify their own `data.table` via the `customDT` argument to have the functions they define operate on it instead of the default matrix-based `data.table` that makes use of `rowCount` and `colCount`.\n\nFor instance, here is an example:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Derived from https://github.com/Rdatatable/data.table/issues/4294\n\nNN <- 1e5\nDT <- data.table(\n  grp1 = as.character(rep(1:(NN / 4), each = 4)),\n  grp2 = sample(5000L, NN, TRUE),\n  V = rpois(NN, 10)\n  )\n\n## doesn't run - kb\n# findOptimalThreadCount(\n#   benchmarksList = list(\n#     test4294 = function(dt) dt[, log(sum(V)), by = grp1]), \n#     customDT = DT)\n```\n:::\n\n\n\n\nOptimal efficiency is observed for a single thread in this case since the code includes creation of many small groups where the computation for each of them is relatively lightweight to see improvement in performance outweighing the overhead from multi-threading.\n\n## Conclusion\n\nInformed thread allocation is the goal. Whether one strives for the fastest execution time or seeks a balance between performance and resource efficiency, the package provides practical estimates within a short period of time for fine tuning the thread count in a given system. I hope it can be useful in some way, may it be in terms of offering convenience through the automated benchmarking process with quick recommendations, or flexibility to define custom benchmarks paired up with diagnostic visualizations. As long as it helps in making informed decisions that align with one's computational needs!\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}